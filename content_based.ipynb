{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82d5ca82",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "277473a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rd\n",
    "from surprise import AlgoBase\n",
    "from surprise.prediction_algorithms.predictions import PredictionImpossible\n",
    "\n",
    "from loaders import load_ratings\n",
    "from loaders import load_items\n",
    "from constants import Constant as C\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42c16bf",
   "metadata": {},
   "source": [
    "# Explore and select content features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8378976",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_items = load_items()\n",
    "df_ratings = load_ratings()\n",
    "\n",
    "\n",
    "# Example 1 : create title_length features\n",
    "df_features = df_items[C.LABEL_COL].apply(lambda x: len(x)).to_frame('n_character_title')\n",
    "display(df_features.head())\n",
    "\n",
    "# 2. Year_of_release\n",
    "df_features = df_items[C.LABEL_COL].str.extract(r'\\((\\d{4})\\)')[0].astype('Int64').to_frame('release_year')\n",
    "display(df_features.head())\n",
    "\n",
    "# 3. Genre_list\n",
    "df_genre_list = df_items[C.GENRES_COL].str.split('|').to_frame('genre_list')\n",
    "display(df_genre_list.head())\n",
    "\n",
    "# 4. Genre_one_hot_encoding\n",
    " #Étape 1 : Exploser les listes de genres\n",
    "df_exploded = df_genre_list.explode('genre_list')\n",
    "# Étape 2 : Créer les variables dummies (one-hot encoding)\n",
    "df_dummies = pd.get_dummies(df_exploded['genre_list'])\n",
    "# Étape 3 : Reformer le DataFrame initial avec les one-hot encodings regroupés par index\n",
    "df_genres = df_dummies.groupby(df_exploded.index).sum()\n",
    "# Assure-toi que l'index corresponde à celui de df_items si nécessaire :\n",
    "df_genres = df_genres.reindex(df_items.index).fillna(0).astype(int)\n",
    "display(df_genres.head())\n",
    "# (explore here other features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c9a2b6",
   "metadata": {},
   "source": [
    "# Build a content-based model\n",
    "When ready, move the following class in the *models.py* script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16b0a602",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ContentBased(AlgoBase):\n",
    "    def __init__(self, features_method, regressor_method):\n",
    "        AlgoBase.__init__(self)\n",
    "        self.features_method = features_method \n",
    "        self.regressor_method = regressor_method\n",
    "        self.content_features = self.create_content_features(features_method)\n",
    "\n",
    "        \n",
    "\n",
    "    def create_content_features(self, features_method):\n",
    "        \"\"\"Content Analyzer\"\"\"\n",
    "        df_items = load_items()\n",
    "        df_features = pd.DataFrame(index=df_items.index)\n",
    "        if features_method is None:\n",
    "           df_features = pd.DataFrame(index=df_items.index)\n",
    "        elif features_method == \"title_length\": # a naive method that creates only 1 feature based on title length\n",
    "            df_title_length = df_items[C.LABEL_COL].apply(lambda x: len(x)).to_frame('title_length')\n",
    "            df_title_length['title_length'] = df_title_length['title_length'].fillna(0).astype(int)\n",
    "            mean_title_length = int(df_title_length['title_length'].replace(0, np.nan).mean())\n",
    "            df_title_length.loc[df_title_length['title_length'] == 0, 'title_length'] = mean_title_length\n",
    "            # Normaliser la longueur des titre entre 0 et 1\n",
    "            title_length_min = df_title_length['title_length'].min()\n",
    "            title_length_max = df_title_length['title_length'].max()\n",
    "            df_title_length['title_length'] = (df_title_length['title_length'] - title_length_min) / (title_length_max - title_length_min)\n",
    "            df_features = pd.concat([df_features, df_title_length], axis=1)\n",
    "        elif features_method == \"Year_of_release\":\n",
    "            year = df_items[C.LABEL_COL].str.extract(r'\\((\\d{4})\\)')[0].astype(float)\n",
    "            df_year = year.to_frame(name='year_of_release')\n",
    "            mean_year = df_year.replace(0, np.nan).mean().iloc[0]\n",
    "            df_year['year_of_release'] = df_year['year_of_release'].fillna(mean_year).astype(int)\n",
    "             # Normaliser les dates de sortie \n",
    "            year_min = df_year['year_of_release'].min()\n",
    "            year_max = df_year['year_of_release'].max()\n",
    "            df_year['year_of_release'] = (df_year['year_of_release'] - year_min) / (year_max - year_min)\n",
    "            df_features = df_features.join(df_year, how='left')\n",
    "        elif features_method == \"Genre_binary\":\n",
    "            # Utilisation de binaires pour les genres\n",
    "            df_genre_list = df_items[C.GENRES_COL].str.split('|').explode().to_frame('genre_list')\n",
    "            df_dummies = pd.get_dummies(df_genre_list['genre_list'])\n",
    "            df_genres = df_dummies.groupby(df_genre_list.index).sum()\n",
    "            # Ensure the index matches that of df_items\n",
    "            df_genres = df_genres.reindex(df_items.index).fillna(0).astype(int)\n",
    "            df_features = pd.concat([df_features, df_genres], axis=1)\n",
    "        elif features_method == \"Genre_tfidf\":\n",
    "            # Combine genres into a single string per item\n",
    "            df_items['genre_string'] = df_items[C.GENRES_COL].fillna('').str.replace('|', ' ')\n",
    "            tfidf = TfidfVectorizer()\n",
    "            tfidf_matrix = tfidf.fit_transform(df_items['genre_string'])\n",
    "            tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), index=df_items.index, columns=tfidf.get_feature_names_out())\n",
    "            df_features = pd.concat([df_features, tfidf_df], axis=1)\n",
    "        elif features_method == \"Tags\":\n",
    "            tags_path = str(C.CONTENT_PATH / \"tags.csv\")\n",
    "            df_tags = pd.read_csv(tags_path)\n",
    "            df_tags = df_tags.dropna(subset=['tag'])\n",
    "            df_tags['tag'] = df_tags['tag'].astype(str)\n",
    "            df_tags_grouped = df_tags.groupby('movieId')['tag'].agg(' '.join).to_frame('tags')\n",
    "            tfidf = TfidfVectorizer()\n",
    "            tfidf_matrix = tfidf.fit_transform(df_tags_grouped['tags'])\n",
    "            tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), index=df_tags_grouped.index, columns=tfidf.get_feature_names_out())\n",
    "            df_features = pd.concat([df_features, tfidf_df], axis=1)\n",
    "\n",
    "        elif features_method == \"title_tfidf\":\n",
    "            # Combine titles into a single string per item\n",
    "            df_items['title_string'] = df_items[C.LABEL_COL].fillna('')\n",
    "            tfidf = TfidfVectorizer()\n",
    "            tfidf_matrix = tfidf.fit_transform(df_items['title_string'])\n",
    "            tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), index=df_items.index, columns=tfidf.get_feature_names_out())\n",
    "            nltk.download('stopwords')\n",
    "            nltk.download('wordnet')\n",
    "            nltk.download('omw-1.4')\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            stop_words = set(stopwords.words('english'))\n",
    "            # Preprocess titles: remove stopwords and apply lemmatization\n",
    "            df_items['title_string'] = df_items[C.LABEL_COL].fillna('').apply(lambda x: ' '.join(\n",
    "                    lemmatizer.lemmatize(word) for word in x.split() if word.lower() not in stop_words\n",
    "                )\n",
    "            )\n",
    "            tfidf = TfidfVectorizer()\n",
    "            tfidf_matrix = tfidf.fit_transform(df_items['title_string'])\n",
    "            tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), index=df_items.index, columns=tfidf.get_feature_names_out())\n",
    "            df_features = pd.concat([df_features, tfidf_df], axis=1)\n",
    "        \n",
    "        elif features_method == \"genome_tags\":\n",
    "            tags_path = C.CONTENT_PATH / \"genome-tags.csv\"\n",
    "            scores_path = C.CONTENT_PATH / \"genome-scores.csv\"\n",
    "\n",
    "            df_scores = pd.read_csv(scores_path)\n",
    "            df_tags = pd.read_csv(tags_path)\n",
    "\n",
    "            # Étape 2 : Merge pour récupérer les noms des tags\n",
    "            df_merged = df_scores.merge(df_tags, on='tagId')\n",
    "\n",
    "            # Étape 3 : Pivot → films × tags, valeurs = relevance\n",
    "            df_features = df_merged.pivot_table(index='movieId', columns='tag', values='relevance', fill_value=0)\n",
    "        \n",
    "        elif features_method == \"tfidf_relevance\":\n",
    "\n",
    "            tags_path = C.CONTENT_PATH / \"genome-tags.csv\"\n",
    "            scores_path = C.CONTENT_PATH / \"genome-scores.csv\"\n",
    "\n",
    "            # Charger les données\n",
    "            df_tags = pd.read_csv(tags_path)\n",
    "            df_scores = pd.read_csv(scores_path)\n",
    "\n",
    "            # Fusionner pour obtenir les noms des tags\n",
    "            df_merged = df_scores.merge(df_tags, on='tagId')\n",
    "\n",
    "            # Grouper les tags pertinents par film en texte\n",
    "            df_merged['tag'] = df_merged['tag'].astype(str)\n",
    "            df_texts = df_merged.groupby('movieId')['tag'].apply(lambda x: ' '.join(x)).to_frame('tags')\n",
    "\n",
    "            # Appliquer TF-IDF\n",
    "            tfidf = TfidfVectorizer()\n",
    "            tfidf_matrix = tfidf.fit_transform(df_texts['tags'])\n",
    "\n",
    "            # Créer le DataFrame final de features\n",
    "            df_features = pd.DataFrame(tfidf_matrix.toarray(), index=df_texts.index, columns=tfidf.get_feature_names_out())\n",
    "\n",
    "\n",
    "\n",
    "        else: # (implement other feature creations here)\n",
    "            raise NotImplementedError(f'Feature method {features_method} not yet implemented')\n",
    "        return df_features\n",
    "    \n",
    "    \n",
    "\n",
    "    def fit(self, trainset):\n",
    "        \"\"\"Profile Learner\"\"\"\n",
    "        self.content_features = self.create_content_features(self.features_method)\n",
    "        AlgoBase.fit(self, trainset)\n",
    "        self.user_profile = {u: None for u in trainset.all_users()}\n",
    "        for u in self.user_profile:\n",
    "            user_items = trainset.ur[u]\n",
    "            if len(user_items) > 0:\n",
    "                # Sépare les item_ids internes et les notes\n",
    "                user_ratings = self.trainset.ur[u]\n",
    "                df_user = pd.DataFrame(user_ratings, columns=['inner_item_id', 'user_ratings'])\n",
    "                # Conversion des item_id internes (Surprise) en item_id \"raw\" (MovieLens)\n",
    "                df_user[\"item_id\"] = df_user[\"inner_item_id\"].map(self.trainset.to_raw_iid)\n",
    "                # Fusion avec les features de contenu (sur l'index = item_id raw)\n",
    "                df_user = df_user.merge(self.content_features, how='left', left_on='item_id', right_index=True)\n",
    "                # Préparation des features et des cibles pour l'entraînement\n",
    "                feature_names = list(self.content_features.columns)\n",
    "                X = df_user[feature_names].values\n",
    "                y = df_user['user_ratings'].values\n",
    "                # Gère les NaNs dans les features\n",
    "                X = np.nan_to_num(X)\n",
    "\n",
    "     \n",
    "                if self.regressor_method == 'linear': # Use linear regression\n",
    "                    model = LinearRegression(fit_intercept=True)\n",
    "                elif self.regressor_method == 'lasso':\n",
    "                    model = Lasso(alpha=0.1)\n",
    "                elif self.regressor_method == 'random_forest':\n",
    "                    model = RandomForestRegressor(n_estimators=10, max_depth=10, random_state=42)\n",
    "                elif self.regressor_method== 'neural_network':\n",
    "                    model = MLPRegressor(hidden_layer_sizes=(60, 60), max_iter=2500, learning_rate_init=0.01, alpha=0.0001, random_state=42)\n",
    "                elif self.regressor_method == 'decision_tree':\n",
    "                    model = DecisionTreeRegressor(max_depth=10, random_state=42)\n",
    "                elif self.regressor_method == 'ridge':\n",
    "                    model = Ridge(alpha=1.0)\n",
    "                elif self.regressor_method == 'gradient_boosting':\n",
    "                    model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "                elif  self.regressor_method == 'knn':\n",
    "                    model = KNeighborsRegressor(n_neighbors=5)\n",
    "                elif self.regressor_method == 'elastic_net':\n",
    "                    model = ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
    "\n",
    "                else:\n",
    "                    self.user_profile[u] = None\n",
    "                    \n",
    "                model.fit(X, y)\n",
    "                self.user_profile[u] = model\n",
    "\n",
    "            else:\n",
    "             self.user_profile[u] = None\n",
    "             \n",
    "        \n",
    "    def estimate(self, u, i):\n",
    "        \"\"\"Scoring component used for item filtering\"\"\"\n",
    "        # First, handle cases for unknown users and items\n",
    "        if not (self.trainset.knows_user(u) and self.trainset.knows_item(i)):\n",
    "            raise PredictionImpossible('User and/or item is unkown.')\n",
    "\n",
    "        if self.user_profile[u] is None:\n",
    "            return self.trainset.global_mean\n",
    "\n",
    "        raw_item_id = self.trainset.to_raw_iid(i)\n",
    "        if raw_item_id in self.content_features.index:\n",
    "            item_features = self.content_features.loc[raw_item_id].values.reshape(1, -1)\n",
    "        else:\n",
    "            return self.trainset.global_mean\n",
    "    \n",
    "        if self.regressor_method == 'linear':\n",
    "            score = self.user_profile[u].predict(item_features)[0]\n",
    "        elif self.regressor_method in [\n",
    "        'linear',\n",
    "        'lasso',\n",
    "        'random_forest',\n",
    "        'neural_network',\n",
    "        'decision_tree',\n",
    "        'ridge',\n",
    "        'gradient_boosting',\n",
    "        'knn',\n",
    "        'elastic_net' ]:\n",
    "          score = self.user_profile[u].predict(item_features)[0]\n",
    "\n",
    "        else:\n",
    "            score=None\n",
    "            \n",
    "\n",
    "        return score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd75b7e",
   "metadata": {},
   "source": [
    "The following script test the ContentBased class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69d12f7d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 21\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(prediction)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# (call here the test functions with different regressor methods)\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Test 1 : prédiction aléatoire entre 0.5 et 5\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Test 2 : prédiction aléatoire parmi les notes données par l'utilisateur\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m#test_contentbased_class(feature_method=None, regressor_method='random_sample')\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[43mtest_contentbased_class\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtfidf_relevance\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mknn\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m test_contentbased_class(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenome_tags\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mridge\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     23\u001b[0m test_contentbased_class(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle_tfidf\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[15], line 7\u001b[0m, in \u001b[0;36mtest_contentbased_class\u001b[1;34m(feature_method, regressor_method)\u001b[0m\n\u001b[0;32m      5\u001b[0m sp_ratings \u001b[38;5;241m=\u001b[39m load_ratings(surprise_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      6\u001b[0m train_set \u001b[38;5;241m=\u001b[39m sp_ratings\u001b[38;5;241m.\u001b[39mbuild_full_trainset()\n\u001b[1;32m----> 7\u001b[0m content_algo \u001b[38;5;241m=\u001b[39m \u001b[43mContentBased\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature_method\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregressor_method\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m content_algo\u001b[38;5;241m.\u001b[39mfit(train_set)\n\u001b[0;32m      9\u001b[0m anti_test_set_first \u001b[38;5;241m=\u001b[39m train_set\u001b[38;5;241m.\u001b[39mbuild_anti_testset()[\u001b[38;5;241m0\u001b[39m]\n",
      "Cell \u001b[1;32mIn[13], line 6\u001b[0m, in \u001b[0;36mContentBased.__init__\u001b[1;34m(self, features_method, regressor_method)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures_method \u001b[38;5;241m=\u001b[39m features_method \n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregressor_method \u001b[38;5;241m=\u001b[39m regressor_method\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontent_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_content_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures_method\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[13], line 114\u001b[0m, in \u001b[0;36mContentBased.create_content_features\u001b[1;34m(self, features_method)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;66;03m# Appliquer TF-IDF\u001b[39;00m\n\u001b[0;32m    113\u001b[0m tfidf \u001b[38;5;241m=\u001b[39m TfidfVectorizer()\n\u001b[1;32m--> 114\u001b[0m tfidf_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mtfidf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_texts\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;66;03m# Créer le DataFrame final de features\u001b[39;00m\n\u001b[0;32m    117\u001b[0m df_features \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(tfidf_matrix\u001b[38;5;241m.\u001b[39mtoarray(), index\u001b[38;5;241m=\u001b[39mdf_texts\u001b[38;5;241m.\u001b[39mindex, columns\u001b[38;5;241m=\u001b[39mtfidf\u001b[38;5;241m.\u001b[39mget_feature_names_out())\n",
      "File \u001b[1;32mc:\\Users\\ducar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:2104\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2097\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[0;32m   2098\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2099\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[0;32m   2100\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[0;32m   2101\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[0;32m   2102\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[0;32m   2103\u001b[0m )\n\u001b[1;32m-> 2104\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2105\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m   2106\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2107\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ducar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ducar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1389\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1388\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sort_features(X, vocabulary)\n\u001b[1;32m-> 1389\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_limit_features\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1390\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocabulary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_doc_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_doc_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_features\u001b[49m\n\u001b[0;32m   1391\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1392\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1393\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sort_features(X, vocabulary)\n",
      "File \u001b[1;32mc:\\Users\\ducar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1244\u001b[0m, in \u001b[0;36mCountVectorizer._limit_features\u001b[1;34m(self, X, vocabulary, high, low, limit)\u001b[0m\n\u001b[0;32m   1240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kept_indices) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1241\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1242\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAfter pruning, no terms remain. Try a lower min_df or a higher max_df.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1243\u001b[0m     )\n\u001b[1;32m-> 1244\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkept_indices\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ducar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\sparse\\_index.py:76\u001b[0m, in \u001b[0;36mIndexMixin.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sliceXslice(row, col)\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m col\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 76\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_sliceXarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex results in >2 dimensions\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m row\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\ducar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\sparse\\_csr.py:208\u001b[0m, in \u001b[0;36m_csr_base._get_sliceXarray\u001b[1;34m(self, row, col)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_sliceXarray\u001b[39m(\u001b[38;5;28mself\u001b[39m, row, col):\n\u001b[1;32m--> 208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_major_slice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_minor_index_fancy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ducar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\sparse\\_compressed.py:797\u001b[0m, in \u001b[0;36m_cs_matrix._minor_index_fancy\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    795\u001b[0m res_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(nnz, dtype\u001b[38;5;241m=\u001b[39midx_dtype)\n\u001b[0;32m    796\u001b[0m res_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(nnz, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m--> 797\u001b[0m \u001b[43mcsr_column_index2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol_order\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol_offsets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mres_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mres_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m((res_data, res_indices, res_indptr),\n\u001b[0;32m    800\u001b[0m                       shape\u001b[38;5;241m=\u001b[39mnew_shape, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def test_contentbased_class(feature_method, regressor_method):\n",
    "    \"\"\"Test the ContentBased class.\n",
    "    Tries to make a prediction on the first (user,item ) tuple of the anti_test_set\n",
    "    \"\"\"\n",
    "    sp_ratings = load_ratings(surprise_format=True)\n",
    "    train_set = sp_ratings.build_full_trainset()\n",
    "    content_algo = ContentBased(feature_method, regressor_method)\n",
    "    content_algo.fit(train_set)\n",
    "    anti_test_set_first = train_set.build_anti_testset()[0]\n",
    "    prediction = content_algo.predict(anti_test_set_first[0], anti_test_set_first[1])\n",
    "    print(prediction)\n",
    "\n",
    "# (call here the test functions with different regressor methods)\n",
    "\n",
    "# Test 1 : prédiction aléatoire entre 0.5 et 5\n",
    "#test_contentbased_class(feature_method=None, regressor_method='random_score')\n",
    "\n",
    "# Test 2 : prédiction aléatoire parmi les notes données par l'utilisateur\n",
    "#test_contentbased_class(feature_method=None, regressor_method='random_sample')\n",
    "\n",
    "test_contentbased_class('tfidf_relevance', 'knn')\n",
    "test_contentbased_class('genome_tags', 'ridge')\n",
    "test_contentbased_class('title_tfidf', 'linear')\n",
    "test_contentbased_class('title_length','linear')\n",
    "test_contentbased_class('title_length','lasso')\n",
    "test_contentbased_class('title_length','random_forest')\n",
    "test_contentbased_class('title_length','neural_network')\n",
    "test_contentbased_class('title_length','decision_tree')\n",
    "test_contentbased_class('title_length','ridge')\n",
    "test_contentbased_class('title_length','gradient_boosting')\n",
    "test_contentbased_class('title_length','knn')\n",
    "test_contentbased_class('title_length','elastic_net')\n",
    "test_contentbased_class('Tags','linear')\n",
    "test_contentbased_class('Genre_tfidf','linear')\n",
    "test_contentbased_class('Genre_binary','linear')\n",
    "test_contentbased_class('title_length','linear')\n",
    "test_contentbased_class('Year_of_release','linear')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
