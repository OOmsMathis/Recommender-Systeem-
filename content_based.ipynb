{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82d5ca82",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "277473a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Standard library imports\n",
    "from collections import defaultdict\n",
    "import heapq\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rd\n",
    "import nltk\n",
    "\n",
    "from surprise import AlgoBase, KNNWithMeans, SVD\n",
    "from surprise.prediction_algorithms.predictions import PredictionImpossible\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Local imports\n",
    "from loaders import load_ratings, load_items\n",
    "from constants import Constant as C\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42c16bf",
   "metadata": {},
   "source": [
    "# Explore and select content features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8378976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_character_title</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movieId</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         n_character_title\n",
       "movieId                   \n",
       "3                       23\n",
       "15                      23\n",
       "34                      11\n",
       "59                      44\n",
       "64                      20"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_items = load_items()\n",
    "df_ratings = load_ratings()\n",
    "\n",
    "# Example 1 : create title_length features\n",
    "df_features = df_items[C.LABEL_COL].apply(lambda x: len(x)).to_frame('n_character_title')\n",
    "display(df_features.head())\n",
    "\n",
    "# (explore here other features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c9a2b6",
   "metadata": {},
   "source": [
    "# Build a content-based model\n",
    "When ready, move the following class in the *models.py* script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16b0a602",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentBased(AlgoBase):\n",
    "    def __init__(self, features_methods, regressor_method):\n",
    "        AlgoBase.__init__(self)\n",
    "        self.features_method = features_methods\n",
    "        self.regressor_method = regressor_method\n",
    "        self.content_features = self.create_content_features(features_methods)\n",
    "        \n",
    "\n",
    "    def create_content_features(self, features_methods):\n",
    "        \"\"\"Content Analyzer\"\"\"\n",
    "        df_items = load_items()\n",
    "        df_ratings = load_ratings()\n",
    "        df_features = pd.DataFrame(index=df_items.index)\n",
    "        if features_methods is None:\n",
    "           df_features = pd.DataFrame(index=df_items.index)\n",
    "        if isinstance(features_methods, str):\n",
    "         features_methods = [features_methods]\n",
    "        \n",
    "        for feature_method in features_methods:\n",
    "         if feature_method == \"title_length\": # a naive method that creates only 1 feature based on title length\n",
    "            df_title_length = df_items[C.LABEL_COL].apply(lambda x: len(x)).to_frame('title_length')\n",
    "            df_title_length['title_length'] = df_title_length['title_length'].fillna(0).astype(int)\n",
    "            mean_title_length = int(df_title_length['title_length'].replace(0, np.nan).mean())\n",
    "            df_title_length.loc[df_title_length['title_length'] == 0, 'title_length'] = mean_title_length\n",
    "            # Normaliser la longueur des titre entre 0 et 1\n",
    "            title_length_min = df_title_length['title_length'].min()\n",
    "            title_length_max = df_title_length['title_length'].max()\n",
    "            df_title_length['title_length'] = (df_title_length['title_length'] - title_length_min) / (title_length_max - title_length_min)\n",
    "            df_features = pd.concat([df_features, df_title_length], axis=1)\n",
    "         elif feature_method == \"Year_of_release\":\n",
    "            year = df_items[C.LABEL_COL].str.extract(r'\\((\\d{4})\\)')[0].astype(float)\n",
    "            df_year = year.to_frame(name='year_of_release')\n",
    "            mean_year = df_year.replace(0, np.nan).mean().iloc[0]\n",
    "            df_year['year_of_release'] = df_year['year_of_release'].fillna(mean_year).astype(int)\n",
    "             # Normaliser les dates de sortie \n",
    "            year_min = df_year['year_of_release'].min()\n",
    "            year_max = df_year['year_of_release'].max()\n",
    "            df_year['year_of_release'] = (df_year['year_of_release'] - year_min) / (year_max - year_min)\n",
    "            df_features = pd.concat([df_features, df_year], axis=1)\n",
    "         elif feature_method ==\"average_ratings\":\n",
    "            # moyenne des notes par films\n",
    "            average_rating = df_ratings.groupby('movieId')[C.RATING_COL].mean().rename('average_rating').to_frame()\n",
    "            global_avg = df_ratings['rating'].mean()\n",
    "            average_rating['average_rating'] = average_rating['average_rating'].fillna(global_avg)\n",
    "            # Normaliser la moyenne des notes par films\n",
    "            avg_rating_min = average_rating['average_rating'].min()\n",
    "            avg_rating_max = average_rating['average_rating'].max()\n",
    "            average_rating['average_rating'] = (average_rating['average_rating'] - avg_rating_min) / (avg_rating_max - avg_rating_min)\n",
    "            df_features = df_features.join(average_rating, how='left')\n",
    "         elif feature_method ==\"count_ratings\":\n",
    "             # Count the number of ratings for each movie\n",
    "            rating_count = df_ratings.groupby('movieId')[C.RATING_COL].size().rename('rating_count').to_frame()\n",
    "            rating_count['rating_count'] = rating_count['rating_count'].fillna(0).astype(int)\n",
    "            mean_rating_count = int(rating_count['rating_count'].replace(0, np.nan).mean())\n",
    "            rating_count.loc[rating_count['rating_count'] == 0, 'rating_count'] = mean_rating_count\n",
    "                # Normalize the rating count\n",
    "            rating_count_min = rating_count['rating_count'].min()\n",
    "            rating_count_max = rating_count['rating_count'].max()\n",
    "            rating_count['rating_count'] = (rating_count['rating_count'] - rating_count_min) / (rating_count_max - rating_count_min)\n",
    "            df_features = df_features.join(rating_count, how='left')\n",
    "         elif feature_method == \"Genre_binary\":\n",
    "                df_genre_list = df_items[C.GENRES_COL].str.split('|').explode().to_frame('genre_list')\n",
    "                df_dummies = pd.get_dummies(df_genre_list['genre_list'])\n",
    "                df_genres = df_dummies.groupby(df_genre_list.index).sum()\n",
    "                df_genres = df_genres.reindex(df_items.index).fillna(0).astype(int)\n",
    "                df_features = pd.concat([df_features, df_genres], axis=1)\n",
    "         elif feature_method == \"Genre_tfidf\":\n",
    "                df_items['genre_string'] = df_items[C.GENRES_COL].fillna('').str.replace('|', ' ')\n",
    "                tfidf = TfidfVectorizer()\n",
    "                tfidf_matrix = tfidf.fit_transform(df_items['genre_string'])\n",
    "                tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), index=df_items.index, columns=tfidf.get_feature_names_out())\n",
    "                df_features = pd.concat([df_features, tfidf_df], axis=1)\n",
    "         elif feature_method == \"Tags\":\n",
    "                tags_path = str(C.CONTENT_PATH / \"tags.csv\")\n",
    "                df_tags = pd.read_csv(tags_path)\n",
    "                df_tags = df_tags.dropna(subset=['tag'])\n",
    "                df_tags['tag'] = df_tags['tag'].astype(str)\n",
    "                df_tags_grouped = df_tags.groupby('movieId')['tag'].agg(' '.join).to_frame('tags')\n",
    "                tfidf = TfidfVectorizer()\n",
    "                tfidf_matrix = tfidf.fit_transform(df_tags_grouped['tags'])\n",
    "                tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), index=df_tags_grouped.index, columns=tfidf.get_feature_names_out())\n",
    "                df_features = pd.concat([df_features, tfidf_df], axis=1)\n",
    "         elif feature_method == \"tmdb_vote_average\":\n",
    "                tmdb_path = str(C.CONTENT_PATH / \"tmdb_full_features.csv\")\n",
    "                df_tmdb = pd.read_csv(tmdb_path)\n",
    "                df_tmdb = df_tmdb[['movieId', 'vote_average']].drop_duplicates('movieId')\n",
    "                df_tmdb = df_tmdb.set_index('movieId')\n",
    "                mean_vote = df_tmdb['vote_average'].mean()\n",
    "                df_tmdb['vote_average'] = df_tmdb['vote_average'].fillna(mean_vote)\n",
    "                min_vote = df_tmdb['vote_average'].min()\n",
    "                max_vote = df_tmdb['vote_average'].max()\n",
    "                df_tmdb['vote_average'] = (df_tmdb['vote_average'] - min_vote) / (max_vote - min_vote)\n",
    "                df_features = df_features.join(df_tmdb, how='left')\n",
    "                df_features = df_features.fillna(0)\n",
    "         elif feature_method == \"title_tfidf\":\n",
    "                # Combine titles into a single string per item\n",
    "                df_items['title_string'] = df_items[C.LABEL_COL].fillna('')\n",
    "                tfidf = TfidfVectorizer()\n",
    "                tfidf_matrix = tfidf.fit_transform(df_items['title_string'])\n",
    "                tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), index=df_items.index, columns=tfidf.get_feature_names_out())\n",
    "                nltk.download('stopwords')\n",
    "                nltk.download('wordnet')\n",
    "                nltk.download('omw-1.4')\n",
    "                lemmatizer = WordNetLemmatizer()\n",
    "                stop_words = set(stopwords.words('english'))\n",
    "                # Preprocess titles: remove stopwords and apply lemmatization\n",
    "                df_items['title_string'] = df_items[C.LABEL_COL].fillna('').apply(lambda x: ' '.join(\n",
    "                        lemmatizer.lemmatize(word) for word in x.split() if word.lower() not in stop_words\n",
    "                    )\n",
    "                )\n",
    "                tfidf = TfidfVectorizer()\n",
    "                tfidf_matrix = tfidf.fit_transform(df_items['title_string'])\n",
    "                tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), index=df_items.index, columns=tfidf.get_feature_names_out())\n",
    "                df_features = pd.concat([df_features, tfidf_df], axis=1)\n",
    "       \n",
    "         elif feature_method == \"genome_tags\":\n",
    "                tags_path = C.CONTENT_PATH / \"genome-tags.csv\"\n",
    "                scores_path = C.CONTENT_PATH / \"genome-scores.csv\"\n",
    "                df_scores = pd.read_csv(scores_path)\n",
    "                df_tags = pd.read_csv(tags_path)\n",
    "                 # Étape 2 : Merge pour récupérer les noms des tags\n",
    "                df_merged = df_scores.merge(df_tags, on='tagId')\n",
    "                # Étape 3 : Pivot → films × tags, valeurs = relevance\n",
    "                df_features = df_merged.pivot_table(index='movieId', columns='tag', values='relevance', fill_value=0)\n",
    "        \n",
    "         elif feature_method == \"tfidf_relevance\":\n",
    "                tags_path = C.CONTENT_PATH / \"genome-tags.csv\"\n",
    "                scores_path = C.CONTENT_PATH / \"genome-scores.csv\"\n",
    "                # Charger les données\n",
    "                df_tags = pd.read_csv(tags_path)\n",
    "                df_scores = pd.read_csv(scores_path)\n",
    "                # Fusionner pour obtenir les noms des tags\n",
    "                df_merged = df_scores.merge(df_tags, on='tagId')\n",
    "                # Grouper les tags pertinents par film en texte\n",
    "                df_merged['tag'] = df_merged['tag'].astype(str)\n",
    "                df_texts = df_merged.groupby('movieId')['tag'].apply(lambda x: ' '.join(x)).to_frame('tags')\n",
    "                # Appliquer TF-IDF\n",
    "                tfidf = TfidfVectorizer()\n",
    "                tfidf_matrix = tfidf.fit_transform(df_texts['tags'])\n",
    "                # Créer le DataFrame final de features\n",
    "                df_features = pd.DataFrame(tfidf_matrix.toarray(), index=df_texts.index, columns=tfidf.get_feature_names_out())\n",
    "         elif feature_method == \"visuals\":\n",
    "                visual_path = C.VISUAL/ \"LLVisualFeatures13K_QuantileLog.csv\"\n",
    "                df_visual = pd.read_csv(visual_path).set_index(\"ML_Id\")\n",
    "\n",
    "                # Nettoyage\n",
    "                df_visual = df_visual.fillna(0)\n",
    "\n",
    "                # Normalisation colonne par colonne\n",
    "                for col in df_visual.columns:\n",
    "                    col_min = df_visual[col].min()\n",
    "                    col_max = df_visual[col].max()\n",
    "                    if col_max != col_min:\n",
    "                        df_visual[col] = (df_visual[col] - col_min) / (col_max - col_min)\n",
    "\n",
    "                # Fusion dans df_features\n",
    "                df_features = df_features.join(df_visual, how='left')\n",
    "                 # Très important : remplacer les NaN après fusion\n",
    "                df_features = df_features.fillna(0)\n",
    "         elif feature_method == \"tmdb_popularity\":\n",
    "                tmdb_path = C.CONTENT_PATH / \"tmdb_full_features.csv\"\n",
    "                df_tmdb = pd.read_csv(tmdb_path)\n",
    "                df_tmdb = df_tmdb[['movieId', 'popularity']].drop_duplicates('movieId')\n",
    "                df_tmdb = df_tmdb.set_index('movieId')\n",
    "                mean_popularity = df_tmdb['popularity'].mean()\n",
    "                df_tmdb['popularity'] = df_tmdb['popularity'].fillna(mean_popularity)\n",
    "                min_popularity = df_tmdb['popularity'].min()\n",
    "                max_popularity = df_tmdb['popularity'].max()\n",
    "                df_tmdb['popularity'] = (df_tmdb['popularity'] - min_popularity) / (max_popularity - min_popularity)\n",
    "                df_features = df_features.join(df_tmdb, how='left')\n",
    "                df_features = df_features.fillna(0)\n",
    "\n",
    "         elif feature_method == \"tmdb_budget\":\n",
    "                tmdb_path = C.CONTENT_PATH / \"tmdb_full_features.csv\"\n",
    "                df_tmdb = pd.read_csv(tmdb_path)\n",
    "                df_tmdb = df_tmdb[['movieId', 'budget']].drop_duplicates('movieId')\n",
    "                df_tmdb = df_tmdb.set_index('movieId')\n",
    "                mean_budget = df_tmdb['budget'].mean()\n",
    "                df_tmdb['budget'] = df_tmdb['budget'].fillna(mean_budget)\n",
    "                min_budget = df_tmdb['budget'].min()\n",
    "                max_budget = df_tmdb['budget'].max()\n",
    "                df_tmdb['budget_norm'] = (df_tmdb['budget'] - min_budget) / (max_budget - min_budget)\n",
    "                # On ne join que la colonne normalisée\n",
    "                df_features = df_features.join(df_tmdb[['budget_norm']], how='left')\n",
    "                df_features = df_features.fillna(0)\n",
    "            \n",
    "         elif feature_method == \"tmdb_revenue\":\n",
    "                tmdb_path = C.CONTENT_PATH / \"tmdb_full_features.csv\"\n",
    "                df_tmdb = pd.read_csv(tmdb_path)\n",
    "                df_tmdb = df_tmdb[['movieId', 'revenue']].drop_duplicates('movieId')\n",
    "                df_tmdb = df_tmdb.set_index('movieId')\n",
    "                mean_revenue = df_tmdb['revenue'].mean()\n",
    "                df_tmdb['revenue'] = df_tmdb['revenue'].fillna(mean_revenue)\n",
    "                min_revenue = df_tmdb['revenue'].min()\n",
    "                max_revenue = df_tmdb['revenue'].max()\n",
    "                df_tmdb['revenue'] = (df_tmdb['revenue'] - min_revenue) / (max_revenue - min_revenue)\n",
    "                df_features = df_features.join(df_tmdb, how='left')\n",
    "                df_features = df_features.fillna(0)\n",
    "            \n",
    "         elif feature_method == \"tmdb_profit\":\n",
    "                tmdb_path = C.CONTENT_PATH / \"tmdb_full_features.csv\"\n",
    "                df_tmdb = pd.read_csv(tmdb_path)\n",
    "                df_tmdb['profit'] = df_tmdb['revenue'] - df_tmdb['budget']\n",
    "                df_tmdb = df_tmdb[['movieId', 'profit']].drop_duplicates('movieId')\n",
    "                df_tmdb = df_tmdb.set_index('movieId')\n",
    "                mean_profit = df_tmdb['profit'].mean()\n",
    "                df_tmdb['profit'] = df_tmdb['profit'].fillna(mean_profit)\n",
    "                min_profit = df_tmdb['profit'].min()\n",
    "                max_profit = df_tmdb['profit'].max()\n",
    "                df_tmdb['profit'] = (df_tmdb['profit'] - min_profit) / (max_profit - min_profit)\n",
    "                df_features = df_features.join(df_tmdb, how='left')\n",
    "                df_features = df_features.fillna(0)\n",
    "            \n",
    "         elif feature_method == \"tmdb_runtime\":\n",
    "                tmdb_path = C.CONTENT_PATH / \"tmdb_full_features.csv\"\n",
    "                df_tmdb = pd.read_csv(tmdb_path)\n",
    "                df_tmdb = df_tmdb[['movieId', 'runtime']].drop_duplicates('movieId')\n",
    "                df_tmdb = df_tmdb.set_index('movieId')\n",
    "                mean_runtime = df_tmdb['runtime'].mean()\n",
    "                df_tmdb['runtime'] = df_tmdb['runtime'].fillna(mean_runtime)\n",
    "                min_runtime = df_tmdb['runtime'].min()\n",
    "                max_runtime = df_tmdb['runtime'].max()\n",
    "                df_tmdb['runtime'] = (df_tmdb['runtime'] - min_runtime) / (max_runtime - min_runtime)\n",
    "                df_features = df_features.join(df_tmdb, how='left')\n",
    "                df_features = df_features.fillna(0)\n",
    "            \n",
    "         elif feature_method == \"tmdb_vote_count\":\n",
    "                tmdb_path = C.CONTENT_PATH / \"tmdb_full_features.csv\"\n",
    "                df_tmdb = pd.read_csv(tmdb_path)\n",
    "                df_tmdb = df_tmdb[['movieId', 'vote_count']].drop_duplicates('movieId')\n",
    "                df_tmdb = df_tmdb.set_index('movieId')\n",
    "                mean_vote_count = df_tmdb['vote_count'].mean()\n",
    "                df_tmdb['vote_count'] = df_tmdb['vote_count'].fillna(mean_vote_count)\n",
    "                min_vote_count = df_tmdb['vote_count'].min()\n",
    "                max_vote_count = df_tmdb['vote_count'].max()\n",
    "                df_tmdb['vote_count'] = (df_tmdb['vote_count'] - min_vote_count) / (max_vote_count - min_vote_count)\n",
    "                df_features = df_features.join(df_tmdb, how='left')\n",
    "                df_features = df_features.fillna(0)\n",
    "                df_features = df_features.fillna(0)\n",
    "            \n",
    "         elif feature_method == \"tmdb_cast\":\n",
    "                tmdb_path = C.CONTENT_PATH / \"tmdb_full_features.csv\"\n",
    "                df_tmdb = pd.read_csv(tmdb_path)\n",
    "                df_tmdb = df_tmdb[['movieId', 'cast']].drop_duplicates('movieId')\n",
    "                df_tmdb['cast'] = df_tmdb['cast'].fillna('')\n",
    "                tfidf = TfidfVectorizer()\n",
    "                tfidf_matrix = tfidf.fit_transform(df_tmdb['cast'])\n",
    "                tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), index=df_tmdb['movieId'], columns=tfidf.get_feature_names_out())\n",
    "                df_features = df_features.join(tfidf_df, how='left')\n",
    "                df_features = df_features.fillna(0)\n",
    "            \n",
    "         elif feature_method == \"tmdb_director\":\n",
    "                tmdb_path = C.CONTENT_PATH / \"tmdb_full_features.csv\"\n",
    "                df_tmdb = pd.read_csv(tmdb_path)\n",
    "                df_tmdb = df_tmdb[['movieId', 'director']].drop_duplicates('movieId')\n",
    "                df_tmdb['director'] = df_tmdb['director'].fillna('')\n",
    "                tfidf = TfidfVectorizer()\n",
    "                tfidf_matrix = tfidf.fit_transform(df_tmdb['director'])\n",
    "                tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), index=df_tmdb['movieId'], columns=tfidf.get_feature_names_out())\n",
    "                df_features = df_features.join(tfidf_df, how='left')\n",
    "                df_features = df_features.fillna(0)\n",
    "\n",
    "         elif feature_method == \"tmdb_original_language\":\n",
    "                tmdb_path = C.CONTENT_PATH / \"tmdb_full_features.csv\"\n",
    "                df_tmdb = pd.read_csv(tmdb_path)\n",
    "                df_tmdb = df_tmdb[['movieId', 'original_language']].drop_duplicates('movieId')\n",
    "                df_tmdb = df_tmdb.set_index('movieId')\n",
    "                df_tmdb['original_language'] = df_tmdb['original_language'].fillna('unknown')\n",
    "                # One-hot encoding des langues\n",
    "                df_lang_dummies = pd.get_dummies(df_tmdb['original_language'], prefix='lang')\n",
    "                # Gérer les valeurs manquantes après le merge\n",
    "                df_lang_dummies = df_lang_dummies.reindex(df_features.index, fill_value=0)\n",
    "                df_features = df_features.join(tfidf_df, how='left')\n",
    "                df_features = df_features.fillna(0)\n",
    "         else: # (implement other feature creations here)\n",
    "            raise NotImplementedError(f'Feature method {features_methods} not yet implemented')\n",
    "        return df_features\n",
    "\n",
    "    def fit(self, trainset):\n",
    "        \"\"\"Profile Learner\"\"\"\n",
    "        AlgoBase.fit(self, trainset)\n",
    "        self.user_profile = {u: None for u in trainset.all_users()}\n",
    "        for u in self.user_profile:\n",
    "            user_items = trainset.ur[u]\n",
    "            if len(user_items) > 0:\n",
    "                # Sépare les item_ids internes et les notes\n",
    "                user_ratings = self.trainset.ur[u]\n",
    "                df_user = pd.DataFrame(user_ratings, columns=['inner_item_id', 'user_ratings'])\n",
    "                # Conversion des item_id internes (Surprise) en item_id \"raw\" (MovieLens)\n",
    "                df_user[\"item_id\"] = df_user[\"inner_item_id\"].map(self.trainset.to_raw_iid)\n",
    "                # Fusion avec les features de contenu (sur l'index = item_id raw)\n",
    "                df_user = df_user.merge(self.content_features, how='left', left_on='item_id', right_index=True)\n",
    "                # Préparation des features et des cibles pour l'entraînement\n",
    "                feature_names = list(self.content_features.columns)\n",
    "                X = df_user[feature_names].values\n",
    "                y = df_user['user_ratings'].values\n",
    "                # Gère les NaNs dans les features\n",
    "                X = np.nan_to_num(X)\n",
    "\n",
    "     \n",
    "                if self.regressor_method == 'linear': # Use linear regression\n",
    "                    model = LinearRegression(fit_intercept=True)\n",
    "                elif self.regressor_method == 'lasso':\n",
    "                    model = Lasso(alpha=0.1)\n",
    "                elif self.regressor_method == 'random_forest':\n",
    "                    model = RandomForestRegressor(n_estimators=10, max_depth=10, random_state=42)\n",
    "                elif self.regressor_method== 'neural_network':\n",
    "                    model = MLPRegressor(hidden_layer_sizes=(60, 60), max_iter=2500, learning_rate_init=0.01, alpha=0.0001, random_state=42)\n",
    "                elif self.regressor_method == 'decision_tree':\n",
    "                    model = DecisionTreeRegressor(max_depth=10, random_state=42)\n",
    "                elif self.regressor_method == 'ridge':\n",
    "                    model = Ridge(alpha=0.15)\n",
    "                elif self.regressor_method == 'gradient_boosting':\n",
    "                    model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "                elif  self.regressor_method == 'knn':\n",
    "                    model = KNeighborsRegressor(n_neighbors=5)\n",
    "                elif self.regressor_method == 'elastic_net':\n",
    "                    model = ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
    "\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown regressor method: {self.regressor_method}\")\n",
    "                    \n",
    "                model.fit(X, y)\n",
    "                self.user_profile[u] = model\n",
    "\n",
    "            else:\n",
    "             self.user_profile[u] = None\n",
    "             \n",
    "        \n",
    "    def estimate(self, u, i):\n",
    "        \"\"\"Scoring component used for item filtering\"\"\"\n",
    "        # First, handle cases for unknown users and items\n",
    "        if not (self.trainset.knows_user(u) and self.trainset.knows_item(i)):\n",
    "            raise PredictionImpossible('User and/or item is unkown.')\n",
    "\n",
    "        if self.user_profile[u] is None:\n",
    "            return self.trainset.global_mean\n",
    "\n",
    "        raw_item_id = self.trainset.to_raw_iid(i)\n",
    "        if raw_item_id in self.content_features.index:\n",
    "            item_features = self.content_features.loc[raw_item_id].values.reshape(1, -1)\n",
    "        else:\n",
    "            return self.trainset.global_mean\n",
    "    \n",
    "        if self.regressor_method == 'linear':\n",
    "            score = self.user_profile[u].predict(item_features)[0]\n",
    "        elif self.regressor_method in [\n",
    "        'linear',\n",
    "        'lasso',\n",
    "        'random_forest',\n",
    "        'neural_network',\n",
    "        'decision_tree',\n",
    "        'ridge',\n",
    "        'gradient_boosting',\n",
    "        'knn',\n",
    "        'elastic_net' ]:\n",
    "          score = self.user_profile[u].predict(item_features)[0]\n",
    "\n",
    "        else:\n",
    "            score=None\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e06d9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features_methods = [\"title_length\", \"Year_of_release\", \"Genre_binary\", ...]  # à adapter selon ton besoin\n",
    "features_methods = [\"genome_tags\"]\n",
    "\n",
    "# Créer une instance temporaire de ContentBased (le regressor_method n'a pas d'importance ici)\n",
    "content_based_tmp = ContentBased(features_methods, regressor_method=\"linear\")\n",
    "features_df = content_based_tmp.create_content_features(features_methods)\n",
    "\n",
    "# On sélectionne les 50 premiers films pour la projection\n",
    "features_50 = features_df.loc[df_50.index].fillna(0)\n",
    "\n",
    "# PCA sur les features sélectionnées\n",
    "pca = PCA(n_components=2)\n",
    "features_2d = pca.fit_transform(features_50)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(features_2d[:, 0], features_2d[:, 1])\n",
    "\n",
    "for i, idx in enumerate(df_50.index):\n",
    "    plt.plot([0, features_2d[i, 0]], [0, features_2d[i, 1]], color='gray', alpha=0.5)\n",
    "    plt.text(features_2d[i, 0], features_2d[i, 1], df_50.loc[idx, 'title'].split(' (')[0], fontsize=8)\n",
    "\n",
    "plt.title(\"Projection 2D des 50 premiers films selon les features sélectionnées\")\n",
    "plt.xlabel(\"Composante principale 1\")\n",
    "plt.ylabel(\"Composante principale 2\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd75b7e",
   "metadata": {},
   "source": [
    "The following script test the ContentBased class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b693ac3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_save_genome_files():\n",
    "    # Nettoyage de genome-scores.csv (tabulation)\n",
    "    scores_path = C.CONTENT_PATH / \"genome-scores.csv\"\n",
    "    with open(scores_path, \"r\", encoding=\"utf-8\") as f_in:\n",
    "        lines = f_in.readlines()\n",
    "    # Remplacer les tabulations par des virgules\n",
    "    lines = [line.replace(\"\\t\", \",\") for line in lines]\n",
    "    # Ajouter l'en-tête\n",
    "    lines.insert(0, \"movieId,tagId,relevance\\n\")\n",
    "    # Sauvegarder le fichier nettoyé\n",
    "    with open(C.CONTENT_PATH / \"genome-scores-clean.csv\", \"w\", encoding=\"utf-8\") as f_out:\n",
    "        f_out.writelines(lines)\n",
    "\n",
    "    # Nettoyage de genome-tags.csv (séparation correcte des colonnes)\n",
    "    tags_path = C.CONTENT_PATH / \"genome-tags.csv\"\n",
    "    with open(tags_path, \"r\", encoding=\"utf-8\") as f_in:\n",
    "        lines = f_in.readlines()\n",
    "    cleaned_lines = []\n",
    "    for line in lines:\n",
    "        parts = line.strip().split('\\t')\n",
    "        if len(parts) == 3:\n",
    "            tag_id, tag, count = parts\n",
    "            cleaned_line = f\"{tag_id},{tag},{count}\\n\"\n",
    "            cleaned_lines.append(cleaned_line)\n",
    "        else:\n",
    "            cleaned_lines.append(line)\n",
    "    # Ajouter l'en-tête\n",
    "    cleaned_lines.insert(0, \"tagId,tag,TagPopularity\\n\")\n",
    "    with open(C.CONTENT_PATH / \"genome-tags-clean.csv\", \"w\", encoding=\"utf-8\") as f_out:\n",
    "        f_out.writelines(cleaned_lines)\n",
    "\n",
    "# Appel de la fonction\n",
    "clean_and_save_genome_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d12f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prédiction pour l'utilisateur 1 et l'item 10 : 2.508\n"
     ]
    }
   ],
   "source": [
    "def test_contentbased_class(feature_method, regressor_method):\n",
    "    \"\"\"Test the ContentBased class.\n",
    "    Tries to make a prediction on the first (user,item ) tuple of the anti_test_set\n",
    "    \"\"\"\n",
    "    sp_ratings = load_ratings(surprise_format=True)\n",
    "    train_set = sp_ratings.build_full_trainset()\n",
    "    content_algo = ContentBased(feature_method, regressor_method)\n",
    "    content_algo.fit(train_set)\n",
    "    anti_test_set_first = train_set.build_anti_testset()[0]\n",
    "    prediction = content_algo.predict(anti_test_set_first[0], anti_test_set_first[1])\n",
    "    user_id = anti_test_set_first[0]\n",
    "    item_id = anti_test_set_first[1]\n",
    "    print(f\"Prédiction pour l'utilisateur {user_id} et l'item {item_id} : {prediction.est:.3f}\")\n",
    "\n",
    "\n",
    "test_contentbased_class(feature_method=\"Year_of_release\", regressor_method='gradient_boosting')\n",
    "# (call here the test functions with different regressor methods)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
