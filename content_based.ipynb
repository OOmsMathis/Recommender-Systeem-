{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82d5ca82",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "277473a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Standard library imports\n",
    "from collections import defaultdict\n",
    "import heapq\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rd\n",
    "import nltk\n",
    "\n",
    "from surprise import AlgoBase, KNNWithMeans, SVD\n",
    "from surprise.prediction_algorithms.predictions import PredictionImpossible\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Local imports\n",
    "from loaders import load_ratings, load_items\n",
    "from constants import Constant as C\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42c16bf",
   "metadata": {},
   "source": [
    "# Explore and select content features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8378976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_character_title</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movieId</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         n_character_title\n",
       "movieId                   \n",
       "1                       16\n",
       "2                       14\n",
       "3                       23\n",
       "4                       24\n",
       "5                       34"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_items = load_items()\n",
    "df_ratings = load_ratings()\n",
    "\n",
    "# Example 1 : create title_length features\n",
    "df_features = df_items[C.LABEL_COL].apply(lambda x: len(x)).to_frame('n_character_title')\n",
    "display(df_features.head())\n",
    "\n",
    "# (explore here other features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c9a2b6",
   "metadata": {},
   "source": [
    "# Build a content-based model\n",
    "When ready, move the following class in the *models.py* script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "16b0a602",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentBased(AlgoBase):\n",
    "    def __init__(self, features_methods, regressor_method): # Changé en pluriel\n",
    "        AlgoBase.__init__(self)\n",
    "        self.features_methods = features_methods  # Changé en pluriel\n",
    "        self.regressor_method = regressor_method\n",
    "        # Appel avec la variable d'instance (maintenant au pluriel)\n",
    "        self.content_features = self.create_content_features(self.features_methods)\n",
    "\n",
    "        \n",
    "\n",
    "    def create_content_features(self, features_methods):\n",
    "        \"\"\"Content Analyzer\"\"\"\n",
    "        df_items = load_items()\n",
    "        df_features = pd.DataFrame(index=df_items.index)\n",
    "        if features_methods is None:\n",
    "            df_features = pd.DataFrame(index=df_items.index)\n",
    "        if isinstance(features_methods, str):\n",
    "            features_methods = [features_methods]\n",
    "        \n",
    "        for feature_method in features_methods:\n",
    "\n",
    "            if feature_method == \"title_length\":\n",
    "                df_title_length = df_items[C.LABEL_COL].apply(lambda x: len(x)).to_frame('title_length')\n",
    "                df_title_length['title_length'] = df_title_length['title_length'].fillna(0).astype(int)\n",
    "                mean_title_length = int(df_title_length['title_length'].replace(0, np.nan).mean())\n",
    "                df_title_length.loc[df_title_length['title_length'] == 0, 'title_length'] = mean_title_length\n",
    "                title_length_min = df_title_length['title_length'].min()\n",
    "                title_length_max = df_title_length['title_length'].max()\n",
    "                df_title_length['title_length'] = (df_title_length['title_length'] - title_length_min) / (title_length_max - title_length_min)\n",
    "                df_features = pd.concat([df_features, df_title_length], axis=1)\n",
    "\n",
    "            elif feature_method == \"Year_of_release\":\n",
    "                year = df_items[C.LABEL_COL].str.extract(r'\\((\\d{4})\\)')[0].astype(float)\n",
    "                df_year = year.to_frame(name='year_of_release')\n",
    "                mean_year = df_year.replace(0, np.nan).mean().iloc[0]\n",
    "                df_year['year_of_release'] = df_year['year_of_release'].fillna(mean_year).astype(int)\n",
    "                year_min = df_year['year_of_release'].min()\n",
    "                year_max = df_year['year_of_release'].max()\n",
    "                df_year['year_of_release'] = (df_year['year_of_release'] - year_min) / (year_max - year_min)\n",
    "                df_features = pd.concat([df_features, df_year], axis=1)\n",
    "\n",
    "            elif feature_method == \"average_ratings\":\n",
    "                average_rating = df_ratings.groupby('movieId')[C.RATING_COL].mean().rename('average_rating').to_frame()\n",
    "                global_avg = df_ratings[C.RATING_COL].mean()\n",
    "                average_rating['average_rating'] = average_rating['average_rating'].fillna(global_avg)\n",
    "                avg_rating_min = average_rating['average_rating'].min()\n",
    "                avg_rating_max = average_rating['average_rating'].max()\n",
    "                average_rating['average_rating'] = (average_rating['average_rating'] - avg_rating_min) / (avg_rating_max - avg_rating_min)\n",
    "                df_features = df_features.join(average_rating, how='left')\n",
    "\n",
    "            elif feature_method == \"count_ratings\":\n",
    "                rating_count = df_ratings.groupby('movieId')[C.RATING_COL].size().rename('rating_count').to_frame()\n",
    "                rating_count['rating_count'] = rating_count['rating_count'].fillna(0).astype(int)\n",
    "                mean_rating_count = int(rating_count['rating_count'].replace(0, np.nan).mean())\n",
    "                rating_count.loc[rating_count['rating_count'] == 0, 'rating_count'] = mean_rating_count\n",
    "                rating_count_min = rating_count['rating_count'].min()\n",
    "                rating_count_max = rating_count['rating_count'].max()\n",
    "                rating_count['rating_count'] = (rating_count['rating_count'] - rating_count_min) / (rating_count_max - rating_count_min)\n",
    "                df_features = df_features.join(rating_count, how='left')\n",
    "\n",
    "            elif feature_method == \"Genre_binary\":\n",
    "                df_genre_list = df_items[C.GENRES_COL].str.split('|').explode().to_frame('genre_list')\n",
    "                df_dummies = pd.get_dummies(df_genre_list['genre_list'])\n",
    "                df_genres = df_dummies.groupby(df_genre_list.index).sum()\n",
    "                df_genres = df_genres.reindex(df_items.index).fillna(0).astype(int)\n",
    "                df_features = pd.concat([df_features, df_genres], axis=1)\n",
    "\n",
    "            elif feature_method == \"Genre_tfidf\":\n",
    "                df_items['genre_string'] = df_items[C.GENRES_COL].fillna('').str.replace('|', ' ')\n",
    "                tfidf = TfidfVectorizer()\n",
    "                tfidf_matrix = tfidf.fit_transform(df_items['genre_string'])\n",
    "                tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), index=df_items.index, columns=tfidf.get_feature_names_out())\n",
    "                df_features = pd.concat([df_features, tfidf_df], axis=1)\n",
    "\n",
    "            elif feature_method == \"Tags\":\n",
    "                tags_path = str(C.CONTENT_PATH / \"tags.csv\")\n",
    "                df_tags = pd.read_csv(tags_path)\n",
    "                df_tags = df_tags.dropna(subset=['tag'])\n",
    "                df_tags['tag'] = df_tags['tag'].astype(str)\n",
    "                df_tags_grouped = df_tags.groupby('movieId')['tag'].agg(' '.join).to_frame('tags')\n",
    "                tfidf = TfidfVectorizer()\n",
    "                tfidf_matrix = tfidf.fit_transform(df_tags_grouped['tags'])\n",
    "                tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), index=df_tags_grouped.index, columns=tfidf.get_feature_names_out())\n",
    "                df_features = pd.concat([df_features, tfidf_df], axis=1)\n",
    "\n",
    "            elif feature_method == \"tmdb_vote_average\":\n",
    "                tmdb_path = str(C.CONTENT_PATH / \"tmdb_full_features.csv\")\n",
    "                df_tmdb = pd.read_csv(tmdb_path)\n",
    "                df_tmdb = df_tmdb[['movieId', 'vote_average']].drop_duplicates('movieId')\n",
    "                df_tmdb = df_tmdb.set_index('movieId')\n",
    "                mean_vote = df_tmdb['vote_average'].mean()\n",
    "                df_tmdb['vote_average'] = df_tmdb['vote_average'].fillna(mean_vote)\n",
    "                min_vote = df_tmdb['vote_average'].min()\n",
    "                max_vote = df_tmdb['vote_average'].max()\n",
    "                df_tmdb['vote_average'] = (df_tmdb['vote_average'] - min_vote) / (max_vote - min_vote)\n",
    "                df_features = df_features.join(df_tmdb, how='left')\n",
    "                    \n",
    "            elif feature_method == \"title_tfidf\":\n",
    "                # Combine titles into a single string per item\n",
    "                df_items['title_string'] = df_items[C.LABEL_COL].fillna('')\n",
    "                tfidf = TfidfVectorizer()\n",
    "                tfidf_matrix = tfidf.fit_transform(df_items['title_string'])\n",
    "                tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), index=df_items.index, columns=tfidf.get_feature_names_out())\n",
    "                nltk.download('stopwords')\n",
    "                nltk.download('wordnet')\n",
    "                nltk.download('omw-1.4')\n",
    "                lemmatizer = WordNetLemmatizer()\n",
    "                stop_words = set(stopwords.words('english'))\n",
    "                # Preprocess titles: remove stopwords and apply lemmatization\n",
    "                df_items['title_string'] = df_items[C.LABEL_COL].fillna('').apply(lambda x: ' '.join(\n",
    "                        lemmatizer.lemmatize(word) for word in x.split() if word.lower() not in stop_words\n",
    "                    )\n",
    "                )\n",
    "                tfidf = TfidfVectorizer()\n",
    "                tfidf_matrix = tfidf.fit_transform(df_items['title_string'])\n",
    "                tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), index=df_items.index, columns=tfidf.get_feature_names_out())\n",
    "                df_features = pd.concat([df_features, tfidf_df], axis=1)\n",
    "            \n",
    "            elif feature_method == \"genome_tags\":\n",
    "                tags_path = C.CONTENT_PATH / \"genome-tags.csv\"\n",
    "                scores_path = C.CONTENT_PATH / \"genome-scores.csv\"\n",
    "\n",
    "                df_scores = pd.read_csv(scores_path)\n",
    "                df_tags = pd.read_csv(tags_path)\n",
    "                # Étape 2 : Merge pour récupérer les noms des tags\n",
    "                df_merged = df_scores.merge(df_tags, on='tagId')\n",
    "                # Étape 3 : Pivot → films × tags, valeurs = relevance\n",
    "                df_features = df_merged.pivot_table(index='movieId', columns='tag', values='relevance', fill_value=0)\n",
    "       \n",
    "            elif feature_method == \"tfidf_relevance\":\n",
    "                tags_path = C.CONTENT_PATH / \"genome-tags.csv\"\n",
    "                scores_path = C.CONTENT_PATH / \"genome-scores.csv\"\n",
    "                # Charger les données\n",
    "                df_tags = pd.read_csv(tags_path)\n",
    "                df_scores = pd.read_csv(scores_path)\n",
    "                # Fusionner pour obtenir les noms des tags\n",
    "                df_merged = df_scores.merge(df_tags, on='tagId')\n",
    "                # Grouper les tags pertinents par film en texte\n",
    "                df_merged['tag'] = df_merged['tag'].astype(str)\n",
    "                df_texts = df_merged.groupby('movieId')['tag'].apply(lambda x: ' '.join(x)).to_frame('tags')\n",
    "                # Appliquer TF-IDF\n",
    "                tfidf = TfidfVectorizer()\n",
    "                tfidf_matrix = tfidf.fit_transform(df_texts['tags'])\n",
    "                # Créer le DataFrame final de features\n",
    "                df_features = pd.DataFrame(tfidf_matrix.toarray(), index=df_texts.index, columns=tfidf.get_feature_names_out())\n",
    "                \n",
    "            elif feature_method == \"tmdb_popularity\":\n",
    "                tmdb_path = C.CONTENT_PATH / \"tmdb_full_features.csv\"\n",
    "                df_tmdb = pd.read_csv(tmdb_path)\n",
    "                df_tmdb = df_tmdb[['movieId', 'popularity']].drop_duplicates('movieId')\n",
    "                df_tmdb = df_tmdb.set_index('movieId')\n",
    "                mean_popularity = df_tmdb['popularity'].mean()\n",
    "                df_tmdb['popularity'] = df_tmdb['popularity'].fillna(mean_popularity)\n",
    "                min_popularity = df_tmdb['popularity'].min()\n",
    "                max_popularity = df_tmdb['popularity'].max()\n",
    "                df_tmdb['popularity'] = (df_tmdb['popularity'] - min_popularity) / (max_popularity - min_popularity)\n",
    "                df_features = df_features.join(df_tmdb, how='left')\n",
    "\n",
    "\n",
    "            elif feature_method == \"tmdb_budget\":\n",
    "                tmdb_path = C.CONTENT_PATH / \"tmdb_full_features.csv\"\n",
    "                df_tmdb = pd.read_csv(tmdb_path)\n",
    "                df_tmdb = df_tmdb[['movieId', 'budget']].drop_duplicates('movieId')\n",
    "                df_tmdb = df_tmdb.set_index('movieId')\n",
    "                mean_budget = df_tmdb['budget'].mean()\n",
    "                df_tmdb['budget'] = df_tmdb['budget'].fillna(mean_budget)\n",
    "                min_budget = df_tmdb['budget'].min()\n",
    "                max_budget = df_tmdb['budget'].max()\n",
    "                df_tmdb['budget'] = (df_tmdb['budget'] - min_budget) / (max_budget - min_budget)\n",
    "                df_features = df_features.join(df_tmdb, how='left')\n",
    "           \n",
    "            elif feature_method == \"tmdb_revenue\":\n",
    "                tmdb_path = C.CONTENT_PATH / \"tmdb_full_features.csv\"\n",
    "                df_tmdb = pd.read_csv(tmdb_path)\n",
    "                df_tmdb = df_tmdb[['movieId', 'revenue']].drop_duplicates('movieId')\n",
    "                df_tmdb = df_tmdb.set_index('movieId')\n",
    "                mean_revenue = df_tmdb['revenue'].mean()\n",
    "                df_tmdb['revenue'] = df_tmdb['revenue'].fillna(mean_revenue)\n",
    "                min_revenue = df_tmdb['revenue'].min()\n",
    "                max_revenue = df_tmdb['revenue'].max()\n",
    "                df_tmdb['revenue'] = (df_tmdb['revenue'] - min_revenue) / (max_revenue - min_revenue)\n",
    "                df_features = df_features.join(df_tmdb, how='left')\n",
    "           \n",
    "            elif feature_method == \"tmdb_profit\":\n",
    "                tmdb_path = C.CONTENT_PATH / \"tmdb_full_features.csv\"\n",
    "                df_tmdb = pd.read_csv(tmdb_path)\n",
    "                df_tmdb['profit'] = df_tmdb['revenue'] - df_tmdb['budget']\n",
    "                df_tmdb = df_tmdb[['movieId', 'profit']].drop_duplicates('movieId')\n",
    "                df_tmdb = df_tmdb.set_index('movieId')\n",
    "                mean_profit = df_tmdb['profit'].mean()\n",
    "                df_tmdb['profit'] = df_tmdb['profit'].fillna(mean_profit)\n",
    "                min_profit = df_tmdb['profit'].min()\n",
    "                max_profit = df_tmdb['profit'].max()\n",
    "                df_tmdb['profit'] = (df_tmdb['profit'] - min_profit) / (max_profit - min_profit)\n",
    "                df_features = df_features.join(df_tmdb, how='left')\n",
    "           \n",
    "            elif feature_method == \"tmdb_runtime\":\n",
    "                tmdb_path = C.CONTENT_PATH / \"tmdb_full_features.csv\"\n",
    "                df_tmdb = pd.read_csv(tmdb_path)\n",
    "                df_tmdb = df_tmdb[['movieId', 'runtime']].drop_duplicates('movieId')\n",
    "                df_tmdb = df_tmdb.set_index('movieId')\n",
    "                mean_runtime = df_tmdb['runtime'].mean()\n",
    "                df_tmdb['runtime'] = df_tmdb['runtime'].fillna(mean_runtime)\n",
    "                min_runtime = df_tmdb['runtime'].min()\n",
    "                max_runtime = df_tmdb['runtime'].max()\n",
    "                df_tmdb['runtime'] = (df_tmdb['runtime'] - min_runtime) / (max_runtime - min_runtime)\n",
    "                df_features = df_features.join(df_tmdb, how='left')\n",
    "           \n",
    "            elif feature_method == \"tmdb_vote_count\":\n",
    "                tmdb_path = C.CONTENT_PATH / \"tmdb_full_features.csv\"\n",
    "                df_tmdb = pd.read_csv(tmdb_path)\n",
    "                df_tmdb = df_tmdb[['movieId', 'vote_count']].drop_duplicates('movieId')\n",
    "                df_tmdb = df_tmdb.set_index('movieId')\n",
    "                mean_vote_count = df_tmdb['vote_count'].mean()\n",
    "                df_tmdb['vote_count'] = df_tmdb['vote_count'].fillna(mean_vote_count)\n",
    "                min_vote_count = df_tmdb['vote_count'].min()\n",
    "                max_vote_count = df_tmdb['vote_count'].max()\n",
    "                df_tmdb['vote_count'] = (df_tmdb['vote_count'] - min_vote_count) / (max_vote_count - min_vote_count)\n",
    "                df_features = df_features.join(df_tmdb, how='left')\n",
    "           \n",
    "            elif feature_method == \"tmdb_cast\":\n",
    "                tmdb_path = C.CONTENT_PATH / \"tmdb_full_features.csv\"\n",
    "                df_tmdb = pd.read_csv(tmdb_path)\n",
    "                df_tmdb = df_tmdb[['movieId', 'cast']].drop_duplicates('movieId')\n",
    "                df_tmdb['cast'] = df_tmdb['cast'].fillna('')\n",
    "                tfidf = TfidfVectorizer()\n",
    "                tfidf_matrix = tfidf.fit_transform(df_tmdb['cast'])\n",
    "                tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), index=df_tmdb['movieId'], columns=tfidf.get_feature_names_out())\n",
    "                df_features = pd.concat([df_features, tfidf_df], axis=1)\n",
    "           \n",
    "            elif feature_method == \"tmdb_director\":\n",
    "                tmdb_path = C.CONTENT_PATH / \"tmdb_full_features.csv\"\n",
    "                df_tmdb = pd.read_csv(tmdb_path)\n",
    "                df_tmdb = df_tmdb[['movieId', 'director']].drop_duplicates('movieId')\n",
    "                df_tmdb['director'] = df_tmdb['director'].fillna('')\n",
    "                tfidf = TfidfVectorizer()\n",
    "                tfidf_matrix = tfidf.fit_transform(df_tmdb['director'])\n",
    "                tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), index=df_tmdb['movieId'], columns=tfidf.get_feature_names_out())\n",
    "                df_features = pd.concat([df_features, tfidf_df], axis=1)\n",
    "\n",
    "\n",
    "            elif feature_method == \"tmdb_original_language\":\n",
    "                tmdb_path = C.CONTENT_PATH / \"tmdb_full_features.csv\"\n",
    "                df_tmdb = pd.read_csv(tmdb_path)\n",
    "                df_tmdb = df_tmdb[['movieId', 'original_language']].drop_duplicates('movieId')\n",
    "                df_tmdb = df_tmdb.set_index('movieId')\n",
    "                df_tmdb['original_language'] = df_tmdb['original_language'].fillna('unknown')\n",
    "                # One-hot encoding des langues\n",
    "                df_lang_dummies = pd.get_dummies(df_tmdb['original_language'], prefix='lang')\n",
    "                # Gérer les valeurs manquantes après le merge\n",
    "                df_lang_dummies = df_lang_dummies.reindex(df_features.index, fill_value=0)\n",
    "                df_features = pd.concat([df_features, df_lang_dummies], axis=1)\n",
    "\n",
    "\n",
    "            else:\n",
    "                raise NotImplementedError(f'Feature method {feature_method} not yet implemented')\n",
    "        return df_features\n",
    "    \n",
    "\n",
    "    def fit(self, trainset):\n",
    "        \"\"\"Profile Learner\"\"\"\n",
    "        self.content_features = self.create_content_features(self.features_methods)\n",
    "        AlgoBase.fit(self, trainset)\n",
    "        self.user_profile = {u: None for u in trainset.all_users()}\n",
    "        for u in self.user_profile:\n",
    "            user_items = trainset.ur[u]\n",
    "            if len(user_items) > 0:\n",
    "                # Sépare les item_ids internes et les notes\n",
    "                user_ratings = self.trainset.ur[u]\n",
    "                df_user = pd.DataFrame(user_ratings, columns=['inner_item_id', 'user_ratings'])\n",
    "                # Conversion des item_id internes (Surprise) en item_id \"raw\" (MovieLens)\n",
    "                df_user[\"item_id\"] = df_user[\"inner_item_id\"].map(self.trainset.to_raw_iid)\n",
    "                # Fusion avec les features de contenu (sur l'index = item_id raw)\n",
    "                df_user = df_user.merge(self.content_features, how='left', left_on='item_id', right_index=True)\n",
    "                # Préparation des features et des cibles pour l'entraînement\n",
    "                feature_names = list(self.content_features.columns)\n",
    "                X = df_user[feature_names].values\n",
    "                y = df_user['user_ratings'].values\n",
    "                # Gère les NaNs dans les features\n",
    "                X = np.nan_to_num(X)\n",
    "\n",
    "     \n",
    "                if self.regressor_method == 'linear': # Use linear regression\n",
    "                    model = LinearRegression(fit_intercept=True)\n",
    "                elif self.regressor_method == 'lasso':\n",
    "                    model = Lasso(alpha=0.1)\n",
    "                elif self.regressor_method == 'random_forest':\n",
    "                    model = RandomForestRegressor(n_estimators=10, max_depth=10, random_state=42)\n",
    "                elif self.regressor_method== 'neural_network':\n",
    "                    model = MLPRegressor(hidden_layer_sizes=(60, 60), max_iter=2500, learning_rate_init=0.01, alpha=0.0001, random_state=42)\n",
    "                elif self.regressor_method == 'decision_tree':\n",
    "                    model = DecisionTreeRegressor(max_depth=10, random_state=42)\n",
    "                elif self.regressor_method == 'ridge':\n",
    "                    model = Ridge(alpha=1.0)\n",
    "                elif self.regressor_method == 'gradient_boosting':\n",
    "                    model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "                elif  self.regressor_method == 'knn':\n",
    "                    model = KNeighborsRegressor(n_neighbors=5)\n",
    "                elif self.regressor_method == 'elastic_net':\n",
    "                    model = ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
    "\n",
    "                else:\n",
    "                    self.user_profile[u] = None\n",
    "                    \n",
    "                model.fit(X, y)\n",
    "                self.user_profile[u] = model\n",
    "\n",
    "            else:\n",
    "             self.user_profile[u] = None\n",
    "             \n",
    "        \n",
    "    def estimate(self, u, i):\n",
    "        \"\"\"Scoring component used for item filtering\"\"\"\n",
    "        # First, handle cases for unknown users and items\n",
    "        if not (self.trainset.knows_user(u) and self.trainset.knows_item(i)):\n",
    "            raise PredictionImpossible('User and/or item is unkown.')\n",
    "\n",
    "        if self.user_profile[u] is None:\n",
    "            return self.trainset.global_mean\n",
    "\n",
    "        raw_item_id = self.trainset.to_raw_iid(i)\n",
    "        if raw_item_id in self.content_features.index:\n",
    "            item_features = self.content_features.loc[raw_item_id].values.reshape(1, -1)\n",
    "        else:\n",
    "            return self.trainset.global_mean\n",
    "    \n",
    "        if self.regressor_method == 'linear':\n",
    "            score = self.user_profile[u].predict(item_features)[0]\n",
    "        elif self.regressor_method in [\n",
    "        'linear',\n",
    "        'lasso',\n",
    "        'random_forest',\n",
    "        'neural_network',\n",
    "        'decision_tree',\n",
    "        'ridge',\n",
    "        'gradient_boosting',\n",
    "        'knn',\n",
    "        'elastic_net' ]:\n",
    "          score = self.user_profile[u].predict(item_features)[0]\n",
    "\n",
    "        else:\n",
    "            score=None\n",
    "            \n",
    "\n",
    "        return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3e06d9db",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'mlsmm2156\\\\data\\\\small\\\\content\\\\genome-scores.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m features_methods \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenome_tags\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Créer une instance temporaire de ContentBased (le regressor_method n'a pas d'importance ici)\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m content_based_tmp \u001b[38;5;241m=\u001b[39m \u001b[43mContentBased\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures_methods\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregressor_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlinear\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m features_df \u001b[38;5;241m=\u001b[39m content_based_tmp\u001b[38;5;241m.\u001b[39mcreate_content_features(features_methods)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# On sélectionne les 50 premiers films pour la projection\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[24], line 7\u001b[0m, in \u001b[0;36mContentBased.__init__\u001b[1;34m(self, features_methods, regressor_method)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregressor_method \u001b[38;5;241m=\u001b[39m regressor_method\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Appel avec la variable d'instance (maintenant au pluriel)\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontent_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_content_features\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures_methods\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[24], line 123\u001b[0m, in \u001b[0;36mContentBased.create_content_features\u001b[1;34m(self, features_methods)\u001b[0m\n\u001b[0;32m    120\u001b[0m tags_path \u001b[38;5;241m=\u001b[39m C\u001b[38;5;241m.\u001b[39mCONTENT_PATH \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenome-tags.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    121\u001b[0m scores_path \u001b[38;5;241m=\u001b[39m C\u001b[38;5;241m.\u001b[39mCONTENT_PATH \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenome-scores.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 123\u001b[0m df_scores \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscores_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    124\u001b[0m df_tags \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(tags_path)\n\u001b[0;32m    125\u001b[0m \u001b[38;5;66;03m# Étape 2 : Merge pour récupérer les noms des tags\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mathi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mathi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\mathi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mathi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\mathi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'mlsmm2156\\\\data\\\\small\\\\content\\\\genome-scores.csv'"
     ]
    }
   ],
   "source": [
    "# features_methods = [\"title_length\", \"Year_of_release\", \"Genre_binary\", ...]  # à adapter selon ton besoin\n",
    "features_methods = [\"genome_tags\"]\n",
    "\n",
    "# Créer une instance temporaire de ContentBased (le regressor_method n'a pas d'importance ici)\n",
    "content_based_tmp = ContentBased(features_methods, regressor_method=\"linear\")\n",
    "features_df = content_based_tmp.create_content_features(features_methods)\n",
    "\n",
    "# On sélectionne les 50 premiers films pour la projection\n",
    "features_50 = features_df.loc[df_50.index].fillna(0)\n",
    "\n",
    "# PCA sur les features sélectionnées\n",
    "pca = PCA(n_components=2)\n",
    "features_2d = pca.fit_transform(features_50)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(features_2d[:, 0], features_2d[:, 1])\n",
    "\n",
    "for i, idx in enumerate(df_50.index):\n",
    "    plt.plot([0, features_2d[i, 0]], [0, features_2d[i, 1]], color='gray', alpha=0.5)\n",
    "    plt.text(features_2d[i, 0], features_2d[i, 1], df_50.loc[idx, 'title'].split(' (')[0], fontsize=8)\n",
    "\n",
    "plt.title(\"Projection 2D des 50 premiers films selon les features sélectionnées\")\n",
    "plt.xlabel(\"Composante principale 1\")\n",
    "plt.ylabel(\"Composante principale 2\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd75b7e",
   "metadata": {},
   "source": [
    "The following script test the ContentBased class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69d12f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prédiction pour l'utilisateur 1 et l'item 10 : 2.508\n"
     ]
    }
   ],
   "source": [
    "def test_contentbased_class(feature_method, regressor_method):\n",
    "    \"\"\"Test the ContentBased class.\n",
    "    Tries to make a prediction on the first (user,item ) tuple of the anti_test_set\n",
    "    \"\"\"\n",
    "    sp_ratings = load_ratings(surprise_format=True)\n",
    "    train_set = sp_ratings.build_full_trainset()\n",
    "    content_algo = ContentBased(feature_method, regressor_method)\n",
    "    content_algo.fit(train_set)\n",
    "    anti_test_set_first = train_set.build_anti_testset()[0]\n",
    "    prediction = content_algo.predict(anti_test_set_first[0], anti_test_set_first[1])\n",
    "    user_id = anti_test_set_first[0]\n",
    "    item_id = anti_test_set_first[1]\n",
    "    print(f\"Prédiction pour l'utilisateur {user_id} et l'item {item_id} : {prediction.est:.3f}\")\n",
    "\n",
    "\n",
    "test_contentbased_class(feature_method=\"Year_of_release\", regressor_method='gradient_boosting')\n",
    "# (call here the test functions with different regressor methods)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
