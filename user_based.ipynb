{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4a8f664",
   "metadata": {},
   "source": [
    "# Custom User-based Model\n",
    "The present notebooks aims at creating a UserBased class that inherits from the Algobase class (surprise package) and that can be customized with various similarity metrics, peer groups and score aggregation functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00d1b249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reloads modules automatically before entering the execution of code\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# standard library imports\n",
    "# -- add new imports here --\n",
    "\n",
    "# third parties imports\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from surprise import AlgoBase\n",
    "# -- add new imports here --\n",
    "\n",
    "# local imports\n",
    "from constants import Constant as C\n",
    "from loaders import load_ratings\n",
    "# -- add new imports here --\n",
    "from surprise import Dataset, KNNWithMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "###\n",
    "import heapq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22716aa3",
   "metadata": {},
   "source": [
    "# 1. Loading Data\n",
    "Prepare a dataset in order to help implementing a user-based recommender system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf3ccdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- load data, build trainset and anti testset --\n",
    "ratings = load_ratings(surprise_format=True)\n",
    "trainset = ratings.build_full_trainset()\n",
    "anti_testset = trainset.build_anti_testset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94adf3a6",
   "metadata": {},
   "source": [
    "# 2. Explore Surprise's user-based algorithm\n",
    "Displays user-based predictions and similarity matrix on the test dataset using the KNNWithMeans class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e6fb78b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Prédiction pour l'utilisateur 11 et l'élément 364 : 4.252920516369196\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 16\u001b[0m\n\u001b[1;32m     12\u001b[0m prediction \u001b[38;5;241m=\u001b[39m algo\u001b[38;5;241m.\u001b[39mpredict(uid\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m11\u001b[39m, iid\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m364\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrédiction pour l\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutilisateur 11 et l\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mélément 364 : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprediction\u001b[38;5;241m.\u001b[39mest\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43malgo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43manti_testset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pred \u001b[38;5;129;01min\u001b[39;00m predictions[:\u001b[38;5;241m30\u001b[39m]:\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUtilisateur \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpred\u001b[38;5;241m.\u001b[39muid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m a évalué l\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mélément \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpred\u001b[38;5;241m.\u001b[39miid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m avec une note de \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpred\u001b[38;5;241m.\u001b[39mest\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (réel : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpred\u001b[38;5;241m.\u001b[39mr_ui\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, actual_k = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpred\u001b[38;5;241m.\u001b[39mdetails\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactual_k\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mN/A\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/surprise/prediction_algorithms/algo_base.py:160\u001b[0m, in \u001b[0;36mAlgoBase.test\u001b[0;34m(self, testset, verbose)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Test the algorithm on given testset, i.e. estimate all the ratings\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;124;03min the given testset.\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;124;03m    that contains all the estimated ratings.\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# The ratings are translated back to their original scale.\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m predictions \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(uid, iid, r_ui_trans, verbose\u001b[38;5;241m=\u001b[39mverbose)\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (uid, iid, r_ui_trans) \u001b[38;5;129;01min\u001b[39;00m testset\n\u001b[1;32m    163\u001b[0m ]\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m predictions\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/surprise/prediction_algorithms/algo_base.py:161\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Test the algorithm on given testset, i.e. estimate all the ratings\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;124;03min the given testset.\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;124;03m    that contains all the estimated ratings.\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# The ratings are translated back to their original scale.\u001b[39;00m\n\u001b[1;32m    160\u001b[0m predictions \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 161\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr_ui_trans\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (uid, iid, r_ui_trans) \u001b[38;5;129;01min\u001b[39;00m testset\n\u001b[1;32m    163\u001b[0m ]\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m predictions\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/surprise/prediction_algorithms/algo_base.py:96\u001b[0m, in \u001b[0;36mAlgoBase.predict\u001b[0;34m(self, uid, iid, r_ui, clip, verbose)\u001b[0m\n\u001b[1;32m     94\u001b[0m     iuid \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUKN__\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(uid)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     iiid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_inner_iid\u001b[49m\u001b[43m(\u001b[49m\u001b[43miid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m     98\u001b[0m     iiid \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUKN__\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(iid)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/surprise/trainset.py:155\u001b[0m, in \u001b[0;36mTrainset.to_inner_iid\u001b[0;34m(self, riid)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Convert an **item** raw id to an inner id.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \n\u001b[1;32m    142\u001b[0m \u001b[38;5;124;03mSee :ref:`this note<raw_inner_note>`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;124;03m    ValueError: When item is not part of the trainset.\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw2inner_id_items\u001b[49m\u001b[43m[\u001b[49m\u001b[43mriid\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mItem \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(riid) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not part of the trainset.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# -- using surprise's user-based algorithm, explore the impact of different parameters and displays predictions --\n",
    "sim_options = {\n",
    "    'name': 'msd',  # Mean Squared Difference\n",
    "    'user_based': True,  # Modèle basé sur les utilisateurs\n",
    "    'min_support': 3  # Minimum de 3 évaluations communes\n",
    "}\n",
    "# Créer une instance de KNNWithMeans\n",
    "algo = KNNWithMeans(k=3, min_k=2, sim_options=sim_options)\n",
    "# Entraîner le modèle\n",
    "algo.fit(trainset)\n",
    "# Faire une prédiction pour l'utilisateur 11 et l'élément 364\n",
    "prediction = algo.predict(uid=11, iid=364)\n",
    "print(f\"Prédiction pour l'utilisateur 11 et l'élément 364 : {prediction.est}\")\n",
    "\n",
    "\n",
    "predictions = algo.test(anti_testset)\n",
    "for pred in predictions[:30]:\n",
    "    print(f\"Utilisateur {pred.uid} a évalué l'élément {pred.iid} avec une note de {pred.est:.2f} (réel : {pred.r_ui}, actual_k = {pred.details.get('actual_k', 'N/A')})\")\n",
    "#1.La valeur de min_support est fixée à 3, ce qui signifie que pour qu'un voisin soit pris en compte dans le calcul de la prédiction il doit avoir au moins 3 évaluations communes avec l'utilisateur cible. Cela peut réduire le nombre de voisins valides et donc influencer la prédiction finale.\n",
    "#2.Quand min_support est fixé à 3, la valeur de actual_k diminue pour certaines prédictions. Cela est dû au fait que actual_k représente le nombre de voisins qui ont réellement été utilisés pour calculer la prédiction et l'augmentation de min_support réduit le nombre de voisins valides\n",
    "\n",
    "# Afficher une partie de la matrice de similarité (exemple pour les 10 premiers utilisateurs)\n",
    "#print(\"\\n--- Aperçu de la matrice de similarité utilisateur-utilisateur ---\")\n",
    "#sim_matrix = algo.sim  # Matrice numpy carrée (n_users x n_users)\n",
    "\n",
    "# Afficher une partie de la matrice de similarité (exemple pour les 10 premiers utilisateurs)\n",
    "#print(\"\\n--- Aperçu de la matrice de similarité utilisateur-utilisateur ---\")\n",
    "#sim_matrix = algo.sim  # Matrice numpy carrée (n_users x n_users)\n",
    "#n_max = min(10, sim_matrix.shape[0])  # On ne dépasse pas la taille réelle\n",
    "#for i in range(n_max):\n",
    " #   print(f\"Similarités de l'utilisateur interne {i} avec les autres : {sim_matrix[i, :n_max]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd7f18c",
   "metadata": {},
   "source": [
    "observations minK:\n",
    "\n",
    "- observations minK 2 par rapport à 1 sont similaire ou légerement plus elevé :Cela s'explique par le fait que l’algorithme impose une faible  contraintes sur le nombre minimum de voisins requis pour effectuer une prédiction. Avec des valeurs faibles de min_k, comme 1 ou 2, les prédictions peuvent être faites même avec très peu de voisins ce qui explique la faible variation entre les deux cas.\n",
    "Cependant, pour certaines prédictions où le nombre de voisins disponibles est inférieur à 2, l’algorithme utilise une valeur par défaut comme la moyenne des évaluations de l’utilisateur ou la moyenne globale.\n",
    "\n",
    "- observation mink= 3: on augmente le nombre minimum de voisins requis pour faire une prédiction ce qui rend l’algorithme plus strict.\n",
    "Dans les 30 premières prédictions observées la majorité des valeurs restent similaires mais plusieurs prédictions augmentent, notamment celle de l'utilisateur 11 pour l’élément 364. Cette hausse est due au fait qu’il n’y avait pas assez de voisins (moins de 3) pour faire une prédiction personnalisée. L’algorithme a donc utilisé une valeur par défaut par exemple (moyenne des évaluations de l'utilisateur ou la moyenne globale) ce qui peut expliquer l’augmentation.\n",
    "\n",
    "observations min support: \n",
    "\n",
    "- Plus la valeur de min_support est élevée, plus actual_k a tendance à diminuer.Cela s'explique par le fait qu’un min_support plus grand impose un critère plus strict : chaque voisin doit avoir au moins ce nombre d’évaluations communes avec l’utilisateur cible (par exemple, min_support = 3 signifie au moins 3 évaluations communes).Ainsi, moins de voisins respectent cette condition  ce qui réduit le nombre de voisins utilisables pour la prédiction et donc reduit le actual_k. \n",
    "\n",
    "- La variable actual_k représente le nombre de voisins qui ont réellement contribué à la prédiction.Plus actual_k est élevé, plus la prédiction est considérée comme fiable, car elle repose sur un plus grand nombre d’avis pertinents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd01f5b",
   "metadata": {},
   "source": [
    "# 3. Implement and explore a customizable user-based algorithm\n",
    "Create a self-made user-based algorithm allowing to customize the similarity metric, peer group calculation and aggregation function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6de27c",
   "metadata": {},
   "source": [
    "changement de sort a heapq "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52b9a4a",
   "metadata": {},
   "source": [
    "erreur dans le calcul du msd (trouver avec chat l'erreur de calcul)\n",
    "\n",
    "remplacement par np.nan  dans estimate \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c2b1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserBased(AlgoBase):\n",
    "    def __init__(self, k=3, min_k=1, sim_options={}, **kwargs):\n",
    "        AlgoBase.__init__(self, sim_options=sim_options, **kwargs)\n",
    "        self.k = k\n",
    "        self.min_k = min_k\n",
    "\n",
    "        \n",
    "    def fit(self, trainset):\n",
    "        AlgoBase.fit(self, trainset)\n",
    "        # Calcul de la matrice des ratings\n",
    "        self.compute_rating_matrix()\n",
    "        # Calcul de la matrice de similarité\n",
    "        self.compute_similarity_matrix()\n",
    "        # Calcul de la moyenne des notes par utilisateur\n",
    "        self.mean_ratings = []\n",
    "        for u in range(self.trainset.n_users):\n",
    "            user_ratings = []\n",
    "            for (_, rating) in self.trainset.ur[u]: #_ correspond à l'index de l'item\n",
    "                user_ratings.append(rating)\n",
    "            if user_ratings:\n",
    "                mean_rating = np.mean(user_ratings)\n",
    "            else:\n",
    "                mean_rating = float('nan')  # ou 0.0 si tu préfères éviter les NaN\n",
    "            self.mean_ratings.append(mean_rating)\n",
    "\n",
    "    \n",
    "    def estimate(self, u, i):\n",
    "            if not (self.trainset.knows_user(u) and self.trainset.knows_item(i)):\n",
    "                raise np.nan \n",
    "            \n",
    "            estimate = self.mean_ratings[u]\n",
    "\n",
    "            ## Obtenir les voisins de l'utilisateur u qui ont noté l'item i\n",
    "            neighbors = []\n",
    "            for (v, rating) in self.trainset.ir[i]:  \n",
    "                if v == u:\n",
    "                    continue  # ne pas se comparer à soi-même\n",
    "\n",
    "                sim_uv = self.sim[u, v]  # similarité entre u et v\n",
    "\n",
    "                if sim_uv > 0 and not np.isnan(self.ratings_matrix[v, i]):  # si la similarité est positive et que v a noté l'item i\n",
    "                    mean_v = self.mean_ratings[v]  # moyenne des notes de v\n",
    "                    neighbors.append((sim_uv, rating - mean_v))\n",
    "\n",
    "            # Trier les voisins par similarité décroissante\n",
    "            top_k_neighbors = heapq.nlargest(self.k, neighbors, key=lambda x: x[0])\n",
    "\n",
    "            # Calcul de la moyenne pondérée sur les k meilleurs voisins\n",
    "            actual_k = 0\n",
    "            weighted_sum = 0.0\n",
    "            sum_sim = 0.0\n",
    "\n",
    "            for sim, rating_diff in top_k_neighbors:\n",
    "                if actual_k == self.k:\n",
    "                    break\n",
    "                weighted_sum += sim * rating_diff\n",
    "                sum_sim += sim\n",
    "                actual_k += 1\n",
    "\n",
    "            # Vérifier si on a suffisamment de voisins\n",
    "            if actual_k >= self.min_k and sum_sim > 0:\n",
    "                estimate += weighted_sum / sum_sim\n",
    "\n",
    "            return estimate\n",
    "\n",
    "\n",
    "                            \n",
    "    def compute_rating_matrix(self):\n",
    "        # -- implement here the compute_rating_matrix function --\n",
    "        self.ratings_matrix = np.empty((self.trainset.n_users, self.trainset.n_items))\n",
    "        self.ratings_matrix[:] = np.nan\n",
    "        for u in range(self.trainset.n_users): # or each user\n",
    "            for i, rating in self.trainset.ur[u]: #for each item rated by the user\n",
    "                self.ratings_matrix[u, i] = rating\n",
    "\n",
    "    \n",
    "    def compute_similarity_matrix(self):\n",
    "        m = self.trainset.n_users\n",
    "        ratings_matrix = self.ratings_matrix\n",
    "        min_support = self.sim_options.get('min_support', 1)\n",
    "        sim_name = self.sim_options.get(\"jaccard\", \"msd\")  # valeur par défaut\n",
    "\n",
    "        # Initialiser la matrice de similarité\n",
    "        self.sim = np.eye(m)\n",
    "\n",
    "        for i in range(m):\n",
    "            for j in range(i + 1, m):  # j > i pour éviter les doublons\n",
    "                row_i = ratings_matrix[i]\n",
    "                row_j = ratings_matrix[j]\n",
    "\n",
    "                if sim_name == \"jaccard\":\n",
    "                    sim = self.jaccard_similarity(row_i, row_j)\n",
    "                    support = np.sum(~np.isnan(row_i) & ~np.isnan(row_j))\n",
    "                elif sim_name == \"msd\":\n",
    "                    diff = row_i - row_j\n",
    "                    support = np.sum(~np.isnan(diff))\n",
    "                    if support >= min_support:\n",
    "                        msd = np.nanmean((diff[~np.isnan(diff)]) ** 2)\n",
    "                        sim = 1 / (1 + msd)\n",
    "                    else:\n",
    "                        sim = 0\n",
    "                else:\n",
    "                    # Par défaut : similarité euclidienne normalisée\n",
    "                    diff = row_i - row_j\n",
    "                    support = np.sum(~np.isnan(diff))\n",
    "                    if support >= min_support:\n",
    "                        msd = np.nanmean((diff[~np.isnan(diff)]) ** 2)\n",
    "                        sim = 1 / (1 + msd)\n",
    "                    else:\n",
    "                        sim = 0\n",
    "\n",
    "                if support >= min_support:\n",
    "                    self.sim[i, j] = sim\n",
    "                    self.sim[j, i] = sim\n",
    "\n",
    "    def jaccard_similarity(self, row_i, row_j):\n",
    "        # Masques binaires : True là où il y a une note\n",
    "        mask_i = ~np.isnan(row_i)\n",
    "        mask_j = ~np.isnan(row_j)\n",
    "\n",
    "        intersection = np.sum(mask_i & mask_j)\n",
    "        union = np.sum(mask_i | mask_j)\n",
    "\n",
    "        if union == 0:\n",
    "            return 0.0\n",
    "        return intersection / union\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdc9cfe",
   "metadata": {},
   "source": [
    "# 4. Compare KNNWithMeans with UserBased\n",
    "Try to replicate KNNWithMeans with your self-made UserBased and check that outcomes are identical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "be53ae27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "  UID   IID   KNN est.  UserBased est.       Diff\n",
      "--------------------------------------------------\n",
      "   11  1214     3.1667          3.1667     0.0000\n",
      "   11   364     2.4920          2.4920     0.0000\n",
      "   11  4308     3.1667          3.1667     0.0000\n",
      "   11   527     3.8989          3.8989     0.0000\n",
      "   13  1997     2.8000          2.8000     0.0000\n",
      "   13  4993     2.8000          2.8000     0.0000\n",
      "   13  2700     2.8000          2.8000     0.0000\n",
      "   13  1721     2.8000          2.8000     0.0000\n",
      "   13   527     2.8000          2.8000     0.0000\n",
      "   17  2028     3.8125          3.8125     0.0000\n",
      "   17  4993     4.1283          4.1283     0.0000\n",
      "   17  1214     3.2500          3.2500     0.0000\n",
      "   17  4308     3.2500          3.2500     0.0000\n",
      "   19  1997     3.5000          3.5000     0.0000\n",
      "   19  2028     3.5000          3.5000     0.0000\n",
      "   19  4993     3.5000          3.5000     0.0000\n",
      "   19  5952     3.5000          3.5000     0.0000\n",
      "   19  2700     3.5000          3.5000     0.0000\n",
      "   19  1721     3.5000          3.5000     0.0000\n",
      "   19  1214     3.5000          3.5000     0.0000\n",
      "   19   364     3.5000          3.5000     0.0000\n",
      "   23  1997     2.7826          2.7826     0.0000\n",
      "   23  2700     2.3498          2.3498     0.0000\n",
      "   27  1997     4.6667          4.6667     0.0000\n",
      "   27  2028     4.6667          4.6667     0.0000\n",
      "   27  5952     4.6667          4.6667     0.0000\n",
      "   27  2700     4.6667          4.6667     0.0000\n",
      "   27  1721     4.6667          4.6667     0.0000\n",
      "   27   364     4.6667          4.6667     0.0000\n",
      "   27  4308     4.6667          4.6667     0.0000\n"
     ]
    }
   ],
   "source": [
    "# -- assert that predictions are the same with different sim_options --\n",
    "#comparaison entre requltat Kmeans et resultqts user based \n",
    "import surprise\n",
    "from surprise import accuracy\n",
    "\n",
    "# Paramètres de similarité\n",
    "sim_options = {\n",
    "    'name': 'msd',\n",
    "    'user_based': True,\n",
    "    'min_support': 3\n",
    "}\n",
    "k = 3\n",
    "min_k = 2\n",
    "\n",
    "# Ton algorithme custom\n",
    "userbased_algo = UserBased(k=k, min_k=min_k, sim_options=sim_options)\n",
    "userbased_algo.fit(trainset)\n",
    "\n",
    "# Algo officiel de surprise\n",
    "knn_algo = KNNWithMeans(k=k, min_k=min_k, sim_options=sim_options)\n",
    "knn_algo.fit(trainset)\n",
    "\n",
    "# Anti-testset (les notes absentes dans le trainset)\n",
    "anti_testset = trainset.build_anti_testset()\n",
    "\n",
    "# Comparer les 30 premières prédictions\n",
    "print(f\"{'UID':>5} {'IID':>5} {'KNN est.':>10} {'UserBased est.':>15} {'Diff':>10}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for i, (uid, iid, _) in enumerate(anti_testset[:30]):\n",
    "    pred_knn = knn_algo.predict(uid, iid)\n",
    "    pred_userbased = userbased_algo.predict(uid, iid)\n",
    "\n",
    "    diff = abs(pred_knn.est - pred_userbased.est)\n",
    "\n",
    "    print(f\"{uid:>5} {iid:>5} {pred_knn.est:10.4f} {pred_userbased.est:15.4f} {diff:10.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cced76d9",
   "metadata": {},
   "source": [
    "# 5. Compare MSD and Jacard\n",
    "Compare predictions made with MSD similarity and Jacard similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20d8e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import surprise\n",
    "from surprise import accuracy\n",
    "\n",
    "# Paramètres de similarité\n",
    "sim_options_userbased = {\n",
    "    'name': 'jaccard',\n",
    "    'user_based': True,\n",
    "    'min_support': 3\n",
    "}\n",
    "# Paramètres de similarité\n",
    "sim_options_userbased2 = {\n",
    "    'name': 'msd',\n",
    "    'user_based': True,\n",
    "    'min_support': 3\n",
    "}\n",
    "k = 3\n",
    "min_k = 2\n",
    "\n",
    "# Ton algorithme custom\n",
    "userbased_algo = UserBased(k=k, min_k=min_k, sim_options=sim_options_userbased)\n",
    "userbased_algo.fit(trainset)\n",
    "\n",
    "# Algo officiel de surprise\n",
    "userbased_algo2 = UserBased(k=k, min_k=min_k, sim_options=sim_options_userbased2)\n",
    "userbased_algo2.fit(trainset)\n",
    "\n",
    "# Anti-testset (les notes absentes dans le trainset)\n",
    "anti_testset = trainset.build_anti_testset()\n",
    "\n",
    "# Comparer les 30 premières prédictions\n",
    "print(f\"{'UID':>5} {'IID':>5} {'userbased_algo2 est.':>10} {'UserBased est.':>15} {'Diff':>10}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for i, (uid, iid, _) in enumerate(anti_testset[:30]):\n",
    "    pred_userbased_algo2 = userbased_algo2.predict(uid, iid)\n",
    "    pred_userbased = userbased_algo.predict(uid, iid)\n",
    "\n",
    "    diff = abs(pred_userbased_algo2.est - pred_userbased.est)\n",
    "\n",
    "    print(f\"{uid:>5} {iid:>5} {pred_userbased_algo2.est:10.4f} {pred_userbased.est:15.4f} {diff:10.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
