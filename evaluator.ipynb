{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a665885b",
   "metadata": {},
   "source": [
    "# Evaluator Module\n",
    "The Evaluator module creates evaluation reports.\n",
    "\n",
    "Reports contain evaluation metrics depending on models specified in the evaluation config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6aaf9140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reloads modules automatically before entering the execution of code\n",
    "#load_ext autoreload\n",
    "#autoreload 2\n",
    "\n",
    "# third parties imports\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "# -- add new imports here --\n",
    "\n",
    "# local imports\n",
    "from configs import EvalConfig\n",
    "from constants import Constant as C\n",
    "from loaders import export_evaluation_report\n",
    "from loaders import load_ratings\n",
    "from models import get_top_n\n",
    "# -- add new imports here --\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import SVD\n",
    "from surprise import accuracy\n",
    "from configs import EvalConfig \n",
    "from surprise import Dataset, Reader, accuracy\n",
    "from constants import Constant as C\n",
    "from surprise.model_selection import LeaveOneOut\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47c24a4",
   "metadata": {},
   "source": [
    "# 1. Model validation functions\n",
    "Validation functions are a way to perform crossvalidation on recommender system models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6d82188",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ratings(surprise_format=False):\n",
    "    df_ratings = pd.read_csv(C.EVIDENCE_PATH / C.RATINGS_FILENAME)\n",
    "    if surprise_format:\n",
    "        reader = Reader(rating_scale=C.RATINGS_SCALE)\n",
    "        data = Dataset.load_from_df(df_ratings[['userId', 'movieId', 'rating']], reader)\n",
    "        return data\n",
    "    else:\n",
    "        return df_ratings\n",
    "\n",
    "def generate_split_predictions(algo, ratings_dataset, eval_config):\n",
    "    \"\"\"\n",
    "    Generate predictions on a random test set specified in eval_config.\n",
    "    \n",
    "    Parameters:\n",
    "        algo: A Surprise algorithm instance (e.g., SVD, KNNBasic).\n",
    "        ratings_dataset: A Surprise Dataset object.\n",
    "        eval_config: An EvalConfig object containing evaluation parameters (e.g., test_size).\n",
    "    \n",
    "    Returns:\n",
    "        List of predictions made by the algorithm on the test set.\n",
    "    \"\"\"\n",
    "    # Récupérer la proportion test depuis eval_config\n",
    "    test_size = eval_config.test_size\n",
    "    # Diviser le dataset en train/test\n",
    "    trainset, testset = train_test_split(ratings_dataset, test_size=test_size)\n",
    "    # Entraîner le modèle sur le trainset\n",
    "    algo.fit(trainset)\n",
    "    # Faire des prédictions sur le testset\n",
    "    predictions = algo.test(testset)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def generate_loo_top_n(algo, ratings_dataset, eval_config):\n",
    "    \"\"\"Generate top-n recommendations for each user on a random Leave-one-out split (LOO)\"\"\"\n",
    "    #leaveOneOut object with one split\n",
    "    loo = LeaveOneOut(n_splits=1)\n",
    "    # Split the dataset into training and testing sets\n",
    "    trainset, testset = next(loo.split(ratings_dataset))\n",
    "    # Train the algorithm on the training set\n",
    "    algo.fit(trainset)\n",
    "    # Generate the anti-testset\n",
    "    anti_testset = trainset.build_anti_testset()\n",
    "    # Generate predictions on the anti-testset\n",
    "    predictions = algo.test(anti_testset)\n",
    "    # Get top-N recommendations\n",
    "    anti_testset_top_n = get_top_n(predictions, n=eval_config.top_n_value)\n",
    "    return anti_testset_top_n, testset\n",
    "\n",
    "\n",
    "def generate_full_top_n(algo, ratings_dataset, eval_config):\n",
    "    \"\"\"Generate top-n recommendations for each user with full training set (LOO)\"\"\"\n",
    "    # Construire l’ensemble d’entraînement complet à partir de toutes les données\n",
    "    full_trainset = ratings_dataset.build_full_trainset()\n",
    "\n",
    "    # Entraîner l’algorithme sur toutes les données disponibles\n",
    "    algo.fit(full_trainset)\n",
    "\n",
    "    # Générer le anti-testset : tous les items que chaque utilisateur n’a pas encore notés\n",
    "    anti_testset = full_trainset.build_anti_testset()\n",
    "\n",
    "    # Générer les prédictions sur le anti-testset\n",
    "    predictions = algo.test(anti_testset)\n",
    "\n",
    "    # Extraire les top-N recommandations\n",
    "    anti_testset_top_n = get_top_n(predictions, n=eval_config.top_n_value)\n",
    "\n",
    "    return anti_testset_top_n\n",
    "\n",
    "\n",
    "def precompute_information():\n",
    "    \"\"\" Returns a dictionary that precomputes relevant information for evaluating in full mode\n",
    "    \n",
    "    Dictionary keys:\n",
    "    - precomputed_dict[\"item_to_rank\"] : contains a dictionary mapping movie ids to rankings\n",
    "    - (-- for your project, add other relevant information here -- )\n",
    "    \"\"\"\n",
    "  \n",
    "    ratings = load_ratings()\n",
    "    # Compter les évaluations par film et trier par popularité décroissante\n",
    "    item_counts = ratings['movieId'].value_counts().sort_values(ascending=False)\n",
    "    # Mapper chaque film à son rang de popularité (1 = plus populaire)\n",
    "    item_to_rank = {movie: idx + 1 for idx, movie in enumerate(item_counts.index)}\n",
    "    \n",
    "    return {'item_to_rank': item_to_rank}            \n",
    "\n",
    "\n",
    "def create_evaluation_report(eval_config, sp_ratings, precomputed_dict, available_metrics):\n",
    "    \"\"\" Create a DataFrame evaluating various models on metrics specified in an evaluation config.  \n",
    "    \"\"\"\n",
    "    evaluation_dict = {}\n",
    "    for model_name, model, arguments in eval_config.models:\n",
    "        print(f'Handling model {model_name}')\n",
    "        algo = model(**arguments)\n",
    "        evaluation_dict[model_name] = {}\n",
    "        \n",
    "        # Type 1 : split evaluations\n",
    "        if len(eval_config.split_metrics) > 0:\n",
    "            print('Training split predictions')\n",
    "            predictions = generate_split_predictions(algo, sp_ratings, eval_config)\n",
    "            for metric in eval_config.split_metrics:\n",
    "                print(f'- computing metric {metric}')\n",
    "                assert metric in available_metrics['split']\n",
    "                evaluation_function, parameters =  available_metrics[\"split\"][metric]\n",
    "                evaluation_dict[model_name][metric] = evaluation_function(predictions, **parameters) \n",
    "\n",
    "        # Type 2 : loo evaluations\n",
    "        if len(eval_config.loo_metrics) > 0:\n",
    "            print('Training loo predictions')\n",
    "            anti_testset_top_n, testset = generate_loo_top_n(algo, sp_ratings, eval_config)\n",
    "            for metric in eval_config.loo_metrics:\n",
    "                assert metric in available_metrics['loo']\n",
    "                evaluation_function, parameters =  available_metrics[\"loo\"][metric]\n",
    "                evaluation_dict[model_name][metric] = evaluation_function(anti_testset_top_n, testset, **parameters)\n",
    "        \n",
    "        # Type 3 : full evaluations\n",
    "        if len(eval_config.full_metrics) > 0:\n",
    "            print('Training full predictions')\n",
    "            anti_testset_top_n = generate_full_top_n(algo, sp_ratings, eval_config)\n",
    "            for metric in eval_config.full_metrics:\n",
    "                assert metric in available_metrics['full']\n",
    "                evaluation_function, parameters =  available_metrics[\"full\"][metric]\n",
    "                evaluation_dict[model_name][metric] = evaluation_function(\n",
    "                    anti_testset_top_n,\n",
    "                    **precomputed_dict,\n",
    "                    **parameters\n",
    "                )\n",
    "        \n",
    "    return pd.DataFrame.from_dict(evaluation_dict).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e83d1d",
   "metadata": {},
   "source": [
    "# 2. Evaluation metrics\n",
    "Implement evaluation metrics for either rating predictions (split metrics) or for top-n recommendations (loo metric, full metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1849e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hit_rate(anti_testset_top_n, testset):\n",
    "    \"\"\"Compute the average hit over the users (loo metric)\n",
    "    \n",
    "    A hit (1) happens when the movie in the testset has been picked by the top-n recommender\n",
    "    A fail (0) happens when the movie in the testset has not been picked by the top-n recommender\n",
    "    \"\"\"\n",
    "   #implement the function get_hit_rate \n",
    "    hits = 0\n",
    "    total = len(testset)  \n",
    "\n",
    "    # Iterate through each entry in the testset\n",
    "    for user_id, movie_id, _ in testset:\n",
    "        top_n_recommendations = anti_testset_top_n.get(user_id, [])\n",
    "        if movie_id in [recommended_movie[0] for recommended_movie in top_n_recommendations]:\n",
    "            hits += 1 \n",
    "\n",
    "    hit_rate = hits / total if total > 0 else 0\n",
    "    return hit_rate\n",
    "\n",
    "\n",
    "def get_novelty(anti_testset_top_n, item_to_rank):\n",
    "    \"\"\"Compute the average novelty of the top-n recommendation over the users (full metric)\n",
    "    \n",
    "    The novelty is defined as the average ranking of the movies recommended\n",
    "    \"\"\"\n",
    "    total_rank = 0\n",
    "    num_entries = 0\n",
    "    total_items = len(item_to_rank)\n",
    "    for user_recommendations in anti_testset_top_n.values():\n",
    "        for movie_id, _ in user_recommendations:\n",
    "            total_rank += item_to_rank.get(movie_id, total_items + 1)\n",
    "            num_entries += 1\n",
    "    average_rank_sum = total_rank / num_entries if num_entries > 0 else 0\n",
    "    normalized_novelty = average_rank_sum / total_items  # Normalization step\n",
    "    return normalized_novelty\n",
    "    #return average_rank_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9855b3",
   "metadata": {},
   "source": [
    "# 3. Evaluation workflow\n",
    "Load data, evaluate models and save the experimental outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "704f4d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling model content_linear\n",
      "Training split predictions\n",
      "- computing metric mae\n",
      "- computing metric rmse\n",
      "Training loo predictions\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m sp_ratings \u001b[38;5;241m=\u001b[39m load_ratings(surprise_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     15\u001b[0m precomputed_dict \u001b[38;5;241m=\u001b[39m precompute_information()\n\u001b[1;32m---> 16\u001b[0m evaluation_report \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_evaluation_report\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEvalConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msp_ratings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprecomputed_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mAVAILABLE_METRICS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRésultats de l\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mévaluation des 4 modèles :\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     18\u001b[0m display(evaluation_report)  \n",
      "Cell \u001b[1;32mIn[4], line 109\u001b[0m, in \u001b[0;36mcreate_evaluation_report\u001b[1;34m(eval_config, sp_ratings, precomputed_dict, available_metrics)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(eval_config\u001b[38;5;241m.\u001b[39mloo_metrics) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining loo predictions\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 109\u001b[0m     anti_testset_top_n, testset \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_loo_top_n\u001b[49m\u001b[43m(\u001b[49m\u001b[43malgo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msp_ratings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m metric \u001b[38;5;129;01min\u001b[39;00m eval_config\u001b[38;5;241m.\u001b[39mloo_metrics:\n\u001b[0;32m    111\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m metric \u001b[38;5;129;01min\u001b[39;00m available_metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloo\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Cell \u001b[1;32mIn[4], line 44\u001b[0m, in \u001b[0;36mgenerate_loo_top_n\u001b[1;34m(algo, ratings_dataset, eval_config)\u001b[0m\n\u001b[0;32m     42\u001b[0m anti_testset \u001b[38;5;241m=\u001b[39m trainset\u001b[38;5;241m.\u001b[39mbuild_anti_testset()\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Generate predictions on the anti-testset\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43malgo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43manti_testset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Get top-N recommendations\u001b[39;00m\n\u001b[0;32m     46\u001b[0m anti_testset_top_n \u001b[38;5;241m=\u001b[39m get_top_n(predictions, n\u001b[38;5;241m=\u001b[39meval_config\u001b[38;5;241m.\u001b[39mtop_n_value)\n",
      "File \u001b[1;32mc:\\Users\\mathi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\surprise\\prediction_algorithms\\algo_base.py:160\u001b[0m, in \u001b[0;36mAlgoBase.test\u001b[1;34m(self, testset, verbose)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Test the algorithm on given testset, i.e. estimate all the ratings\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;124;03min the given testset.\u001b[39;00m\n\u001b[0;32m    144\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;124;03m    that contains all the estimated ratings.\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;66;03m# The ratings are translated back to their original scale.\u001b[39;00m\n\u001b[1;32m--> 160\u001b[0m predictions \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(uid, iid, r_ui_trans, verbose\u001b[38;5;241m=\u001b[39mverbose)\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (uid, iid, r_ui_trans) \u001b[38;5;129;01min\u001b[39;00m testset\n\u001b[0;32m    163\u001b[0m ]\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m predictions\n",
      "File \u001b[1;32mc:\\Users\\mathi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\surprise\\prediction_algorithms\\algo_base.py:161\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Test the algorithm on given testset, i.e. estimate all the ratings\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;124;03min the given testset.\u001b[39;00m\n\u001b[0;32m    144\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;124;03m    that contains all the estimated ratings.\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;66;03m# The ratings are translated back to their original scale.\u001b[39;00m\n\u001b[0;32m    160\u001b[0m predictions \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m--> 161\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr_ui_trans\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (uid, iid, r_ui_trans) \u001b[38;5;129;01min\u001b[39;00m testset\n\u001b[0;32m    163\u001b[0m ]\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m predictions\n",
      "File \u001b[1;32mc:\\Users\\mathi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\surprise\\prediction_algorithms\\algo_base.py:102\u001b[0m, in \u001b[0;36mAlgoBase.predict\u001b[1;34m(self, uid, iid, r_ui, clip, verbose)\u001b[0m\n\u001b[0;32m    100\u001b[0m details \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 102\u001b[0m     est \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimate\u001b[49m\u001b[43m(\u001b[49m\u001b[43miuid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miiid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;66;03m# If the details dict was also returned\u001b[39;00m\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(est, \u001b[38;5;28mtuple\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\mathi\\OneDrive - UCL\\Documents\\GitHub\\Recommender-Systeem-\\models.py:226\u001b[0m, in \u001b[0;36mContentBased.estimate\u001b[1;34m(self, u, i)\u001b[0m\n\u001b[0;32m    224\u001b[0m     score \u001b[38;5;241m=\u001b[39m rd\u001b[38;5;241m.\u001b[39mchoice(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_profile[u])\n\u001b[0;32m    225\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregressor_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 226\u001b[0m     score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muser_profile\u001b[49m\u001b[43m[\u001b[49m\u001b[43mu\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem_features\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    229\u001b[0m     score\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mathi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_base.py:297\u001b[0m, in \u001b[0;36mLinearModel.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    284\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;124;03m    Predict using the linear model.\u001b[39;00m\n\u001b[0;32m    286\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;124;03m        Returns predicted values.\u001b[39;00m\n\u001b[0;32m    296\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 297\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decision_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mathi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_base.py:274\u001b[0m, in \u001b[0;36mLinearModel._decision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_decision_function\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m--> 274\u001b[0m     \u001b[43mcheck_is_fitted\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    276\u001b[0m     X \u001b[38;5;241m=\u001b[39m validate_data(\u001b[38;5;28mself\u001b[39m, X, accept_sparse\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoo\u001b[39m\u001b[38;5;124m\"\u001b[39m], reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    277\u001b[0m     coef_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_\n",
      "File \u001b[1;32mc:\\Users\\mathi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1751\u001b[0m, in \u001b[0;36mcheck_is_fitted\u001b[1;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[0;32m   1748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(estimator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not an estimator instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (estimator))\n\u001b[1;32m-> 1751\u001b[0m tags \u001b[38;5;241m=\u001b[39m \u001b[43mget_tags\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1753\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tags\u001b[38;5;241m.\u001b[39mrequires_fit \u001b[38;5;129;01mand\u001b[39;00m attributes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1754\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mathi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_tags.py:393\u001b[0m, in \u001b[0;36mget_tags\u001b[1;34m(estimator)\u001b[0m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_tags\u001b[39m(estimator) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tags:\n\u001b[0;32m    368\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get estimator tags.\u001b[39;00m\n\u001b[0;32m    369\u001b[0m \n\u001b[0;32m    370\u001b[0m \u001b[38;5;124;03m    :class:`~sklearn.BaseEstimator` provides the estimator tags machinery.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    390\u001b[0m \u001b[38;5;124;03m        The estimator tags.\u001b[39;00m\n\u001b[0;32m    391\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 393\u001b[0m     tag_provider \u001b[38;5;241m=\u001b[39m \u001b[43m_find_tags_provider\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    395\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tag_provider \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__sklearn_tags__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    396\u001b[0m         \u001b[38;5;66;03m# TODO(1.7): turn the warning into an error\u001b[39;00m\n\u001b[0;32m    397\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\mathi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_tags.py:346\u001b[0m, in \u001b[0;36m_find_tags_provider\u001b[1;34m(estimator, warn)\u001b[0m\n\u001b[0;32m    343\u001b[0m has_sklearn_tags \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__sklearn_tags__\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m tags_mro[klass]\n\u001b[0;32m    345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tags_mro[klass] \u001b[38;5;129;01mand\u001b[39;00m tag_provider \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__sklearn_tags__\u001b[39m\u001b[38;5;124m\"\u001b[39m:  \u001b[38;5;66;03m# is it empty\u001b[39;00m\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_get_or_more_tags \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_sklearn_tags:\n\u001b[0;32m    347\u001b[0m         \u001b[38;5;66;03m# Case where a class does not implement __sklearn_tags__ and we fallback\u001b[39;00m\n\u001b[0;32m    348\u001b[0m         \u001b[38;5;66;03m# to _get_tags. We should therefore warn for implementing\u001b[39;00m\n\u001b[0;32m    349\u001b[0m         \u001b[38;5;66;03m# __sklearn_tags__.\u001b[39;00m\n\u001b[0;32m    350\u001b[0m         tag_provider \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_get_tags\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    351\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "AVAILABLE_METRICS = {\n",
    "    \"split\": {\n",
    "        \"mae\": (accuracy.mae, {'verbose': False}),\n",
    "        \"rmse\": (accuracy.rmse, {'verbose': False}),\n",
    "    },\n",
    "    \"loo\": {\n",
    "        \"hit_rate\": (get_hit_rate, {}),\n",
    "    },\n",
    "    \"full\": {\n",
    "        \"novelty\": (get_novelty, {}),\n",
    "    }\n",
    "}\n",
    "\n",
    "sp_ratings = load_ratings(surprise_format=True)\n",
    "precomputed_dict = precompute_information()\n",
    "evaluation_report = create_evaluation_report(EvalConfig, sp_ratings, precomputed_dict, AVAILABLE_METRICS)\n",
    "print(\"Résultats de l'évaluation des 4 modèles :\")\n",
    "display(evaluation_report)  \n",
    "print(evaluation_report)  \n",
    "export_evaluation_report(evaluation_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460cc779",
   "metadata": {},
   "source": [
    "Commentaire:\n",
    "MAE / RMSE :\n",
    "La baseline 4 est de loin la plus performante et donc la plus précise selon ces deux indicateurs. Elle est suivie par la baseline 3. En revanche la baseline 1 est la moins performante ce qui en fait le modèle le moins précis.\n",
    "\n",
    "Hit Rate :\n",
    "Le taux de succès est très faible pour les baselines 1, 2 et 3, avec seulement environ 0,2 %, 0,4 % et 0,6 % respectivement dans ce cas. Cela montre leur incapacité à recommander efficacement des items pertinents. En revanche, la baseline 4 se distingue avec un Hit Rate nettement supérieur (5,2 %).\n",
    "\n",
    "Novelty :\n",
    "Les baselines 1, 2 et 3 obtiennent des scores de novelty relativement élevés, ce qui signifie qu’elles recommandent des items moins populaires. À l’inverse, la baseline 4 présente une valeur de nouveauté nettement plus faible ce qui indique qu’elle recommande majoritairement des films très vus. Cela maximise la précision mais se fait au détriment de la diversité des recommandations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
