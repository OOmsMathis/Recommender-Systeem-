{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a665885b",
   "metadata": {},
   "source": [
    "# Evaluator Module\n",
    "The Evaluator module creates evaluation reports.\n",
    "\n",
    "Reports contain evaluation metrics depending on models specified in the evaluation config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6aaf9140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      userId  movieId  rating   timestamp\n",
      "0         15       34     3.0   997938310\n",
      "1         15       95     1.5  1093028331\n",
      "2         15      101     4.0  1134522072\n",
      "3         15      123     4.0   997938358\n",
      "4         15      125     3.5  1245362506\n",
      "...      ...      ...     ...         ...\n",
      "5291     665     3908     1.0  1046967201\n",
      "5292     665     4052     4.0   992838277\n",
      "5293     665     4351     4.0   992837743\n",
      "5294     665     4643     4.0   997239207\n",
      "5295     665     5502     4.0  1046967596\n",
      "\n",
      "[5296 rows x 4 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mathi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mathi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\mathi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# reloads modules automatically before entering the execution of code\n",
    "#load_ext autoreload\n",
    "#autoreload 2\n",
    "\n",
    "# third parties imports\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "# -- add new imports here --\n",
    "\n",
    "# local imports\n",
    "from configs import EvalConfig\n",
    "from constants import Constant as C\n",
    "from loaders import export_evaluation_report\n",
    "from loaders import load_ratings\n",
    "from models import get_top_n\n",
    "# -- add new imports here --\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import SVD\n",
    "from surprise import accuracy\n",
    "from configs import EvalConfig \n",
    "from surprise import Dataset, Reader, accuracy\n",
    "from constants import Constant as C\n",
    "from surprise.model_selection import LeaveOneOut\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47c24a4",
   "metadata": {},
   "source": [
    "# 1. Model validation functions\n",
    "Validation functions are a way to perform crossvalidation on recommender system models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "922721bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from loaders import load_ratings, load_items # Assurez-vous que ces fonctions sont disponibles\n",
    "import constants as C # Assurez-vous que vos constantes (C.EVIDENCE_PATH, C.RATINGS_FILENAME, etc.) sont bien définies\n",
    "from constants import Constant as C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6d82188",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ratings(surprise_format=False):\n",
    "    df_ratings = pd.read_csv(C.EVIDENCE_PATH / C.RATINGS_FILENAME)\n",
    "    if surprise_format:\n",
    "        reader = Reader(rating_scale=C.RATINGS_SCALE)\n",
    "        data = Dataset.load_from_df(df_ratings[['userId', 'movieId', 'rating']], reader)\n",
    "        return data\n",
    "    else:\n",
    "        return df_ratings\n",
    "\n",
    "def generate_split_predictions(algo, ratings_dataset, eval_config):\n",
    "    \"\"\"\n",
    "    Generate predictions on a random test set specified in eval_config.\n",
    "    \n",
    "    Parameters:\n",
    "        algo: A Surprise algorithm instance (e.g., SVD, KNNBasic).\n",
    "        ratings_dataset: A Surprise Dataset object.\n",
    "        eval_config: An EvalConfig object containing evaluation parameters (e.g., test_size).\n",
    "    \n",
    "    Returns:\n",
    "        List of predictions made by the algorithm on the test set.\n",
    "    \"\"\"\n",
    "    # Récupérer la proportion test depuis eval_config\n",
    "    test_size = eval_config.test_size\n",
    "    # Diviser le dataset en train/test\n",
    "    trainset, testset = train_test_split(ratings_dataset, test_size=test_size)\n",
    "    # Entraîner le modèle sur le trainset\n",
    "    algo.fit(trainset)\n",
    "    # Faire des prédictions sur le testset\n",
    "    predictions = algo.test(testset)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def generate_loo_top_n(algo, ratings_dataset, eval_config):\n",
    "    #Generate top-n recommendations for each user on a random Leave-one-out split (LOO)\n",
    "    #leaveOneOut object with one split\n",
    "    loo = LeaveOneOut(n_splits=1)\n",
    "    # Split the dataset into training and testing sets\n",
    "    trainset, testset = next(loo.split(ratings_dataset))\n",
    "    # Train the algorithm on the training set\n",
    "    algo.fit(trainset)\n",
    "    # Generate the anti-testset\n",
    "    anti_testset = trainset.build_anti_testset()\n",
    "    # Generate predictions on the anti-testset\n",
    "    predictions = algo.test(anti_testset)\n",
    "    # Get top-N recommendations\n",
    "    anti_testset_top_n = get_top_n(predictions, n=eval_config.top_n_value)\n",
    "    return anti_testset_top_n, testset\n",
    "\n",
    "\n",
    "def generate_full_top_n(algo, ratings_dataset, eval_config):\n",
    "    #Generate top-n recommendations for each user with full training set (LOO)\n",
    "    # Construire l’ensemble d’entraînement complet à partir de toutes les données\n",
    "    full_trainset = ratings_dataset.build_full_trainset()\n",
    "\n",
    "    # Entraîner l’algorithme sur toutes les données disponibles\n",
    "    algo.fit(full_trainset)\n",
    "\n",
    "    # Générer le anti-testset : tous les items que chaque utilisateur n’a pas encore notés\n",
    "    anti_testset = full_trainset.build_anti_testset()\n",
    "\n",
    "    # Générer les prédictions sur le anti-testset\n",
    "    predictions = algo.test(anti_testset)\n",
    "\n",
    "    # Extraire les top-N recommandations\n",
    "    anti_testset_top_n = get_top_n(predictions, n=eval_config.top_n_value)\n",
    "\n",
    "    return anti_testset_top_n \n",
    "\n",
    "def precompute_information():\n",
    "    \"\"\"Returns a dictionary that precomputes relevant information for evaluating in full mode\n",
    "    and for initializing ContentBased models with pre-computed features and scalers.\n",
    "\n",
    "    Dictionary keys:\n",
    "    - precomputed_dict[\"item_to_rank\"] : contains a dictionary mapping movie ids to rankings (from original code)\n",
    "    - precomputed_dict[\"feature_stats\"] : dictionary of min/max values for numeric features\n",
    "    - precomputed_dict[\"tfidf_vectorizers\"] : dictionary of pre-fitted TfidfVectorizer objects\n",
    "    - precomputed_dict[\"one_hot_columns\"] : dictionary of column names for one-hot encoded features\n",
    "    \"\"\"\n",
    "\n",
    "    ratings = load_ratings()\n",
    "    items = load_items() # Charger les items ici\n",
    "\n",
    "    precomputed_dict = {}\n",
    "\n",
    "    # --- 1. Informations existantes (popularité des films) ---\n",
    "    item_counts = ratings['movieId'].value_counts().sort_values(ascending=False)\n",
    "    precomputed_dict['item_to_rank'] = {movie: idx + 1 for idx, movie in enumerate(item_counts.index)}\n",
    "\n",
    "    # --- 2. Précalcul des statistiques pour la normalisation des features numériques ---\n",
    "    feature_stats = {}\n",
    "\n",
    "    # title_length\n",
    "    df_title_length = items[C.LABEL_COL].apply(lambda x: len(x)).to_frame('title_length')\n",
    "    df_title_length['title_length'] = df_title_length['title_length'].fillna(0).astype(int)\n",
    "    mean_title_length = int(df_title_length['title_length'].replace(0, np.nan).mean())\n",
    "    feature_stats['title_length'] = {\n",
    "        'min': df_title_length['title_length'].min(),\n",
    "        'max': df_title_length['title_length'].max(),\n",
    "        'mean_fillna': mean_title_length\n",
    "    }\n",
    "\n",
    "    # Year_of_release\n",
    "    year = items[C.LABEL_COL].str.extract(r'\\((\\d{4})\\)')[0].astype(float)\n",
    "    df_year = year.to_frame(name='year_of_release')\n",
    "    mean_year = df_year.replace(0, np.nan).mean().iloc[0]\n",
    "    feature_stats['year_of_release'] = {\n",
    "        'min': df_year['year_of_release'].min(),\n",
    "        'max': df_year['year_of_release'].max(),\n",
    "        'mean_fillna': mean_year\n",
    "    }\n",
    "\n",
    "    # average_ratings\n",
    "    average_rating = ratings.groupby('movieId')[C.RATING_COL].mean().rename('average_rating').to_frame()\n",
    "    global_avg_rating = ratings[C.RATING_COL].mean()\n",
    "    feature_stats['average_rating'] = {\n",
    "        'min': average_rating['average_rating'].min(),\n",
    "        'max': average_rating['average_rating'].max(),\n",
    "        'mean_fillna': global_avg_rating\n",
    "    }\n",
    "\n",
    "    # count_ratings\n",
    "    rating_count = ratings.groupby('movieId')[C.RATING_COL].size().rename('rating_count').to_frame()\n",
    "    rating_count['rating_count'] = rating_count['rating_count'].fillna(0).astype(int)\n",
    "    mean_rating_count = int(rating_count['rating_count'].replace(0, np.nan).mean())\n",
    "    feature_stats['count_ratings'] = {\n",
    "        'min': rating_count['rating_count'].min(),\n",
    "        'max': rating_count['rating_count'].max(),\n",
    "        'mean_fillna': mean_rating_count\n",
    "    }\n",
    "\n",
    "    # TMDB features (vote_average, popularity, budget, revenue, runtime, vote_count, profit)\n",
    "    tmdb_path = C.CONTENT_PATH / \"tmdb_full_features.csv\"\n",
    "    df_tmdb = pd.read_csv(tmdb_path).drop_duplicates('movieId').set_index('movieId')\n",
    "\n",
    "    for col in ['vote_average', 'popularity', 'budget', 'revenue', 'runtime', 'vote_count']:\n",
    "        mean_val = df_tmdb[col].mean()\n",
    "        feature_stats[f'tmdb_{col}'] = {\n",
    "            'min': df_tmdb[col].min(),\n",
    "            'max': df_tmdb[col].max(),\n",
    "            'mean_fillna': mean_val\n",
    "        }\n",
    "\n",
    "    # Profit (dérivé du budget et revenue)\n",
    "    df_tmdb['profit'] = df_tmdb['revenue'] - df_tmdb['budget']\n",
    "    mean_profit = df_tmdb['profit'].mean()\n",
    "    feature_stats['tmdb_profit'] = {\n",
    "        'min': df_tmdb['profit'].min(),\n",
    "        'max': df_tmdb['profit'].max(),\n",
    "        'mean_fillna': mean_profit\n",
    "    }\n",
    "    \n",
    "    precomputed_dict['feature_stats'] = feature_stats\n",
    "\n",
    "    # --- 3. Pré-entraînement des TF-IDF Vectorizers ---\n",
    "    tfidf_vectorizers = {}\n",
    "    \n",
    "    # Genre_tfidf\n",
    "    items_copy = items.copy()\n",
    "    items_copy['genre_string'] = items_copy[C.GENRES_COL].fillna('').str.replace('|', ' ', regex=False)\n",
    "    tfidf_genre = TfidfVectorizer()\n",
    "    tfidf_genre.fit(items_copy['genre_string'])\n",
    "    tfidf_vectorizers['Genre_tfidf'] = tfidf_genre\n",
    "\n",
    "    # Tags\n",
    "    tags_path = C.CONTENT_PATH / \"tags.csv\"\n",
    "    df_tags = pd.read_csv(tags_path)\n",
    "    df_tags = df_tags.dropna(subset=['tag'])\n",
    "    df_tags['tag'] = df_tags['tag'].astype(str)\n",
    "    df_tags_grouped = df_tags.groupby('movieId')['tag'].agg(' '.join).to_frame('tags')\n",
    "    tfidf_tags = TfidfVectorizer()\n",
    "    tfidf_tags.fit(df_tags_grouped['tags'])\n",
    "    tfidf_vectorizers['Tags'] = tfidf_tags\n",
    "\n",
    "    # title_tfidf (avec lemmatisation et stopwords)\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "    nltk.download('omw-1.4', quiet=True)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    items_copy['title_string_processed'] = items_copy[C.LABEL_COL].fillna('').apply(lambda x: ' '.join(\n",
    "                                lemmatizer.lemmatize(word) for word in x.split() if word.lower() not in stop_words\n",
    "                            ))\n",
    "    tfidf_title = TfidfVectorizer()\n",
    "    tfidf_title.fit(items_copy['title_string_processed'])\n",
    "    tfidf_vectorizers['title_tfidf'] = tfidf_title\n",
    "\n",
    "    # tmdb_cast\n",
    "    df_tmdb_cast = pd.read_csv(tmdb_path).drop_duplicates('movieId')\n",
    "    df_tmdb_cast['cast'] = df_tmdb_cast['cast'].fillna('')\n",
    "    tfidf_cast = TfidfVectorizer()\n",
    "    tfidf_cast.fit(df_tmdb_cast['cast'])\n",
    "    tfidf_vectorizers['tmdb_cast'] = tfidf_cast\n",
    "\n",
    "    # tmdb_director\n",
    "    df_tmdb_director = pd.read_csv(tmdb_path).drop_duplicates('movieId')\n",
    "    df_tmdb_director['director'] = df_tmdb_director['director'].fillna('')\n",
    "    tfidf_director = TfidfVectorizer()\n",
    "    tfidf_director.fit(df_tmdb_director['director'])\n",
    "    tfidf_vectorizers['tmdb_director'] = tfidf_director\n",
    "\n",
    "    precomputed_dict['tfidf_vectorizers'] = tfidf_vectorizers\n",
    "\n",
    "    # --- 4. Colonnes pour One-Hot Encoding (pour garantir la cohérence des colonnes) ---\n",
    "    one_hot_columns = {}\n",
    "\n",
    "    # tmdb_original_language\n",
    "    df_tmdb_lang = pd.read_csv(tmdb_path).drop_duplicates('movieId')\n",
    "    df_tmdb_lang['original_language'] = df_tmdb_lang['original_language'].fillna('unknown')\n",
    "    one_hot_columns['tmdb_original_language'] = pd.get_dummies(df_tmdb_lang['original_language'], prefix='lang').columns.tolist()\n",
    "    \n",
    "    precomputed_dict['one_hot_columns'] = one_hot_columns\n",
    "\n",
    "    return precomputed_dict        \n",
    "\n",
    "\n",
    "def create_evaluation_report(eval_config, sp_ratings, precomputed_dict, available_metrics):\n",
    "    \"\"\" Create a DataFrame evaluating various models on metrics specified in an evaluation config.  \n",
    "    \"\"\"\n",
    "    evaluation_dict = {}\n",
    "    for model_name, model, arguments in eval_config.models:\n",
    "        print(f'Handling model {model_name}')\n",
    "        algo = model(**arguments)\n",
    "        evaluation_dict[model_name] = {}\n",
    "        \n",
    "        # Type 1 : split evaluations\n",
    "        if len(eval_config.split_metrics) > 0:\n",
    "            print('Training split predictions')\n",
    "            predictions = generate_split_predictions(algo, sp_ratings, eval_config)\n",
    "            for metric in eval_config.split_metrics:\n",
    "                print(f'- computing metric {metric}')\n",
    "                assert metric in available_metrics['split']\n",
    "                evaluation_function, parameters =  available_metrics[\"split\"][metric]\n",
    "                evaluation_dict[model_name][metric] = evaluation_function(predictions, **parameters) \n",
    "\n",
    "        # Type 2 : loo evaluations\n",
    "        if len(eval_config.loo_metrics) > 0:\n",
    "            print('Training loo predictions')\n",
    "            anti_testset_top_n, testset = generate_loo_top_n(algo, sp_ratings, eval_config)\n",
    "            for metric in eval_config.loo_metrics:\n",
    "                assert metric in available_metrics['loo']\n",
    "                evaluation_function, parameters =  available_metrics[\"loo\"][metric]\n",
    "                evaluation_dict[model_name][metric] = evaluation_function(anti_testset_top_n, testset, **parameters)\n",
    "        \n",
    "        # Type 3 : full evaluations\n",
    "        if len(eval_config.full_metrics) > 0:\n",
    "            print('Training full predictions')\n",
    "            anti_testset_top_n = generate_full_top_n(algo, sp_ratings, eval_config)\n",
    "            for metric in eval_config.full_metrics:\n",
    "                assert metric in available_metrics['full']\n",
    "                evaluation_function, parameters = available_metrics[\"full\"][metric]\n",
    "                # Passe item_to_rank si la métrique attend cet argument\n",
    "                if metric == \"novelty\":\n",
    "                    evaluation_dict[model_name][metric] = evaluation_function(\n",
    "                        anti_testset_top_n,\n",
    "                        item_to_rank=precomputed_dict[\"item_to_rank\"],\n",
    "                        **parameters\n",
    "                    )\n",
    "                else:\n",
    "                    evaluation_dict[model_name][metric] = evaluation_function(\n",
    "                        anti_testset_top_n,\n",
    "                        **parameters\n",
    "                    )\n",
    "    return pd.DataFrame.from_dict(evaluation_dict).T \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e83d1d",
   "metadata": {},
   "source": [
    "# 2. Evaluation metrics\n",
    "Implement evaluation metrics for either rating predictions (split metrics) or for top-n recommendations (loo metric, full metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1849e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hit_rate(anti_testset_top_n, testset):\n",
    "    \"\"\"Compute the average hit over the users (loo metric)\n",
    "    \n",
    "    A hit (1) happens when the movie in the testset has been picked by the top-n recommender\n",
    "    A fail (0) happens when the movie in the testset has not been picked by the top-n recommender\"\"\"\n",
    "    \n",
    "   #implement the function get_hit_rate \n",
    "    hits = 0\n",
    "    total = len(testset)  \n",
    "\n",
    "    # Iterate through each entry in the testset\n",
    "    for user_id, movie_id, _ in testset:\n",
    "        top_n_recommendations = anti_testset_top_n.get(user_id, [])\n",
    "        if movie_id in [recommended_movie[0] for recommended_movie in top_n_recommendations]:\n",
    "            hits += 1 \n",
    "\n",
    "    hit_rate = hits / total if total > 0 else 0\n",
    "    return hit_rate\n",
    "\n",
    "\n",
    "def get_novelty(anti_testset_top_n, item_to_rank):\n",
    "    #Compute the average novelty of the top-n recommendation over the users (full metric)\n",
    "    \n",
    "    \"\"\"The novelty is defined as the average ranking of the movies recommended\"\"\"\n",
    "    \n",
    "    total_rank = 0\n",
    "    num_entries = 0\n",
    "    total_items = len(item_to_rank)\n",
    "    for user_recommendations in anti_testset_top_n.values():\n",
    "        for movie_id, _ in user_recommendations:\n",
    "            total_rank += item_to_rank.get(movie_id, total_items + 1)\n",
    "            num_entries += 1\n",
    "    average_rank_sum = total_rank / num_entries if num_entries > 0 else 0\n",
    "    normalized_novelty = average_rank_sum / total_items  # Normalization step\n",
    "    return normalized_novelty\n",
    "    #return average_rank_sum \"\"\"\n",
    "\n",
    "def get_accuracy(predictions):\n",
    "        \"\"\"Compute the accuracy for rating predictions (split metric).\n",
    "        Accuracy is defined as the proportion of predictions where the rounded predicted rating equals the true rating.\"\"\"\n",
    "        correct = 0\n",
    "        total = len(predictions)\n",
    "        for pred in predictions:\n",
    "            if round(pred.est) == round(pred.r_ui):\n",
    "                correct += 1\n",
    "        return correct / total if total > 0 else 0\n",
    "\n",
    "def get_precision(anti_testset_top_n, testset):\n",
    "            \"\"\"\n",
    "            Compute the average precision over all users (loo metric).\n",
    "            Precision is defined as the proportion of recommended items that are relevant (i.e., the test item is in the top-n recommendations).\n",
    "            \"\"\"\n",
    "            hits = 0\n",
    "            total_recommended = 0\n",
    "\n",
    "            # Build a mapping from user to their test movie(s)\n",
    "            test_movies = defaultdict(set)\n",
    "            for user_id, movie_id, _ in testset:\n",
    "                test_movies[user_id].add(movie_id)\n",
    "\n",
    "            for user_id, recommendations in anti_testset_top_n.items():\n",
    "                recommended_movies = set([movie_id for movie_id, _ in recommendations])\n",
    "                relevant = test_movies.get(user_id, set())\n",
    "                hits += len(recommended_movies & relevant)\n",
    "                total_recommended += len(recommended_movies)\n",
    "\n",
    "            precision = hits / total_recommended if total_recommended > 0 else 0\n",
    "            return precision\n",
    "\n",
    "    # Ajout de la métrique accuracy dans AVAILABLE_METRICS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9855b3",
   "metadata": {},
   "source": [
    "# 3. Evaluation workflow\n",
    "Load data, evaluate models and save the experimental outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93361d19",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "704f4d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Informations pré-calculées :\n",
      "item_to_rank: {1240: 1, 1210: 2, 858: 3, 527: 4, 500: 5, 1208: 6, 590: 7, 1073: 8, 1968: 9, 2987: 10, 2011: 11, 34: 12, 6377: 13, 1225: 14, 923: 15, 5502: 16, 6333: 17, 2804: 18, 1219: 19, 4979: 20, 1092: 21, 5669: 22, 1250: 23, 899: 24, 2717: 25, 3499: 26, 1953: 27, 79132: 28, 49272: 29, 784: 30, 95: 31, 317: 32, 55820: 33, 1339: 34, 2006: 35, 3504: 36, 2746: 37, 4643: 38, 8622: 39, 6537: 40, 1345: 41, 708: 42, 509: 43, 8528: 44, 2724: 45, 3101: 46, 1032: 47, 1952: 48, 2881: 49, 5945: 50, 1172: 51, 673: 52, 2605: 53, 1095: 54, 5388: 55, 2427: 56, 3168: 57, 1212: 58, 2352: 59, 2953: 60, 2087: 61, 160: 62, 1299: 63, 3: 64, 91529: 65, 2942: 66, 1945: 67, 3450: 68, 515: 69, 1217: 70, 162: 71, 3362: 72, 1274: 73, 2501: 74, 535: 75, 494: 76, 3555: 77, 915: 78, 942: 79, 6323: 80, 2739: 81, 30749: 82, 37741: 83, 481: 84, 4105: 85, 2065: 86, 2805: 87, 4388: 88, 5064: 89, 2146: 90, 3146: 91, 44199: 92, 34319: 93, 3198: 94, 1779: 95, 2376: 96, 2642: 97, 3639: 98, 1711: 99, 3334: 100, 3269: 101, 1342: 102, 56782: 103, 7325: 104, 4351: 105, 5015: 106, 1416: 107, 2422: 108, 125: 109, 1515: 110, 106920: 111, 3438: 112, 599: 113, 60072: 114, 532: 115, 3259: 116, 1619: 117, 96610: 118, 2672: 119, 106916: 120, 1974: 121, 1687: 122, 1354: 123, 3186: 124, 101: 125, 3264: 126, 6156: 127, 256: 128, 556: 129, 1713: 130, 44761: 131, 3452: 132, 1305: 133, 6155: 134, 4467: 135, 27611: 136, 3724: 137, 60756: 138, 1279: 139, 2259: 140, 6975: 141, 276: 142, 109374: 143, 3633: 144, 69306: 145, 86332: 146, 8860: 147, 2252: 148, 663: 149, 3007: 150, 33162: 151, 3185: 152, 1218: 153, 948: 154, 3270: 155, 3011: 156, 2042: 157, 3593: 158, 3681: 159, 68319: 160, 3701: 161, 3070: 162, 96737: 163, 78469: 164, 4557: 165, 95441: 166, 546: 167, 26614: 168, 31658: 169, 73268: 170, 2060: 171, 1772: 172, 2183: 173, 3129: 174, 3310: 175, 3429: 176, 2117: 177, 2921: 178, 2995: 179, 65: 180, 1873: 181, 33836: 182, 5066: 183, 5577: 184, 3213: 185, 369: 186, 122892: 187, 2123: 188, 249: 189, 1264: 190, 139385: 191, 69951: 192, 67197: 193, 1425: 194, 3182: 195, 1346: 196, 5690: 197, 3426: 198, 15: 199, 48043: 200, 5147: 201, 829: 202, 2877: 203, 4052: 204, 1453: 205, 3744: 206, 3030: 207, 95167: 208, 2135: 209, 422: 210, 3682: 211, 6731: 212, 2453: 213, 71520: 214, 1821: 215, 4941: 216, 2349: 217, 33171: 218, 79592: 219, 7439: 220, 4889: 221, 106002: 222, 233: 223, 5247: 224, 2266: 225, 64: 226, 2473: 227, 3734: 228, 7980: 229, 5364: 230, 5452: 231, 70293: 232, 62155: 233, 4835: 234, 1785: 235, 206: 236, 1649: 237, 89804: 238, 36527: 239, 3189: 240, 139644: 241, 3928: 242, 3728: 243, 1683: 244, 123: 245, 52885: 246, 2414: 247, 62999: 248, 89864: 249, 3646: 250, 7089: 251, 7150: 252, 181: 253, 2429: 254, 27878: 255, 37380: 256, 1942: 257, 74532: 258, 71745: 259, 1535: 260, 1480: 261, 96829: 262, 6297: 263, 1681: 264, 3908: 265, 67923: 266, 2077: 267, 5250: 268, 3506: 269, 3537: 270, 51412: 271, 6290: 272, 6535: 273, 73015: 274, 103141: 275, 3713: 276, 421: 277, 2587: 278, 4697: 279, 3599: 280, 65188: 281, 2460: 282, 5021: 283, 444: 284, 65261: 285, 1976: 286, 3028: 287, 3091: 288, 1598: 289, 58299: 290, 7584: 291, 43396: 292, 1404: 293, 2458: 294, 2171: 295, 2522: 296, 26133: 297, 78105: 298, 3723: 299, 77658: 300, 8620: 301, 90647: 302, 63072: 303, 49651: 304, 8998: 305, 52975: 306, 8530: 307, 8661: 308, 2280: 309, 55267: 310, 6664: 311, 504: 312, 522: 313, 46967: 314, 50601: 315, 4735: 316, 86345: 317, 26712: 318, 72395: 319, 2548: 320, 52604: 321, 87298: 322, 8838: 323, 26152: 324, 106766: 325, 5265: 326, 4247: 327, 8879: 328, 1816: 329, 34334: 330, 6887: 331, 3371: 332, 62374: 333, 7976: 334, 5909: 335, 68952: 336, 4282: 337, 27822: 338, 869: 339, 44972: 340, 5389: 341, 1744: 342, 5668: 343, 4133: 344, 580: 345, 4915: 346, 7981: 347, 3961: 348, 3680: 349, 54004: 350, 4480: 351, 127136: 352, 2902: 353, 1973: 354, 4410: 355, 3700: 356, 57274: 357, 27020: 358, 79553: 359, 27838: 360, 5382: 361, 5334: 362, 3794: 363, 8581: 364, 3428: 365, 7056: 366, 5012: 367, 5752: 368, 30898: 369, 2964: 370, 3833: 371, 1190: 372, 302: 373, 1365: 374, 3932: 375, 470: 376, 92535: 377, 26178: 378, 489: 379, 5056: 380, 2771: 381, 7396: 382, 4191: 383, 4783: 384, 4317: 385, 7572: 386, 4563: 387, 5380: 388, 3725: 389, 358: 390, 6935: 391, 88785: 392, 71732: 393, 7579: 394, 60943: 395, 6480: 396, 70599: 397, 6868: 398, 48783: 399, 4166: 400, 8499: 401, 7294: 402, 7505: 403, 2697: 404, 3640: 405, 33499: 406, 8927: 407, 53207: 408, 68194: 409, 115216: 410, 4801: 411, 98124: 412, 3029: 413, 1322: 414, 74946: 415, 78637: 416, 241: 417, 32582: 418, 59333: 419, 2634: 420, 79139: 421, 3406: 422, 74580: 423, 74530: 424, 4682: 425, 2164: 426, 2283: 427, 2149: 428, 240: 429, 1631: 430, 875: 431, 7701: 432, 25788: 433, 8129: 434, 58301: 435, 46337: 436, 97673: 437, 1446: 438, 113064: 439, 89759: 440, 87522: 441, 81083: 442, 83976: 443, 1601: 444, 3655: 445, 100163: 446, 3550: 447, 95207: 448, 2689: 449, 2659: 450, 4273: 451, 54256: 452, 3417: 453, 3739: 454, 45837: 455, 7062: 456, 6264: 457, 4251: 458, 1668: 459, 110655: 460, 8584: 461, 870: 462, 89837: 463, 83132: 464, 57532: 465, 92264: 466, 113862: 467, 81512: 468, 1783: 469, 1996: 470, 4999: 471, 781: 472, 8987: 473, 6249: 474, 6989: 475, 31270: 476, 32369: 477, 6322: 478, 5359: 479, 143385: 480, 56805: 481, 5732: 482, 6237: 483, 6419: 484, 7382: 485, 96563: 486, 5649: 487, 7010: 488, 4810: 489, 2024: 490, 4862: 491, 4486: 492, 4231: 493, 4925: 494, 4710: 495, 3922: 496, 4832: 497, 3810: 498, 129937: 499, 2500: 500, 7809: 501, 38992: 502, 695: 503, 1397: 504, 331: 505, 2996: 506, 40614: 507, 135518: 508, 66019: 509, 7212: 510, 7700: 511, 49220: 512, 8933: 513, 34326: 514, 25753: 515, 70697: 516, 5633: 517, 2536: 518, 30894: 519, 6228: 520, 8576: 521, 5097: 522, 569: 523, 7835: 524, 5874: 525, 31952: 526, 6927: 527, 217: 528, 8612: 529, 8190: 530, 8600: 531, 7107: 532, 4360: 533, 27839: 534, 31422: 535, 5343: 536, 1564: 537, 68959: 538, 25962: 539, 118082: 540, 6559: 541, 1750: 542, 69685: 543, 26422: 544, 98829: 545, 8790: 546, 56069: 547, 33558: 548, 83506: 549, 56095: 550, 2835: 551, 56336: 552, 70159: 553, 2552: 554, 36289: 555, 72683: 556, 27899: 557, 51698: 558, 57951: 559, 1826: 560, 80615: 561, 92681: 562, 3240: 563, 2173: 564, 3657: 565, 5440: 566, 6920: 567, 95858: 568, 72104: 569, 106441: 570, 48972: 571, 136592: 572, 63826: 573, 9005: 574, 966: 575, 8019: 576, 8871: 577, 1624: 578, 5841: 579, 2817: 580, 5696: 581, 6940: 582, 40833: 583, 8840: 584, 88272: 585, 90357: 586, 6414: 587, 96815: 588, 8745: 589, 8057: 590, 6356: 591, 4500: 592, 69746: 593, 8253: 594, 6103: 595, 5773: 596, 1181: 597, 44613: 598, 4863: 599, 408: 600, 6114: 601, 32892: 602, 32797: 603, 6665: 604, 54785: 605, 3531: 606, 126106: 607, 2824: 608, 7407: 609, 130490: 610, 54281: 611, 84160: 612, 89047: 613, 93498: 614, 98369: 615, 7316: 616, 27815: 617, 82167: 618, 6757: 619, 4200: 620, 6800: 621, 53999: 622, 54787: 623, 939: 624, 3737: 625, 4688: 626, 4350: 627, 78316: 628, 25901: 629, 26025: 630, 27857: 631, 54910: 632, 112767: 633, 148652: 634, 5796: 635, 6211: 636, 104726: 637, 100517: 638, 5826: 639, 8142: 640, 94939: 641, 71490: 642, 72479: 643, 74156: 644, 5054: 645, 2839: 646, 6739: 647, 133195: 648, 49394: 649, 25839: 650, 1896: 651, 3799: 652, 4077: 653, 4371: 654, 6794: 655, 6203: 656, 110110: 657, 47714: 658, 120392: 659, 145307: 660, 8722: 661, 2210: 662, 6579: 663, 34129: 664, 57038: 665, 26915: 666, 56869: 667, 460: 668, 74486: 669, 80599: 670, 27075: 671, 152173: 672, 5092: 673, 33646: 674, 51939: 675, 71525: 676, 112911: 677, 65088: 678, 5568: 679, 5560: 680, 48165: 681, 49299: 682, 114766: 683, 97817: 684, 85316: 685, 109042: 686, 118326: 687, 124859: 688, 96530: 689, 96565: 690, 2767: 691, 4106: 692, 30712: 693, 1854: 694, 26835: 695, 26731: 696, 26732: 697, 26564: 698, 97168: 699, 26271: 700, 26180: 701, 116887: 702, 106542: 703, 8903: 704, 103543: 705, 127319: 706, 4662: 707, 7245: 708, 7895: 709, 43558: 710, 80590: 711, 86028: 712, 89427: 713, 92048: 714, 92198: 715, 2032: 716, 95508: 717, 98279: 718, 99992: 719, 103502: 720, 25842: 721, 64321: 722, 89300: 723, 3302: 724, 60135: 725, 43635: 726, 76763: 727, 74089: 728, 78111: 729, 41714: 730, 34608: 731, 57418: 732, 78967: 733, 66200: 734, 2674: 735, 34002: 736, 70751: 737, 2481: 738, 2128: 739, 4927: 740, 4383: 741, 4082: 742, 5521: 743, 8853: 744, 2631: 745, 8003: 746, 7921: 747, 7883: 748, 7881: 749, 7354: 750, 4606: 751, 7300: 752, 6684: 753, 6671: 754, 4716: 755, 5646: 756, 87383: 757, 4831: 758, 132157: 759, 135536: 760, 136654: 761, 4003: 762, 3487: 763, 4319: 764, 4401: 765, 5182: 766, 27441: 767, 46772: 768, 5223: 769, 1349: 770, 980: 771, 775: 772, 697: 773, 98473: 774, 98611: 775, 103444: 776, 104321: 777, 1928: 778, 3960: 779, 140739: 780, 3939: 781, 5122: 782, 4608: 783, 5038: 784, 106870: 785, 112070: 786, 73344: 787, 889: 788, 1369: 789, 1550: 790, 1998: 791, 2091: 792, 2855: 793, 3041: 794, 3661: 795, 3662: 796, 104339: 797, 105355: 798, 108076: 799, 109205: 800, 160440: 801, 6022: 802, 6679: 803, 8273: 804, 57845: 805, 1903: 806, 6033: 807, 7260: 808, 8453: 809, 60086: 810, 98933: 811, 99609: 812, 99615: 813, 108795: 814, 127728: 815, 143255: 816, 142997: 817, 139915: 818, 147845: 819, 1044: 820, 118898: 821, 128606: 822, 136018: 823, 140247: 824, 142258: 825, 151307: 826, 59: 827, 160656: 828, 219: 829, 304: 830, 127114: 831, 96849: 832, 1654: 833, 140755: 834}\n",
      "feature_stats: {'title_length': {'min': 10, 'max': 121, 'mean_fillna': 26}, 'year_of_release': {'min': 1921.0, 'max': 2016.0, 'mean_fillna': 1991.6750823271132}, 'average_rating': {'min': 0.5, 'max': 5.0, 'mean_fillna': 3.4190898791540785}, 'count_ratings': {'min': 1, 'max': 75, 'mean_fillna': 6}, 'tmdb_vote_average': {'min': 0.0, 'max': 8.234, 'mean_fillna': 6.986061224489797}, 'tmdb_popularity': {'min': 0.0387, 'max': 2.1368, 'mean_fillna': 0.30788775510204086}, 'tmdb_budget': {'min': 0.0, 'max': 5750000.0, 'mean_fillna': 229591.83673469388}, 'tmdb_revenue': {'min': 0.0, 'max': 16661077.0, 'mean_fillna': 442068.5918367347}, 'tmdb_runtime': {'min': 40.0, 'max': 180.0, 'mean_fillna': 85.20408163265306}, 'tmdb_vote_count': {'min': 0.0, 'max': 624.0, 'mean_fillna': 65.06122448979592}, 'tmdb_profit': {'min': -4316992.0, 'max': 16661077.0, 'mean_fillna': 212476.75510204083}}\n",
      "tfidf_vectorizers: {'Genre_tfidf': TfidfVectorizer(), 'Tags': TfidfVectorizer(), 'title_tfidf': TfidfVectorizer(), 'tmdb_cast': TfidfVectorizer(), 'tmdb_director': TfidfVectorizer()}\n",
      "one_hot_columns: {'tmdb_original_language': ['lang_de', 'lang_en', 'lang_fr', 'lang_unknown']}\n",
      "Handling model User_based\n",
      "Training split predictions\n",
      "- computing metric mae\n",
      "- computing metric rmse\n",
      "- computing metric accuracy\n",
      "Training loo predictions\n",
      "Training full predictions\n",
      "Résultats de l'évaluation des 4 modèles :\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mae</th>\n",
       "      <th>rmse</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>hit_rate</th>\n",
       "      <th>precision</th>\n",
       "      <th>novelty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>User_based</th>\n",
       "      <td>0.740548</td>\n",
       "      <td>0.958466</td>\n",
       "      <td>0.4071</td>\n",
       "      <td>0.018692</td>\n",
       "      <td>0.000935</td>\n",
       "      <td>0.710489</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 mae      rmse  accuracy  hit_rate  precision   novelty\n",
       "User_based  0.740548  0.958466    0.4071  0.018692   0.000935  0.710489"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 mae      rmse  accuracy  hit_rate  precision   novelty\n",
      "User_based  0.740548  0.958466    0.4071  0.018692   0.000935  0.710489\n",
      "Evaluation report successfully exported to: mlsmm2156\\data\\small\\evaluations\\evaluation_report_2025_05_26.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "AVAILABLE_METRICS = {\n",
    "    \"split\": {\n",
    "        \"mae\": (accuracy.mae, {'verbose': False}),\n",
    "        \"rmse\": (accuracy.rmse, {'verbose': False}),\n",
    "        \"accuracy\": (get_accuracy, {}),\n",
    "    },\n",
    "    \"loo\": {\n",
    "        \"hit_rate\": (get_hit_rate, {}),\n",
    "        \"precision\": (get_precision, {}),\n",
    "    },\n",
    "    \"full\": {\n",
    "        \"novelty\": (get_novelty, {}),\n",
    "    \n",
    "     } \n",
    "}\n",
    "\n",
    "sp_ratings = load_ratings(surprise_format=True)\n",
    "precomputed_dict = precompute_information()\n",
    "# afficher les informations pré-calculées\n",
    "print(\"Informations pré-calculées :\")\n",
    "for key, value in precomputed_dict.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "evaluation_report = create_evaluation_report(EvalConfig, sp_ratings, precomputed_dict, AVAILABLE_METRICS)\n",
    "print(\"Résultats de l'évaluation des 4 modèles :\")\n",
    "display(evaluation_report)   \n",
    "print(evaluation_report)  \n",
    "export_evaluation_report(evaluation_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460cc779",
   "metadata": {},
   "source": [
    "Commentaire:\n",
    "MAE / RMSE :\n",
    "La baseline 4 est de loin la plus performante et donc la plus précise selon ces deux indicateurs. Elle est suivie par la baseline 3. En revanche la baseline 1 est la moins performante ce qui en fait le modèle le moins précis.\n",
    "\n",
    "Hit Rate :\n",
    "Le taux de succès est très faible pour les baselines 1, 2 et 3, avec seulement environ 0,2 %, 0,4 % et 0,6 % respectivement dans ce cas. Cela montre leur incapacité à recommander efficacement des items pertinents. En revanche, la baseline 4 se distingue avec un Hit Rate nettement supérieur (5,2 %).\n",
    "\n",
    "Novelty :\n",
    "Les baselines 1, 2 et 3 obtiennent des scores de novelty relativement élevés, ce qui signifie qu’elles recommandent des items moins populaires. À l’inverse, la baseline 4 présente une valeur de nouveauté nettement plus faible ce qui indique qu’elle recommande majoritairement des films très vus. Cela maximise la précision mais se fait au détriment de la diversité des recommandations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
