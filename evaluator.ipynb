{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a665885b",
   "metadata": {},
   "source": [
    "# Evaluator Module\n",
    "The Evaluator module creates evaluation reports.\n",
    "\n",
    "Reports contain evaluation metrics depending on models specified in the evaluation config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aaf9140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reloads modules automatically before entering the execution of code\n",
    "#load_ext autoreload\n",
    "#autoreload 2\n",
    "\n",
    "# third parties imports\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "# -- add new imports here --\n",
    "\n",
    "# local imports\n",
    "from configs import EvalConfig\n",
    "from constants import Constant as C\n",
    "from loaders import export_evaluation_report\n",
    "from loaders import load_ratings\n",
    "from models import get_top_n\n",
    "# -- add new imports here --\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import SVD\n",
    "from surprise import accuracy\n",
    "from configs import EvalConfig \n",
    "from surprise import Dataset, Reader, accuracy\n",
    "from constants import Constant as C\n",
    "from surprise.model_selection import LeaveOneOut\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47c24a4",
   "metadata": {},
   "source": [
    "# 1. Model validation functions\n",
    "Validation functions are a way to perform crossvalidation on recommender system models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6d82188",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ratings(surprise_format=False):\n",
    "    df_ratings = pd.read_csv(C.EVIDENCE_PATH / C.RATINGS_FILENAME)\n",
    "    if surprise_format:\n",
    "        reader = Reader(rating_scale=C.RATINGS_SCALE)\n",
    "        data = Dataset.load_from_df(df_ratings[['userId', 'movieId', 'rating']], reader)\n",
    "        return data\n",
    "    else:\n",
    "        return df_ratings\n",
    "\n",
    "def generate_split_predictions(algo, ratings_dataset, eval_config):\n",
    "    \"\"\"\n",
    "    Generate predictions on a random test set specified in eval_config.\n",
    "    \n",
    "    Parameters:\n",
    "        algo: A Surprise algorithm instance (e.g., SVD, KNNBasic).\n",
    "        ratings_dataset: A Surprise Dataset object.\n",
    "        eval_config: An EvalConfig object containing evaluation parameters (e.g., test_size).\n",
    "    \n",
    "    Returns:\n",
    "        List of predictions made by the algorithm on the test set.\n",
    "    \"\"\"\n",
    "    # Récupérer la proportion test depuis eval_config\n",
    "    test_size = eval_config.test_size\n",
    "    # Diviser le dataset en train/test\n",
    "    trainset, testset = train_test_split(ratings_dataset, test_size=test_size)\n",
    "    # Entraîner le modèle sur le trainset\n",
    "    algo.fit(trainset)\n",
    "    # Faire des prédictions sur le testset\n",
    "    predictions = algo.test(testset)\n",
    "    return predictions\n",
    "\n",
    "\"\"\"\"\n",
    "def generate_loo_top_n(algo, ratings_dataset, eval_config):\n",
    "    #Generate top-n recommendations for each user on a random Leave-one-out split (LOO)\n",
    "    #leaveOneOut object with one split\n",
    "    loo = LeaveOneOut(n_splits=1)\n",
    "    # Split the dataset into training and testing sets\n",
    "    trainset, testset = next(loo.split(ratings_dataset))\n",
    "    # Train the algorithm on the training set\n",
    "    algo.fit(trainset)\n",
    "    # Generate the anti-testset\n",
    "    anti_testset = trainset.build_anti_testset()\n",
    "    # Generate predictions on the anti-testset\n",
    "    predictions = algo.test(anti_testset)\n",
    "    # Get top-N recommendations\n",
    "    anti_testset_top_n = get_top_n(predictions, n=eval_config.top_n_value)\n",
    "    return anti_testset_top_n, testset \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\"\n",
    "def generate_full_top_n(algo, ratings_dataset, eval_config):\n",
    "    #Generate top-n recommendations for each user with full training set (LOO)\n",
    "    # Construire l’ensemble d’entraînement complet à partir de toutes les données\n",
    "    full_trainset = ratings_dataset.build_full_trainset()\n",
    "\n",
    "    # Entraîner l’algorithme sur toutes les données disponibles\n",
    "    algo.fit(full_trainset)\n",
    "\n",
    "    # Générer le anti-testset : tous les items que chaque utilisateur n’a pas encore notés\n",
    "    anti_testset = full_trainset.build_anti_testset()\n",
    "\n",
    "    # Générer les prédictions sur le anti-testset\n",
    "    predictions = algo.test(anti_testset)\n",
    "\n",
    "    # Extraire les top-N recommandations\n",
    "    anti_testset_top_n = get_top_n(predictions, n=eval_config.top_n_value)\n",
    "\n",
    "    return anti_testset_top_n\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "def precompute_information():\n",
    "    # Returns a dictionary that precomputes relevant information for evaluating in full mode\n",
    "    \n",
    "    #Dictionary keys:\n",
    "   # - precomputed_dict[\"item_to_rank\"] : contains a dictionary mapping movie ids to rankings\n",
    "   # - (-- for your project, add other relevant information here -- )\n",
    "    \n",
    "  \n",
    "    ratings = load_ratings()\n",
    "    # Compter les évaluations par film et trier par popularité décroissante\n",
    "    item_counts = ratings['movieId'].value_counts().sort_values(ascending=False)\n",
    "    # Mapper chaque film à son rang de popularité (1 = plus populaire)\n",
    "    item_to_rank = {movie: idx + 1 for idx, movie in enumerate(item_counts.index)}\n",
    "    \n",
    "    return {'item_to_rank': item_to_rank}            \n",
    "\"\"\"\n",
    "\n",
    "def create_evaluation_report(eval_config, sp_ratings, precomputed_dict, available_metrics):\n",
    "    \"\"\" Create a DataFrame evaluating various models on metrics specified in an evaluation config.  \n",
    "    \"\"\"\n",
    "    evaluation_dict = {}\n",
    "    for model_name, model, arguments in eval_config.models:\n",
    "        print(f'Handling model {model_name}')\n",
    "        algo = model(**arguments)\n",
    "        evaluation_dict[model_name] = {}\n",
    "        \n",
    "        # Type 1 : split evaluations\n",
    "        if len(eval_config.split_metrics) > 0:\n",
    "            print('Training split predictions')\n",
    "            predictions = generate_split_predictions(algo, sp_ratings, eval_config)\n",
    "            for metric in eval_config.split_metrics:\n",
    "                print(f'- computing metric {metric}')\n",
    "                assert metric in available_metrics['split']\n",
    "                evaluation_function, parameters =  available_metrics[\"split\"][metric]\n",
    "                evaluation_dict[model_name][metric] = evaluation_function(predictions, **parameters) \n",
    "\n",
    "        \"\"\"  # Type 2 : loo evaluations\n",
    "        if len(eval_config.loo_metrics) > 0:\n",
    "            print('Training loo predictions')\n",
    "            anti_testset_top_n, testset = generate_loo_top_n(algo, sp_ratings, eval_config)\n",
    "            for metric in eval_config.loo_metrics:\n",
    "                assert metric in available_metrics['loo']\n",
    "                evaluation_function, parameters =  available_metrics[\"loo\"][metric]\n",
    "                evaluation_dict[model_name][metric] = evaluation_function(anti_testset_top_n, testset, **parameters)\n",
    "        \n",
    "        # Type 3 : full evaluations\n",
    "        if len(eval_config.full_metrics) > 0:\n",
    "            print('Training full predictions')\n",
    "            anti_testset_top_n = generate_full_top_n(algo, sp_ratings, eval_config)\n",
    "            for metric in eval_config.full_metrics:\n",
    "                assert metric in available_metrics['full']\n",
    "                evaluation_function, parameters =  available_metrics[\"full\"][metric]\n",
    "                evaluation_dict[model_name][metric] = evaluation_function(\n",
    "                    anti_testset_top_n,\n",
    "                    **precomputed_dict,\n",
    "                    **parameters\n",
    "                )\"\"\"\n",
    "        \n",
    "    return pd.DataFrame.from_dict(evaluation_dict).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e83d1d",
   "metadata": {},
   "source": [
    "# 2. Evaluation metrics\n",
    "Implement evaluation metrics for either rating predictions (split metrics) or for top-n recommendations (loo metric, full metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1849e55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' def get_hit_rate(anti_testset_top_n, testset):\\n    #Compute the average hit over the users (loo metric)\\n    \\n    #A hit (1) happens when the movie in the testset has been picked by the top-n recommender\\n    #A fail (0) happens when the movie in the testset has not been picked by the top-n recommender\\n    \\n   #implement the function get_hit_rate \\n    hits = 0\\n    total = len(testset)  \\n\\n    # Iterate through each entry in the testset\\n    for user_id, movie_id, _ in testset:\\n        top_n_recommendations = anti_testset_top_n.get(user_id, [])\\n        if movie_id in [recommended_movie[0] for recommended_movie in top_n_recommendations]:\\n            hits += 1 \\n\\n    hit_rate = hits / total if total > 0 else 0\\n    return hit_rate\\n\\n\\ndef get_novelty(anti_testset_top_n, item_to_rank):\\n    #Compute the average novelty of the top-n recommendation over the users (full metric)\\n    \\n    #The novelty is defined as the average ranking of the movies recommended\\n    \\n    total_rank = 0\\n    num_entries = 0\\n    total_items = len(item_to_rank)\\n    for user_recommendations in anti_testset_top_n.values():\\n        for movie_id, _ in user_recommendations:\\n            total_rank += item_to_rank.get(movie_id, total_items + 1)\\n            num_entries += 1\\n    average_rank_sum = total_rank / num_entries if num_entries > 0 else 0\\n    normalized_novelty = average_rank_sum / total_items  # Normalization step\\n    return normalized_novelty'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" def get_hit_rate(anti_testset_top_n, testset):\n",
    "    #Compute the average hit over the users (loo metric)\n",
    "    \n",
    "    #A hit (1) happens when the movie in the testset has been picked by the top-n recommender\n",
    "    #A fail (0) happens when the movie in the testset has not been picked by the top-n recommender\n",
    "    \n",
    "   #implement the function get_hit_rate \n",
    "    hits = 0\n",
    "    total = len(testset)  \n",
    "\n",
    "    # Iterate through each entry in the testset\n",
    "    for user_id, movie_id, _ in testset:\n",
    "        top_n_recommendations = anti_testset_top_n.get(user_id, [])\n",
    "        if movie_id in [recommended_movie[0] for recommended_movie in top_n_recommendations]:\n",
    "            hits += 1 \n",
    "\n",
    "    hit_rate = hits / total if total > 0 else 0\n",
    "    return hit_rate\n",
    "\n",
    "\n",
    "def get_novelty(anti_testset_top_n, item_to_rank):\n",
    "    #Compute the average novelty of the top-n recommendation over the users (full metric)\n",
    "    \n",
    "    #The novelty is defined as the average ranking of the movies recommended\n",
    "    \n",
    "    total_rank = 0\n",
    "    num_entries = 0\n",
    "    total_items = len(item_to_rank)\n",
    "    for user_recommendations in anti_testset_top_n.values():\n",
    "        for movie_id, _ in user_recommendations:\n",
    "            total_rank += item_to_rank.get(movie_id, total_items + 1)\n",
    "            num_entries += 1\n",
    "    average_rank_sum = total_rank / num_entries if num_entries > 0 else 0\n",
    "    normalized_novelty = average_rank_sum / total_items  # Normalization step\n",
    "    return normalized_novelty\"\"\"\"\"\n",
    "    #return average_rank_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9855b3",
   "metadata": {},
   "source": [
    "# 3. Evaluation workflow\n",
    "Load data, evaluate models and save the experimental outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "704f4d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling model content_1\n",
      "Training split predictions\n",
      "- computing metric mae\n",
      "- computing metric rmse\n",
      "Handling model content_2\n",
      "Training split predictions\n",
      "- computing metric mae\n",
      "- computing metric rmse\n",
      "Handling model content_3\n",
      "Training split predictions\n",
      "- computing metric mae\n",
      "- computing metric rmse\n",
      "Résultats de l'évaluation des 4 modèles :\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mae</th>\n",
       "      <th>rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>content_1</th>\n",
       "      <td>0.746454</td>\n",
       "      <td>0.963854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>content_2</th>\n",
       "      <td>0.758656</td>\n",
       "      <td>0.976203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>content_3</th>\n",
       "      <td>0.749208</td>\n",
       "      <td>0.964692</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                mae      rmse\n",
       "content_1  0.746454  0.963854\n",
       "content_2  0.758656  0.976203\n",
       "content_3  0.749208  0.964692"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                mae      rmse\n",
      "content_1  0.746454  0.963854\n",
      "content_2  0.758656  0.976203\n",
      "content_3  0.749208  0.964692\n",
      "Evaluation report successfully exported to: /Users/delhoutecharles/Documents/GitHub/Recommender-Systeem-/data/small/evaluations/evaluation_report_2025_05_16.csv\n"
     ]
    }
   ],
   "source": [
    "AVAILABLE_METRICS = {\n",
    "    \"split\": {\n",
    "        \"mae\": (accuracy.mae, {'verbose': False}),\n",
    "        \"rmse\": (accuracy.rmse, {'verbose': False}),\n",
    "    },\n",
    "    #\"loo\": {\n",
    "    #   \"hit_rate\": (get_hit_rate, {}),\n",
    "    #},\n",
    "    #\"full\": {\n",
    "    #   \"novelty\": (get_novelty, {}),\n",
    "    #}\n",
    "    \n",
    "    \n",
    "}\n",
    "\n",
    "sp_ratings = load_ratings(surprise_format=True)\n",
    "#precomputed_dict = precompute_information()\n",
    "precomputed_dict = {}  # Placeholder for precomputed information\n",
    "evaluation_report = create_evaluation_report(EvalConfig, sp_ratings, precomputed_dict, AVAILABLE_METRICS)\n",
    "print(\"Résultats de l'évaluation des 4 modèles :\")\n",
    "display(evaluation_report)  \n",
    "print(evaluation_report)  \n",
    "export_evaluation_report(evaluation_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460cc779",
   "metadata": {},
   "source": [
    "Commentaire:\n",
    "MAE / RMSE :\n",
    "La baseline 4 est de loin la plus performante et donc la plus précise selon ces deux indicateurs. Elle est suivie par la baseline 3. En revanche la baseline 1 est la moins performante ce qui en fait le modèle le moins précis.\n",
    "\n",
    "Hit Rate :\n",
    "Le taux de succès est très faible pour les baselines 1, 2 et 3, avec seulement environ 0,2 %, 0,4 % et 0,6 % respectivement dans ce cas. Cela montre leur incapacité à recommander efficacement des items pertinents. En revanche, la baseline 4 se distingue avec un Hit Rate nettement supérieur (5,2 %).\n",
    "\n",
    "Novelty :\n",
    "Les baselines 1, 2 et 3 obtiennent des scores de nouveauté relativement élevés, ce qui signifie qu’elles recommandent des items moins populaires. À l’inverse, la baseline 4 présente une valeur de nouveauté nettement plus faible ce qui indique qu’elle recommande majoritairement des films très vus. Cela maximise la précision mais se fait au détriment de la diversité des recommandations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
