{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a665885b",
   "metadata": {},
   "source": [
    "# Evaluator Module\n",
    "The Evaluator module creates evaluation reports.\n",
    "\n",
    "Reports contain evaluation metrics depending on models specified in the evaluation config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6aaf9140",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import os\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from surprise import SVD, Dataset, Reader, accuracy\n",
    "from surprise.model_selection import train_test_split, LeaveOneOut\n",
    "\n",
    "from configs import EvalConfig\n",
    "from constants import Constant as C\n",
    "import constants as C  # If both are needed for different usages\n",
    "from loaders import load_ratings, load_items, export_evaluation_report\n",
    "from models import get_top_n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47c24a4",
   "metadata": {},
   "source": [
    "# 1. Model validation functions\n",
    "Validation functions are a way to perform crossvalidation on recommender system models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6d82188",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ratings(surprise_format=False):\n",
    "    df_ratings = pd.read_csv(C.EVIDENCE_PATH / C.RATINGS_FILENAME)\n",
    "    if surprise_format:\n",
    "        reader = Reader(rating_scale=C.RATINGS_SCALE)\n",
    "        data = Dataset.load_from_df(df_ratings[['userId', 'movieId', 'rating']], reader)\n",
    "        return data\n",
    "    else:\n",
    "        return df_ratings\n",
    "\n",
    "def generate_split_predictions(algo, ratings_dataset, eval_config):\n",
    "    \"\"\"\n",
    "    Generate predictions on a random test set specified in eval_config.\n",
    "    \n",
    "    Parameters:\n",
    "        algo: A Surprise algorithm instance (e.g., SVD, KNNBasic).\n",
    "        ratings_dataset: A Surprise Dataset object.\n",
    "        eval_config: An EvalConfig object containing evaluation parameters (e.g., test_size).\n",
    "    \n",
    "    Returns:\n",
    "        List of predictions made by the algorithm on the test set.\n",
    "    \"\"\"\n",
    "    # Récupérer la proportion test depuis eval_config\n",
    "    test_size = eval_config.test_size\n",
    "    # Diviser le dataset en train/test\n",
    "    trainset, testset = train_test_split(ratings_dataset, test_size=test_size)\n",
    "    # Entraîner le modèle sur le trainset\n",
    "    algo.fit(trainset)\n",
    "    # Faire des prédictions sur le testset\n",
    "    predictions = algo.test(testset)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def generate_loo_top_n(algo, ratings_dataset, eval_config):\n",
    "    #Generate top-n recommendations for each user on a random Leave-one-out split (LOO)\n",
    "    #leaveOneOut object with one split\n",
    "    loo = LeaveOneOut(n_splits=1)\n",
    "    # Split the dataset into training and testing sets\n",
    "    trainset, testset = next(loo.split(ratings_dataset))\n",
    "    # Train the algorithm on the training set\n",
    "    algo.fit(trainset)\n",
    "    # Generate the anti-testset\n",
    "    anti_testset = trainset.build_anti_testset()\n",
    "    # Generate predictions on the anti-testset\n",
    "    predictions = algo.test(anti_testset)\n",
    "    # Get top-N recommendations\n",
    "    anti_testset_top_n = get_top_n(predictions, n=eval_config.top_n_value)\n",
    "    return anti_testset_top_n, testset\n",
    "\n",
    "\n",
    "def generate_full_top_n(algo, ratings_dataset, eval_config):\n",
    "    #Generate top-n recommendations for each user with full training set (LOO)\n",
    "    # Construire l’ensemble d’entraînement complet à partir de toutes les données\n",
    "    full_trainset = ratings_dataset.build_full_trainset()\n",
    "\n",
    "    # Entraîner l’algorithme sur toutes les données disponibles\n",
    "    algo.fit(full_trainset)\n",
    "\n",
    "    # Générer le anti-testset : tous les items que chaque utilisateur n’a pas encore notés\n",
    "    anti_testset = full_trainset.build_anti_testset()\n",
    "\n",
    "    # Générer les prédictions sur le anti-testset\n",
    "    predictions = algo.test(anti_testset)\n",
    "\n",
    "    # Extraire les top-N recommandations\n",
    "    anti_testset_top_n = get_top_n(predictions, n=eval_config.top_n_value)\n",
    "\n",
    "    return anti_testset_top_n \n",
    "\n",
    "def precompute_information():\n",
    "    \"\"\"Returns a dictionary that precomputes relevant information for evaluating in full mode\n",
    "    and for initializing ContentBased models with pre-computed features and scalers.\n",
    "\n",
    "    Dictionary keys:\n",
    "    - precomputed_dict[\"item_to_rank\"] : contains a dictionary mapping movie ids to rankings (from original code)\n",
    "    - precomputed_dict[\"feature_stats\"] : dictionary of min/max values for numeric features\n",
    "    - precomputed_dict[\"tfidf_vectorizers\"] : dictionary of pre-fitted TfidfVectorizer objects\n",
    "    - precomputed_dict[\"one_hot_columns\"] : dictionary of column names for one-hot encoded features\n",
    "    \"\"\"\n",
    "\n",
    "    ratings = load_ratings()\n",
    "    items = load_items() # Charger les items ici\n",
    "\n",
    "    precomputed_dict = {}\n",
    "\n",
    "    # --- 1. Informations existantes (popularité des films) ---\n",
    "    item_counts = ratings['movieId'].value_counts().sort_values(ascending=False)\n",
    "    precomputed_dict['item_to_rank'] = {movie: idx + 1 for idx, movie in enumerate(item_counts.index)}\n",
    "\n",
    "    # --- 2. Précalcul des statistiques pour la normalisation des features numériques ---\n",
    "    feature_stats = {}\n",
    "\n",
    "    # title_length\n",
    "    df_title_length = items[C.LABEL_COL].apply(lambda x: len(x)).to_frame('title_length')\n",
    "    df_title_length['title_length'] = df_title_length['title_length'].fillna(0).astype(int)\n",
    "    mean_title_length = int(df_title_length['title_length'].replace(0, np.nan).mean())\n",
    "    feature_stats['title_length'] = {\n",
    "        'min': df_title_length['title_length'].min(),\n",
    "        'max': df_title_length['title_length'].max(),\n",
    "        'mean_fillna': mean_title_length\n",
    "    }\n",
    "\n",
    "    # Year_of_release\n",
    "    year = items[C.LABEL_COL].str.extract(r'\\((\\d{4})\\)')[0].astype(float)\n",
    "    df_year = year.to_frame(name='year_of_release')\n",
    "    mean_year = df_year.replace(0, np.nan).mean().iloc[0]\n",
    "    feature_stats['year_of_release'] = {\n",
    "        'min': df_year['year_of_release'].min(),\n",
    "        'max': df_year['year_of_release'].max(),\n",
    "        'mean_fillna': mean_year\n",
    "    }\n",
    "\n",
    "    # average_ratings\n",
    "    average_rating = ratings.groupby('movieId')[C.RATING_COL].mean().rename('average_rating').to_frame()\n",
    "    global_avg_rating = ratings[C.RATING_COL].mean()\n",
    "    feature_stats['average_rating'] = {\n",
    "        'min': average_rating['average_rating'].min(),\n",
    "        'max': average_rating['average_rating'].max(),\n",
    "        'mean_fillna': global_avg_rating\n",
    "    }\n",
    "\n",
    "    # count_ratings\n",
    "    rating_count = ratings.groupby('movieId')[C.RATING_COL].size().rename('rating_count').to_frame()\n",
    "    rating_count['rating_count'] = rating_count['rating_count'].fillna(0).astype(int)\n",
    "    mean_rating_count = int(rating_count['rating_count'].replace(0, np.nan).mean())\n",
    "    feature_stats['count_ratings'] = {\n",
    "        'min': rating_count['rating_count'].min(),\n",
    "        'max': rating_count['rating_count'].max(),\n",
    "        'mean_fillna': mean_rating_count\n",
    "    }\n",
    "\n",
    "    # TMDB features (vote_average, popularity, budget, revenue, runtime, vote_count, profit)\n",
    "    tmdb_path = C.CONTENT_PATH / \"tmdb_full_features.csv\"\n",
    "    df_tmdb = pd.read_csv(tmdb_path).drop_duplicates('movieId').set_index('movieId')\n",
    "\n",
    "    for col in ['vote_average', 'popularity', 'budget', 'revenue', 'runtime', 'vote_count']:\n",
    "        mean_val = df_tmdb[col].mean()\n",
    "        feature_stats[f'tmdb_{col}'] = {\n",
    "            'min': df_tmdb[col].min(),\n",
    "            'max': df_tmdb[col].max(),\n",
    "            'mean_fillna': mean_val\n",
    "        }\n",
    "\n",
    "    # Profit (dérivé du budget et revenue)\n",
    "    df_tmdb['profit'] = df_tmdb['revenue'] - df_tmdb['budget']\n",
    "    mean_profit = df_tmdb['profit'].mean()\n",
    "    feature_stats['tmdb_profit'] = {\n",
    "        'min': df_tmdb['profit'].min(),\n",
    "        'max': df_tmdb['profit'].max(),\n",
    "        'mean_fillna': mean_profit\n",
    "    }\n",
    "    \n",
    "    precomputed_dict['feature_stats'] = feature_stats\n",
    "\n",
    "    # --- 3. Pré-entraînement des TF-IDF Vectorizers ---\n",
    "    tfidf_vectorizers = {}\n",
    "    \n",
    "    # Genre_tfidf\n",
    "    items_copy = items.copy()\n",
    "    items_copy['genre_string'] = items_copy[C.GENRES_COL].fillna('').str.replace('|', ' ', regex=False)\n",
    "    tfidf_genre = TfidfVectorizer()\n",
    "    tfidf_genre.fit(items_copy['genre_string'])\n",
    "    tfidf_vectorizers['Genre_tfidf'] = tfidf_genre\n",
    "\n",
    "    # Tags\n",
    "    tags_path = C.CONTENT_PATH / \"tags.csv\"\n",
    "    df_tags = pd.read_csv(tags_path)\n",
    "    df_tags = df_tags.dropna(subset=['tag'])\n",
    "    df_tags['tag'] = df_tags['tag'].astype(str)\n",
    "    df_tags_grouped = df_tags.groupby('movieId')['tag'].agg(' '.join).to_frame('tags')\n",
    "    tfidf_tags = TfidfVectorizer()\n",
    "    tfidf_tags.fit(df_tags_grouped['tags'])\n",
    "    tfidf_vectorizers['Tags'] = tfidf_tags\n",
    "\n",
    "    # title_tfidf (avec lemmatisation et stopwords)\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "    nltk.download('omw-1.4', quiet=True)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    items_copy['title_string_processed'] = items_copy[C.LABEL_COL].fillna('').apply(lambda x: ' '.join(\n",
    "                                lemmatizer.lemmatize(word) for word in x.split() if word.lower() not in stop_words\n",
    "                            ))\n",
    "    tfidf_title = TfidfVectorizer()\n",
    "    tfidf_title.fit(items_copy['title_string_processed'])\n",
    "    tfidf_vectorizers['title_tfidf'] = tfidf_title\n",
    "\n",
    "    # tmdb_cast\n",
    "    df_tmdb_cast = pd.read_csv(tmdb_path).drop_duplicates('movieId')\n",
    "    df_tmdb_cast['cast'] = df_tmdb_cast['cast'].fillna('')\n",
    "    tfidf_cast = TfidfVectorizer()\n",
    "    tfidf_cast.fit(df_tmdb_cast['cast'])\n",
    "    tfidf_vectorizers['tmdb_cast'] = tfidf_cast\n",
    "\n",
    "    # tmdb_director\n",
    "    df_tmdb_director = pd.read_csv(tmdb_path).drop_duplicates('movieId')\n",
    "    df_tmdb_director['director'] = df_tmdb_director['director'].fillna('')\n",
    "    tfidf_director = TfidfVectorizer()\n",
    "    tfidf_director.fit(df_tmdb_director['director'])\n",
    "    tfidf_vectorizers['tmdb_director'] = tfidf_director\n",
    "\n",
    "    precomputed_dict['tfidf_vectorizers'] = tfidf_vectorizers\n",
    "\n",
    "    # --- 4. Colonnes pour One-Hot Encoding (pour garantir la cohérence des colonnes) ---\n",
    "    one_hot_columns = {}\n",
    "\n",
    "    # tmdb_original_language\n",
    "    df_tmdb_lang = pd.read_csv(tmdb_path).drop_duplicates('movieId')\n",
    "    df_tmdb_lang['original_language'] = df_tmdb_lang['original_language'].fillna('unknown')\n",
    "    one_hot_columns['tmdb_original_language'] = pd.get_dummies(df_tmdb_lang['original_language'], prefix='lang').columns.tolist()\n",
    "    \n",
    "    precomputed_dict['one_hot_columns'] = one_hot_columns\n",
    "\n",
    "    return precomputed_dict        \n",
    "\n",
    "\n",
    "def create_evaluation_report(eval_config, sp_ratings, precomputed_dict, available_metrics):\n",
    "    \"\"\" Create a DataFrame evaluating various models on metrics specified in an evaluation config.  \n",
    "    \"\"\"\n",
    "    evaluation_dict = {}\n",
    "    for model_name, model, arguments in eval_config.models:\n",
    "        print(f'Handling model {model_name}')\n",
    "        algo = model(**arguments)\n",
    "        evaluation_dict[model_name] = {}\n",
    "        \n",
    "        # Type 1 : split evaluations\n",
    "        if len(eval_config.split_metrics) > 0:\n",
    "            print('Training split predictions')\n",
    "            predictions = generate_split_predictions(algo, sp_ratings, eval_config)\n",
    "            for metric in eval_config.split_metrics:\n",
    "                print(f'- computing metric {metric}')\n",
    "                assert metric in available_metrics['split']\n",
    "                evaluation_function, parameters =  available_metrics[\"split\"][metric]\n",
    "                evaluation_dict[model_name][metric] = evaluation_function(predictions, **parameters) \n",
    "\n",
    "        # Type 2 : loo evaluations\n",
    "        if len(eval_config.loo_metrics) > 0:\n",
    "            print('Training loo predictions')\n",
    "            anti_testset_top_n, testset = generate_loo_top_n(algo, sp_ratings, eval_config)\n",
    "            for metric in eval_config.loo_metrics:\n",
    "                assert metric in available_metrics['loo']\n",
    "                evaluation_function, parameters =  available_metrics[\"loo\"][metric]\n",
    "                evaluation_dict[model_name][metric] = evaluation_function(anti_testset_top_n, testset, **parameters)\n",
    "        \n",
    "        # Type 3 : full evaluations\n",
    "        if len(eval_config.full_metrics) > 0:\n",
    "            print('Training full predictions')\n",
    "            anti_testset_top_n = generate_full_top_n(algo, sp_ratings, eval_config)\n",
    "            for metric in eval_config.full_metrics:\n",
    "                assert metric in available_metrics['full']\n",
    "                evaluation_function, parameters = available_metrics[\"full\"][metric]\n",
    "            #    Pour novelty, passer item_to_rank comme argument\n",
    "                if metric == \"novelty\":\n",
    "                    evaluation_dict[model_name][metric] = evaluation_function(\n",
    "                    anti_testset_top_n,\n",
    "                    item_to_rank=precomputed_dict[\"item_to_rank\"],\n",
    "                    **parameters\n",
    "                    )\n",
    "                else:\n",
    "                    evaluation_dict[model_name][metric] = evaluation_function(\n",
    "                    anti_testset_top_n,\n",
    "                    **parameters\n",
    "                    )\n",
    "    return pd.DataFrame.from_dict(evaluation_dict).T \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e83d1d",
   "metadata": {},
   "source": [
    "# 2. Evaluation metrics\n",
    "Implement evaluation metrics for either rating predictions (split metrics) or for top-n recommendations (loo metric, full metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1849e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def get_hit_rate(anti_testset_top_n, testset):\n",
    "    \"\"\"Compute the average hit over the users (loo metric)\n",
    "    \n",
    "    A hit (1) happens when the movie in the testset has been picked by the top-n recommender\n",
    "    A fail (0) happens when the movie in the testset has not been picked by the top-n recommender\"\"\"\n",
    "    \n",
    "   #implement the function get_hit_rate \n",
    "    hits = 0\n",
    "    total = len(testset)  \n",
    "\n",
    "    # Iterate through each entry in the testset\n",
    "    for user_id, movie_id, _ in testset:\n",
    "        top_n_recommendations = anti_testset_top_n.get(user_id, [])\n",
    "        if movie_id in [recommended_movie[0] for recommended_movie in top_n_recommendations]:\n",
    "            hits += 1 \n",
    "\n",
    "    hit_rate = hits / total if total > 0 else 0\n",
    "    return hit_rate\n",
    "\n",
    "\n",
    "def get_novelty(anti_testset_top_n, item_to_rank):\n",
    "    \"\"\"Compute the average novelty of the top-n recommendation over the users (full metric)\n",
    "    \n",
    "    The novelty is defined as the average ranking of the movies recommended\n",
    "    \"\"\"\n",
    "    total_rank_sum = 0\n",
    "    total_items = 0\n",
    "    \n",
    "    if item_to_rank is None:\n",
    "        raise ValueError(\"item_to_rank cannot be None\")\n",
    "\n",
    "    for uid, user_recs in anti_testset_top_n.items():\n",
    "        for iid, _ in user_recs:\n",
    "            rank = item_to_rank.get(iid, None)\n",
    "            if rank is not None:\n",
    "                total_rank_sum += rank\n",
    "                total_items += 1\n",
    " \n",
    "    # Calculate the average novelty\n",
    "    if total_items > 0:\n",
    "        average_rank_sum = total_rank_sum / total_items\n",
    "        normalized_average_rank_sum = (average_rank_sum - 1) / (len(item_to_rank) - 1)\n",
    "    else:\n",
    "        average_rank_sum = 0\n",
    "\n",
    "    return normalized_average_rank_sum\n",
    "\n",
    "def get_accuracy(predictions):\n",
    "        \"\"\"Compute the accuracy for rating predictions (split metric).\n",
    "        Accuracy is defined as the proportion of predictions where the rounded predicted rating equals the true rating.\"\"\"\n",
    "        correct = 0\n",
    "        total = len(predictions)\n",
    "        for pred in predictions:\n",
    "            if round(pred.est) == round(pred.r_ui):\n",
    "                correct += 1\n",
    "        return correct / total if total > 0 else 0\n",
    "\n",
    "def get_precision(anti_testset_top_n, testset):\n",
    "            \"\"\"\n",
    "            Compute the average precision over all users (loo metric).\n",
    "            Precision is defined as the proportion of recommended items that are relevant (i.e., the test item is in the top-n recommendations).\n",
    "            \"\"\"\n",
    "            hits = 0\n",
    "            total_recommended = 0\n",
    "\n",
    "            # Build a mapping from user to their test movie(s)\n",
    "            test_movies = defaultdict(set)\n",
    "            for user_id, movie_id, _ in testset:\n",
    "                test_movies[user_id].add(movie_id)\n",
    "\n",
    "            for user_id, recommendations in anti_testset_top_n.items():\n",
    "                recommended_movies = set([movie_id for movie_id, _ in recommendations])\n",
    "                relevant = test_movies.get(user_id, set())\n",
    "                hits += len(recommended_movies & relevant)\n",
    "                total_recommended += len(recommended_movies)\n",
    "\n",
    "            precision = hits / total_recommended if total_recommended > 0 else 0\n",
    "            return precision\n",
    "\n",
    "def get_diversity(anti_testset_top_n):\n",
    "    \"\"\"\n",
    "    Compute the average diversity of the top-n recommendations (full metric).\n",
    "    Diversity is defined as the average pairwise dissimilarity between recommended items for each user.\n",
    "    Here, we use the cosine distance between tag vectors as a proxy for dissimilarity.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load tags\n",
    "    tags_df = pd.read_csv(r\"C:\\Users\\mathi\\OneDrive - UCL\\Documents\\GitHub\\Recommender-Systeem-\\mlsmm2156\\data\\tiny\\content\\tags.csv\")\n",
    "    # Aggregate tags per movie\n",
    "    movie_tags = tags_df.groupby('movieId')['tag'].apply(lambda tags: ' '.join(str(tag) for tag in tags)).to_dict()\n",
    "\n",
    "    # Build TF-IDF vectors for tags\n",
    "    movie_ids = list(movie_tags.keys())\n",
    "    tag_corpus = [movie_tags[mid] for mid in movie_ids]\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tag_matrix = vectorizer.fit_transform(tag_corpus)\n",
    "    movieid_to_idx = {mid: idx for idx, mid in enumerate(movie_ids)}\n",
    "\n",
    "    def cosine_distance(vec1, vec2):\n",
    "        if np.linalg.norm(vec1) == 0 or np.linalg.norm(vec2) == 0:\n",
    "            return 0\n",
    "        return 1 - np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "    diversities = []\n",
    "    for user_recommendations in anti_testset_top_n.values():\n",
    "        rec_movie_ids = [movie_id for movie_id, _ in user_recommendations]\n",
    "        if len(rec_movie_ids) < 2:\n",
    "            continue\n",
    "        pairwise_distances = []\n",
    "        for i in range(len(rec_movie_ids)):\n",
    "            for j in range(i + 1, len(rec_movie_ids)):\n",
    "                idx1 = movieid_to_idx.get(rec_movie_ids[i])\n",
    "                idx2 = movieid_to_idx.get(rec_movie_ids[j])\n",
    "                if idx1 is not None and idx2 is not None:\n",
    "                    vec1 = tag_matrix[idx1].toarray().flatten()\n",
    "                    vec2 = tag_matrix[idx2].toarray().flatten()\n",
    "                    pairwise_distances.append(cosine_distance(vec1, vec2))\n",
    "        if pairwise_distances:\n",
    "            diversities.append(np.mean(pairwise_distances))\n",
    "    return np.mean(diversities) if diversities else 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9855b3",
   "metadata": {},
   "source": [
    "# 3. Evaluation workflow\n",
    "Load data, evaluate models and save the experimental outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93361d19",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "704f4d2a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'constants' has no attribute 'EVIDENCE_PATH'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 18\u001b[0m\n\u001b[0;32m      1\u001b[0m AVAILABLE_METRICS \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmae\u001b[39m\u001b[38;5;124m\"\u001b[39m: (accuracy\u001b[38;5;241m.\u001b[39mmae, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mverbose\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m      } \n\u001b[0;32m     16\u001b[0m }\n\u001b[1;32m---> 18\u001b[0m sp_ratings \u001b[38;5;241m=\u001b[39m \u001b[43mload_ratings\u001b[49m\u001b[43m(\u001b[49m\u001b[43msurprise_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m precomputed_dict \u001b[38;5;241m=\u001b[39m precompute_information()\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# afficher les informations pré-calculées\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m, in \u001b[0;36mload_ratings\u001b[1;34m(surprise_format)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_ratings\u001b[39m(surprise_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m----> 2\u001b[0m     df_ratings \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[43mC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEVIDENCE_PATH\u001b[49m \u001b[38;5;241m/\u001b[39m C\u001b[38;5;241m.\u001b[39mRATINGS_FILENAME)\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m surprise_format:\n\u001b[0;32m      4\u001b[0m         reader \u001b[38;5;241m=\u001b[39m Reader(rating_scale\u001b[38;5;241m=\u001b[39mC\u001b[38;5;241m.\u001b[39mRATINGS_SCALE)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'constants' has no attribute 'EVIDENCE_PATH'"
     ]
    }
   ],
   "source": [
    "\n",
    "AVAILABLE_METRICS = {\n",
    "    \"split\": {\n",
    "        \"mae\": (accuracy.mae, {'verbose': False}),\n",
    "        \"rmse\": (accuracy.rmse, {'verbose': False}),\n",
    "        \"accuracy\": (get_accuracy, {}),\n",
    "    },\n",
    "    \"loo\": {\n",
    "        \"hit_rate\": (get_hit_rate, {}),\n",
    "        \"precision\": (get_precision, {}),\n",
    "    },\n",
    "    \"full\": {\n",
    "        \"diversity\" : (get_diversity, {}),\n",
    "        \"novelty\": (get_novelty, {}),\n",
    "    \n",
    "     } \n",
    "}\n",
    "\n",
    "sp_ratings = load_ratings(surprise_format=True)\n",
    "\n",
    "precomputed_dict = precompute_information()\n",
    "# afficher les informations pré-calculées\n",
    "print(\"Informations pré-calculées :\")\n",
    "for key, value in precomputed_dict.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "evaluation_report = create_evaluation_report(EvalConfig, sp_ratings, precomputed_dict, AVAILABLE_METRICS)\n",
    "print(\"Résultats de l'évaluation des 4 modèles :\")\n",
    "display(evaluation_report)   \n",
    "model_names = evaluation_report.index.tolist()\n",
    "export_evaluation_report(evaluation_report, model_name= model_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d0963b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from constants import Constant as C\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_evaluation_comparison(evaluation_df=None):\n",
    "    \"\"\"\n",
    "    Affiche un graphique comparatif des scores pour chaque modèle.\n",
    "    Les noms des modèles sont affichés en abscisse, chaque métrique est représentée par une barre.\n",
    "    Si evaluation_df n'est pas fourni, utilise la variable globale evaluation_report.\n",
    "    \"\"\"\n",
    "    if evaluation_df is None:\n",
    "        evaluation_df = evaluation_report\n",
    "\n",
    "    ax = evaluation_df.plot(kind='bar', figsize=(12, 6))\n",
    "    plt.title(\"Comparison of evaluation scores with different similarity mesures\") \n",
    "    plt.ylabel(\"Metric value\")\n",
    "    plt.xlabel(\"Messures\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(title=\"Metrics\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Exemple d'utilisation :\n",
    "plot_evaluation_comparison()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69e42df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_mae_rmse_comparison(evaluation_df=None):\n",
    "    \"\"\"\n",
    "    Affiche un graphique comparatif du MAE et du RMSE pour chaque modèle.\n",
    "    Les noms des modèles sont affichés en abscisse, chaque métrique (MAE, RMSE) est représentée par une barre.\n",
    "    Si evaluation_df n'est pas fourni, utilise la variable globale evaluation_report.\n",
    "    \"\"\"\n",
    "    if evaluation_df is None:\n",
    "        evaluation_df = evaluation_report\n",
    "\n",
    "    metrics = ['mae', 'rmse']\n",
    "    ax = evaluation_df[metrics].plot(kind='bar', figsize=(8, 5))\n",
    "    plt.title(\"Comparison of MAE and RMSE for each model\")\n",
    "    plt.ylabel(\"Metric value\")\n",
    "    plt.xlabel(\"Model\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(title=\"Metrics\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Exemple d'utilisation :\n",
    "#plot_mae_rmse_comparison()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25876971",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from configs import EvalConfig\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_param_variation_metrics(eval_file=None, param_name=\"epochs_\", param_pattern=None, metrics_to_exclude=None, model_prefix=None):\n",
    "    \"\"\"\n",
    "    Compare la variation des métriques en fonction d'un paramètre arbitraire pour tous les modèles dont le nom contient une valeur correspondante.\n",
    "    param_name: nom du paramètre à extraire (ex: \"factors_\", \"k\", \"min_k\", etc.).\n",
    "    param_pattern: regex personnalisée pour extraire la valeur du paramètre depuis le nom du modèle (doit contenir un groupe de capture).\n",
    "                   Si None, utilise des patterns courants selon le param_name.\n",
    "    metrics_to_exclude: liste de noms de colonnes à exclure du tracé (ex: [\"accuracy\"]).\n",
    "    model_prefix: si fourni, ne considère que les modèles dont le nom commence par ce préfixe (ex: \"SVD_epochs_\").\n",
    "    Si eval_file est fourni, charge le DataFrame depuis ce fichier CSV dans le dossier evaluations.\n",
    "    Sinon, utilise la variable globale evaluation_report.\n",
    "    \"\"\"\n",
    "    # Charger le DataFrame depuis le fichier si fourni, sinon utiliser la variable globale\n",
    "    if eval_file is not None:\n",
    "        eval_path = os.path.join(C.EVALUATION_PATH, eval_file)\n",
    "        df = pd.read_csv(eval_path)\n",
    "        if 'name' in df.columns:\n",
    "            df = df.set_index('name')\n",
    "    else:\n",
    "        df = evaluation_report.copy()\n",
    "\n",
    "    # Filtrer selon le préfixe du modèle si demandé\n",
    "    if model_prefix is not None:\n",
    "        df = df[df.index.str.startswith(model_prefix)]\n",
    "\n",
    "    # Définir le pattern de recherche selon le paramètre si non fourni\n",
    "    if param_pattern is None:\n",
    "        if param_name == \"epochs_\":\n",
    "            param_pattern = r'epochs_(\\d+)'\n",
    "        elif param_name == \"ridge_alpha\":\n",
    "            param_pattern = r'_ridge_([0-9.]+)$'\n",
    "        elif param_name == \"k\":\n",
    "            param_pattern = r'_k(\\d+)'\n",
    "        elif param_name == \"min_k\":\n",
    "            param_pattern = r'_min_k(\\d+)'\n",
    "        else:\n",
    "            # Par défaut, cherche _paramname(valeur) ou _paramname_valeur\n",
    "            param_pattern = rf'_{param_name}[_()]([0-9.]+)'\n",
    "\n",
    "    # Fonction pour extraire la valeur du paramètre depuis le nom du modèle\n",
    "    def extract_param(model_name):\n",
    "        match = re.search(param_pattern, model_name)\n",
    "        if match:\n",
    "            try:\n",
    "                val = match.group(1)\n",
    "                return float(val) if '.' in val else int(val)\n",
    "            except Exception:\n",
    "                return None\n",
    "        # Sinon, essayer de trouver dans une colonne de paramètres si elle existe\n",
    "        if param_name in df.columns:\n",
    "            try:\n",
    "                return float(df.loc[model_name, param_name])\n",
    "            except Exception:\n",
    "                return None\n",
    "        return None\n",
    "\n",
    "    # Ajouter une colonne temporaire avec la valeur extraite\n",
    "    df[\"_param_value_tmp\"] = df.index.map(extract_param)\n",
    "    # Garder uniquement les modèles pour lesquels la valeur a été trouvée\n",
    "    df = df[df[\"_param_value_tmp\"].notnull()]\n",
    "    df = df.sort_values(\"_param_value_tmp\")\n",
    "\n",
    "    # Déterminer les métriques à tracer (on exclut la colonne temporaire du paramètre)\n",
    "    if metrics_to_exclude is None:\n",
    "        metrics_to_exclude = [\"index\", param_name, \"accuracy\"]\n",
    "    metrics_to_exclude = set(metrics_to_exclude) | {\"_param_value_tmp\"}\n",
    "    metrics = [col for col in df.columns if col not in metrics_to_exclude and col != \"_param_value_tmp\"]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for metric in metrics:\n",
    "        plt.plot(df[\"_param_value_tmp\"], df[metric], marker='o', label=metric)\n",
    "    plt.xlabel(f\"{param_name}\")\n",
    "    plt.ylabel(\"Metric value\")\n",
    "    plt.title(f\"Variation of metrics as a function of {param_name} value\")\n",
    "    plt.legend(loc='upper right', bbox_to_anchor=(1.15, 1))\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Exemple d'utilisation :\n",
    "plot_param_variation_metrics(eval_file=\"evaluation_report_2025_06_03_5.csv\", param_name=\"epochs_\", metrics_to_exclude=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba53d4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def get_top_models_from_evaluation(metric, top_n=3, evaluations_dir=None, eval_file=None, asc=None):\n",
    "    \"\"\"\n",
    "    Retourne les noms des modèles ayant les meilleurs scores pour une métrique donnée\n",
    "    à partir d'un fichier d'évaluation dans le dossier 'evaluations'.\n",
    "\n",
    "    Args:\n",
    "        metric (str): Le nom de la métrique (ex: 'mae', 'rmse', 'accuracy', etc.).\n",
    "        top_n (int): Le nombre de modèles à retourner.\n",
    "        evaluations_dir (str, optional): Chemin du dossier contenant les fichiers d'évaluation.\n",
    "        eval_file (str, optional): Nom du fichier d'évaluation à utiliser (sinon, prend le plus récent).\n",
    "        asc (bool, optional): True pour les plus faibles valeurs, False pour les plus hautes valeurs.\n",
    "                              Si None, utilise le comportement par défaut selon la métrique.\n",
    "\n",
    "    Returns:\n",
    "        list: Liste des noms de modèles avec les meilleurs scores pour la métrique choisie.\n",
    "    \"\"\"\n",
    "    # Par défaut, utiliser le dossier défini dans la constante C\n",
    "    if evaluations_dir is None:\n",
    "        evaluations_dir = C.EVALUATION_PATH\n",
    "\n",
    "    # Trouver le fichier d'évaluation à utiliser\n",
    "    if eval_file is None:\n",
    "        eval_files = [f for f in os.listdir(evaluations_dir) if f.endswith('.csv')]\n",
    "        if not eval_files:\n",
    "            raise FileNotFoundError(\"Aucun fichier d'évaluation trouvé dans le dossier spécifié.\")\n",
    "        eval_files = sorted(eval_files, key=lambda f: os.path.getmtime(os.path.join(evaluations_dir, f)), reverse=True)\n",
    "        eval_file = eval_files[0]\n",
    "    eval_path = os.path.join(evaluations_dir, eval_file)\n",
    "\n",
    "    # Charger le DataFrame\n",
    "    df = pd.read_csv(eval_path)\n",
    "    if 'name' in df.columns:\n",
    "        df = df.set_index('name')\n",
    "\n",
    "    if metric not in df.columns:\n",
    "        raise ValueError(f\"La métrique '{metric}' n'existe pas dans le fichier d'évaluation.\")\n",
    "\n",
    "    # Pour les métriques où un score plus élevé est meilleur\n",
    "    higher_is_better = ['accuracy', 'hit_rate', 'precision', 'diversity', 'novelty']\n",
    "    if asc is None:\n",
    "        ascending = metric not in higher_is_better\n",
    "    else:\n",
    "        ascending = asc\n",
    "\n",
    "    # Trier et sélectionner les meilleurs modèles\n",
    "    top_models = df.sort_values(metric, ascending=ascending).head(top_n)\n",
    "    return list(top_models.index)\n",
    "\n",
    "# Exemple d'utilisation :\n",
    "# top_models = get_top_models_from_evaluation('accuracy', top_n=5)\n",
    "top_models = get_top_models_from_evaluation('rmse', top_n=8, \n",
    "                                             evaluations_dir=C.EVALUATION_PATH,\n",
    "                                             eval_file='evaluation_report_2025_05_28_6.csv', asc=True)\n",
    "print(top_models)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
