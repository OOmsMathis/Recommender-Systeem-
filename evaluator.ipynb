{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a665885b",
   "metadata": {},
   "source": [
    "# Evaluator Module\n",
    "The Evaluator module creates evaluation reports.\n",
    "\n",
    "Reports contain evaluation metrics depending on models specified in the evaluation config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6aaf9140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        userId  movieId  rating   timestamp\n",
      "0            1       31     2.5  1260759144\n",
      "1            1     1029     3.0  1260759179\n",
      "2            1     1061     3.0  1260759182\n",
      "3            1     1129     2.0  1260759185\n",
      "4            1     1172     4.0  1260759205\n",
      "...        ...      ...     ...         ...\n",
      "99999      671     6268     2.5  1065579370\n",
      "100000     671     6269     4.0  1065149201\n",
      "100001     671     6365     4.0  1070940363\n",
      "100002     671     6385     2.5  1070979663\n",
      "100003     671     6565     3.5  1074784724\n",
      "\n",
      "[100004 rows x 4 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mathi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mathi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\mathi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# reloads modules automatically before entering the execution of code\n",
    "#load_ext autoreload\n",
    "#autoreload 2\n",
    "\n",
    "# third parties imports\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "# -- add new imports here --\n",
    "\n",
    "# local imports\n",
    "from configs import EvalConfig\n",
    "from constants import Constant as C\n",
    "from loaders import export_evaluation_report\n",
    "from loaders import load_ratings\n",
    "from models import get_top_n\n",
    "# -- add new imports here --\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import SVD\n",
    "from surprise import accuracy\n",
    "from configs import EvalConfig \n",
    "from surprise import Dataset, Reader, accuracy\n",
    "from constants import Constant as C\n",
    "from surprise.model_selection import LeaveOneOut\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47c24a4",
   "metadata": {},
   "source": [
    "# 1. Model validation functions\n",
    "Validation functions are a way to perform crossvalidation on recommender system models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922721bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from loaders import load_ratings, load_items # Assurez-vous que ces fonctions sont disponibles\n",
    "import constants as C # Assurez-vous que vos constantes (C.EVIDENCE_PATH, C.RATINGS_FILENAME, etc.) sont bien définies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d82188",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ratings(surprise_format=False):\n",
    "    df_ratings = pd.read_csv(C.EVIDENCE_PATH / C.RATINGS_FILENAME)\n",
    "    if surprise_format:\n",
    "        reader = Reader(rating_scale=C.RATINGS_SCALE)\n",
    "        data = Dataset.load_from_df(df_ratings[['userId', 'movieId', 'rating']], reader)\n",
    "        return data\n",
    "    else:\n",
    "        return df_ratings\n",
    "\n",
    "def generate_split_predictions(algo, ratings_dataset, eval_config):\n",
    "    \"\"\"\n",
    "    Generate predictions on a random test set specified in eval_config.\n",
    "    \n",
    "    Parameters:\n",
    "        algo: A Surprise algorithm instance (e.g., SVD, KNNBasic).\n",
    "        ratings_dataset: A Surprise Dataset object.\n",
    "        eval_config: An EvalConfig object containing evaluation parameters (e.g., test_size).\n",
    "    \n",
    "    Returns:\n",
    "        List of predictions made by the algorithm on the test set.\n",
    "    \"\"\"\n",
    "    # Récupérer la proportion test depuis eval_config\n",
    "    test_size = eval_config.test_size\n",
    "    # Diviser le dataset en train/test\n",
    "    trainset, testset = train_test_split(ratings_dataset, test_size=test_size)\n",
    "    # Entraîner le modèle sur le trainset\n",
    "    algo.fit(trainset)\n",
    "    # Faire des prédictions sur le testset\n",
    "    predictions = algo.test(testset)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def generate_loo_top_n(algo, ratings_dataset, eval_config):\n",
    "    #Generate top-n recommendations for each user on a random Leave-one-out split (LOO)\n",
    "    #leaveOneOut object with one split\n",
    "    loo = LeaveOneOut(n_splits=1)\n",
    "    # Split the dataset into training and testing sets\n",
    "    trainset, testset = next(loo.split(ratings_dataset))\n",
    "    # Train the algorithm on the training set\n",
    "    algo.fit(trainset)\n",
    "    # Generate the anti-testset\n",
    "    anti_testset = trainset.build_anti_testset()\n",
    "    # Generate predictions on the anti-testset\n",
    "    predictions = algo.test(anti_testset)\n",
    "    # Get top-N recommendations\n",
    "    anti_testset_top_n = get_top_n(predictions, n=eval_config.top_n_value)\n",
    "    return anti_testset_top_n, testset\n",
    "\n",
    "\n",
    "def generate_full_top_n(algo, ratings_dataset, eval_config):\n",
    "    #Generate top-n recommendations for each user with full training set (LOO)\n",
    "    # Construire l’ensemble d’entraînement complet à partir de toutes les données\n",
    "    full_trainset = ratings_dataset.build_full_trainset()\n",
    "\n",
    "    # Entraîner l’algorithme sur toutes les données disponibles\n",
    "    algo.fit(full_trainset)\n",
    "\n",
    "    # Générer le anti-testset : tous les items que chaque utilisateur n’a pas encore notés\n",
    "    anti_testset = full_trainset.build_anti_testset()\n",
    "\n",
    "    # Générer les prédictions sur le anti-testset\n",
    "    predictions = algo.test(anti_testset)\n",
    "\n",
    "    # Extraire les top-N recommandations\n",
    "    anti_testset_top_n = get_top_n(predictions, n=eval_config.top_n_value)\n",
    "\n",
    "    return anti_testset_top_n \n",
    "\n",
    "def precompute_information():\n",
    "    \"\"\"Returns a dictionary that precomputes relevant information for evaluating in full mode\n",
    "    and for initializing ContentBased models with pre-computed features and scalers.\n",
    "\n",
    "    Dictionary keys:\n",
    "    - precomputed_dict[\"item_to_rank\"] : contains a dictionary mapping movie ids to rankings (from original code)\n",
    "    - precomputed_dict[\"feature_stats\"] : dictionary of min/max values for numeric features\n",
    "    - precomputed_dict[\"tfidf_vectorizers\"] : dictionary of pre-fitted TfidfVectorizer objects\n",
    "    - precomputed_dict[\"one_hot_columns\"] : dictionary of column names for one-hot encoded features\n",
    "    \"\"\"\n",
    "\n",
    "    ratings = load_ratings()\n",
    "    items = load_items() # Charger les items ici\n",
    "\n",
    "    precomputed_dict = {}\n",
    "\n",
    "    # --- 1. Informations existantes (popularité des films) ---\n",
    "    item_counts = ratings['movieId'].value_counts().sort_values(ascending=False)\n",
    "    precomputed_dict['item_to_rank'] = {movie: idx + 1 for idx, movie in enumerate(item_counts.index)}\n",
    "\n",
    "    # --- 2. Précalcul des statistiques pour la normalisation des features numériques ---\n",
    "    feature_stats = {}\n",
    "\n",
    "    # title_length\n",
    "    df_title_length = items[C.LABEL_COL].apply(lambda x: len(x)).to_frame('title_length')\n",
    "    df_title_length['title_length'] = df_title_length['title_length'].fillna(0).astype(int)\n",
    "    mean_title_length = int(df_title_length['title_length'].replace(0, np.nan).mean())\n",
    "    # Note: On stocke la moyenne pour le fillna si nécessaire, mais le min/max pour la normalisation\n",
    "    feature_stats['title_length'] = {\n",
    "        'min': df_title_length['title_length'].min(),\n",
    "        'max': df_title_length['title_length'].max(),\n",
    "        'mean_fillna': mean_title_length\n",
    "    }\n",
    "\n",
    "    # Year_of_release\n",
    "    year = items[C.LABEL_COL].str.extract(r'\\((\\d{4})\\)')[0].astype(float)\n",
    "    df_year = year.to_frame(name='year_of_release')\n",
    "    mean_year = df_year.replace(0, np.nan).mean().iloc[0]\n",
    "    feature_stats['year_of_release'] = {\n",
    "        'min': df_year['year_of_release'].min(),\n",
    "        'max': df_year['year_of_release'].max(),\n",
    "        'mean_fillna': mean_year\n",
    "    }\n",
    "\n",
    "    # average_ratings\n",
    "    average_rating = ratings.groupby('movieId')[C.RATING_COL].mean().rename('average_rating').to_frame()\n",
    "    global_avg_rating = ratings[C.RATING_COL].mean()\n",
    "    feature_stats['average_rating'] = {\n",
    "        'min': average_rating['average_rating'].min(),\n",
    "        'max': average_rating['average_rating'].max(),\n",
    "        'mean_fillna': global_avg_rating\n",
    "    }\n",
    "\n",
    "    # count_ratings\n",
    "    rating_count = ratings.groupby('movieId')[C.RATING_COL].size().rename('rating_count').to_frame()\n",
    "    rating_count['rating_count'] = rating_count['rating_count'].fillna(0).astype(int)\n",
    "    mean_rating_count = int(rating_count['rating_count'].replace(0, np.nan).mean())\n",
    "    feature_stats['count_ratings'] = {\n",
    "        'min': rating_count['rating_count'].min(),\n",
    "        'max': rating_count['rating_count'].max(),\n",
    "        'mean_fillna': mean_rating_count\n",
    "    }\n",
    "\n",
    "    # TMDB features (vote_average, popularity, budget, revenue, runtime, vote_count, profit)\n",
    "    tmdb_path = C.CONTENT_PATH / \"tmdb_full_features.csv\"\n",
    "    df_tmdb = pd.read_csv(tmdb_path).drop_duplicates('movieId').set_index('movieId')\n",
    "\n",
    "    for col in ['vote_average', 'popularity', 'budget', 'revenue', 'runtime', 'vote_count']:\n",
    "        mean_val = df_tmdb[col].mean()\n",
    "        # On calcule les min/max sur les valeurs disponibles (même si fillna sera appliqué plus tard)\n",
    "        feature_stats[f'tmdb_{col}'] = {\n",
    "            'min': df_tmdb[col].min(),\n",
    "            'max': df_tmdb[col].max(),\n",
    "            'mean_fillna': mean_val\n",
    "        }\n",
    "\n",
    "    # Profit (dérivé du budget et revenue)\n",
    "    df_tmdb['profit'] = df_tmdb['revenue'] - df_tmdb['budget']\n",
    "    mean_profit = df_tmdb['profit'].mean()\n",
    "    feature_stats['tmdb_profit'] = {\n",
    "        'min': df_tmdb['profit'].min(),\n",
    "        'max': df_tmdb['profit'].max(),\n",
    "        'mean_fillna': mean_profit\n",
    "    }\n",
    "    \n",
    "    precomputed_dict['feature_stats'] = feature_stats\n",
    "\n",
    "    # --- 3. Pré-entraînement des TF-IDF Vectorizers ---\n",
    "    tfidf_vectorizers = {}\n",
    "    \n",
    "    # Genre_tfidf\n",
    "    items_copy = items.copy() # Travailler sur une copie pour éviter SettingWithCopyWarning\n",
    "    items_copy['genre_string'] = items_copy[C.GENRES_COL].fillna('').str.replace('|', ' ', regex=False)\n",
    "    tfidf_genre = TfidfVectorizer()\n",
    "    tfidf_genre.fit(items_copy['genre_string'])\n",
    "    tfidf_vectorizers['Genre_tfidf'] = tfidf_genre\n",
    "\n",
    "    # Tags\n",
    "    tags_path = C.CONTENT_PATH / \"tags.csv\"\n",
    "    df_tags = pd.read_csv(tags_path)\n",
    "    df_tags = df_tags.dropna(subset=['tag'])\n",
    "    df_tags['tag'] = df_tags['tag'].astype(str)\n",
    "    df_tags_grouped = df_tags.groupby('movieId')['tag'].agg(' '.join).to_frame('tags')\n",
    "    tfidf_tags = TfidfVectorizer()\n",
    "    tfidf_tags.fit(df_tags_grouped['tags'])\n",
    "    tfidf_vectorizers['Tags'] = tfidf_tags\n",
    "\n",
    "    # title_tfidf (avec lemmatisation et stopwords)\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "    nltk.download('omw-1.4', quiet=True)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Créer une colonne traitée pour le TF-IDF du titre\n",
    "    items_copy['title_string_processed'] = items_copy[C.LABEL_COL].fillna('').apply(lambda x: ' '.join(\n",
    "                                lemmatizer.lemmatize(word) for word in x.split() if word.lower() not in stop_words\n",
    "                            ))\n",
    "    tfidf_title = TfidfVectorizer()\n",
    "    tfidf_title.fit(items_copy['title_string_processed'])\n",
    "    tfidf_vectorizers['title_tfidf'] = tfidf_title\n",
    "\n",
    "    # tmdb_cast\n",
    "    df_tmdb_cast = pd.read_csv(tmdb_path).drop_duplicates('movieId')\n",
    "    df_tmdb_cast['cast'] = df_tmdb_cast['cast'].fillna('')\n",
    "    tfidf_cast = TfidfVectorizer()\n",
    "    tfidf_cast.fit(df_tmdb_cast['cast'])\n",
    "    tfidf_vectorizers['tmdb_cast'] = tfidf_cast\n",
    "\n",
    "    # tmdb_director\n",
    "    df_tmdb_director = pd.read_csv(tmdb_path).drop_duplicates('movieId')\n",
    "    df_tmdb_director['director'] = df_tmdb_director['director'].fillna('')\n",
    "    tfidf_director = TfidfVectorizer()\n",
    "    tfidf_director.fit(df_tmdb_director['director'])\n",
    "    tfidf_vectorizers['tmdb_director'] = tfidf_director\n",
    "    \n",
    "    # tfidf_relevance (basé sur genome-scores et genome-tags)\n",
    "    genome_tags_path = C.CONTENT_PATH / \"genome-tags.csv\"\n",
    "    genome_scores_path = C.CONTENT_PATH / \"genome-scores.csv\"\n",
    "    df_tags_genome = pd.read_csv(genome_tags_path)\n",
    "    df_scores_genome = pd.read_csv(genome_scores_path)\n",
    "    df_merged_genome = df_scores_genome.merge(df_tags_genome, on='tagId')\n",
    "    df_merged_genome['tag'] = df_merged_genome['tag'].astype(str)\n",
    "    df_texts_genome = df_merged_genome.groupby('movieId')['tag'].apply(lambda x: ' '.join(x)).to_frame('tags')\n",
    "    tfidf_relevance = TfidfVectorizer()\n",
    "    tfidf_relevance.fit(df_texts_genome['tags'])\n",
    "    tfidf_vectorizers['tfidf_relevance'] = tfidf_relevance\n",
    "\n",
    "    precomputed_dict['tfidf_vectorizers'] = tfidf_vectorizers\n",
    "\n",
    "    # --- 4. Colonnes pour One-Hot Encoding (pour garantir la cohérence des colonnes) ---\n",
    "    one_hot_columns = {}\n",
    "\n",
    "    # tmdb_original_language\n",
    "    df_tmdb_lang = pd.read_csv(tmdb_path).drop_duplicates('movieId')\n",
    "    df_tmdb_lang['original_language'] = df_tmdb_lang['original_language'].fillna('unknown')\n",
    "    # Obtenir la liste complète des colonnes dummy possibles\n",
    "    one_hot_columns['tmdb_original_language'] = pd.get_dummies(df_tmdb_lang['original_language'], prefix='lang').columns.tolist()\n",
    "    \n",
    "    precomputed_dict['one_hot_columns'] = one_hot_columns\n",
    "\n",
    "    return precomputed_dict        \n",
    "\n",
    "\n",
    "def create_evaluation_report(eval_config, sp_ratings, precomputed_dict, available_metrics):\n",
    "    \"\"\" Create a DataFrame evaluating various models on metrics specified in an evaluation config.  \n",
    "    \"\"\"\n",
    "    evaluation_dict = {}\n",
    "    for model_name, model, arguments in eval_config.models:\n",
    "        print(f'Handling model {model_name}')\n",
    "        algo = model(**arguments)\n",
    "        evaluation_dict[model_name] = {}\n",
    "        \n",
    "        # Type 1 : split evaluations\n",
    "        if len(eval_config.split_metrics) > 0:\n",
    "            print('Training split predictions')\n",
    "            predictions = generate_split_predictions(algo, sp_ratings, eval_config)\n",
    "            for metric in eval_config.split_metrics:\n",
    "                print(f'- computing metric {metric}')\n",
    "                assert metric in available_metrics['split']\n",
    "                evaluation_function, parameters =  available_metrics[\"split\"][metric]\n",
    "                evaluation_dict[model_name][metric] = evaluation_function(predictions, **parameters) \n",
    "\n",
    "        # Type 2 : loo evaluations\n",
    "        if len(eval_config.loo_metrics) > 0:\n",
    "            print('Training loo predictions')\n",
    "            anti_testset_top_n, testset = generate_loo_top_n(algo, sp_ratings, eval_config)\n",
    "            for metric in eval_config.loo_metrics:\n",
    "                assert metric in available_metrics['loo']\n",
    "                evaluation_function, parameters =  available_metrics[\"loo\"][metric]\n",
    "                evaluation_dict[model_name][metric] = evaluation_function(anti_testset_top_n, testset, **parameters)\n",
    "        \n",
    "        # Type 3 : full evaluations\n",
    "        if len(eval_config.full_metrics) > 0:\n",
    "            print('Training full predictions')\n",
    "            anti_testset_top_n = generate_full_top_n(algo, sp_ratings, eval_config)\n",
    "            for metric in eval_config.full_metrics:\n",
    "                assert metric in available_metrics['full']\n",
    "                evaluation_function, parameters =  available_metrics[\"full\"][metric]\n",
    "                evaluation_dict[model_name][metric] = evaluation_function(\n",
    "                    anti_testset_top_n,\n",
    "                    **precomputed_dict,\n",
    "                    **parameters\n",
    "                )\n",
    "    return pd.DataFrame.from_dict(evaluation_dict).T "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e83d1d",
   "metadata": {},
   "source": [
    "# 2. Evaluation metrics\n",
    "Implement evaluation metrics for either rating predictions (split metrics) or for top-n recommendations (loo metric, full metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1849e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hit_rate(anti_testset_top_n, testset):\n",
    "    \"\"\"Compute the average hit over the users (loo metric)\n",
    "    \n",
    "    A hit (1) happens when the movie in the testset has been picked by the top-n recommender\n",
    "    A fail (0) happens when the movie in the testset has not been picked by the top-n recommender\"\"\"\n",
    "    \n",
    "   #implement the function get_hit_rate \n",
    "    hits = 0\n",
    "    total = len(testset)  \n",
    "\n",
    "    # Iterate through each entry in the testset\n",
    "    for user_id, movie_id, _ in testset:\n",
    "        top_n_recommendations = anti_testset_top_n.get(user_id, [])\n",
    "        if movie_id in [recommended_movie[0] for recommended_movie in top_n_recommendations]:\n",
    "            hits += 1 \n",
    "\n",
    "    hit_rate = hits / total if total > 0 else 0\n",
    "    return hit_rate\n",
    "\n",
    "\n",
    "def get_novelty(anti_testset_top_n, item_to_rank):\n",
    "    #Compute the average novelty of the top-n recommendation over the users (full metric)\n",
    "    \n",
    "    \"\"\"The novelty is defined as the average ranking of the movies recommended\"\"\"\n",
    "    \n",
    "    total_rank = 0\n",
    "    num_entries = 0\n",
    "    total_items = len(item_to_rank)\n",
    "    for user_recommendations in anti_testset_top_n.values():\n",
    "        for movie_id, _ in user_recommendations:\n",
    "            total_rank += item_to_rank.get(movie_id, total_items + 1)\n",
    "            num_entries += 1\n",
    "    average_rank_sum = total_rank / num_entries if num_entries > 0 else 0\n",
    "    normalized_novelty = average_rank_sum / total_items  # Normalization step\n",
    "    return normalized_novelty\n",
    "    #return average_rank_sum \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9855b3",
   "metadata": {},
   "source": [
    "# 3. Evaluation workflow\n",
    "Load data, evaluate models and save the experimental outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93361d19",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704f4d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling model content_ridge_title_length\n",
      "Training split predictions\n",
      "- computing metric mae\n",
      "- computing metric rmse\n",
      "Training loo predictions\n",
      "Training full predictions\n"
     ]
    }
   ],
   "source": [
    "\n",
    "AVAILABLE_METRICS = {\n",
    "    \"split\": {\n",
    "        \"mae\": (accuracy.mae, {'verbose': False}),\n",
    "        \"rmse\": (accuracy.rmse, {'verbose': False}),\n",
    "    },\n",
    "    \"loo\": {\n",
    "        \"hit_rate\": (get_hit_rate, {}),\n",
    "    },\n",
    "    \"full\": {\n",
    "        \"novelty\": (get_novelty, {}),\n",
    "     } \n",
    "}\n",
    "\n",
    "sp_ratings = load_ratings(surprise_format=True)\n",
    "precomputed_dict = precompute_information()\n",
    "\n",
    "evaluation_report = create_evaluation_report(EvalConfig, sp_ratings, precomputed_dict, AVAILABLE_METRICS)\n",
    "print(\"Résultats de l'évaluation des 4 modèles :\")\n",
    "display(evaluation_report)   \n",
    "print(evaluation_report)  \n",
    "export_evaluation_report(evaluation_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460cc779",
   "metadata": {},
   "source": [
    "Commentaire:\n",
    "MAE / RMSE :\n",
    "La baseline 4 est de loin la plus performante et donc la plus précise selon ces deux indicateurs. Elle est suivie par la baseline 3. En revanche la baseline 1 est la moins performante ce qui en fait le modèle le moins précis.\n",
    "\n",
    "Hit Rate :\n",
    "Le taux de succès est très faible pour les baselines 1, 2 et 3, avec seulement environ 0,2 %, 0,4 % et 0,6 % respectivement dans ce cas. Cela montre leur incapacité à recommander efficacement des items pertinents. En revanche, la baseline 4 se distingue avec un Hit Rate nettement supérieur (5,2 %).\n",
    "\n",
    "Novelty :\n",
    "Les baselines 1, 2 et 3 obtiennent des scores de novelty relativement élevés, ce qui signifie qu’elles recommandent des items moins populaires. À l’inverse, la baseline 4 présente une valeur de nouveauté nettement plus faible ce qui indique qu’elle recommande majoritairement des films très vus. Cela maximise la précision mais se fait au détriment de la diversité des recommandations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
