{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a665885b",
   "metadata": {},
   "source": [
    "# Evaluator Module\n",
    "The Evaluator module creates evaluation reports.\n",
    "\n",
    "Reports contain evaluation metrics depending on models specified in the evaluation config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6aaf9140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reloads modules automatically before entering the execution of code\n",
    "#load_ext autoreload\n",
    "#autoreload 2\n",
    "\n",
    "# third parties imports\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "# -- add new imports here --\n",
    "\n",
    "# local imports\n",
    "from configs import EvalConfig\n",
    "from constants import Constant as C\n",
    "from loaders import export_evaluation_report\n",
    "from loaders import load_ratings\n",
    "from models import get_top_n\n",
    "# -- add new imports here --\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import SVD\n",
    "from surprise import accuracy\n",
    "from configs import EvalConfig \n",
    "from surprise import Dataset, Reader, accuracy\n",
    "from constants import Constant as C\n",
    "from surprise.model_selection import LeaveOneOut\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47c24a4",
   "metadata": {},
   "source": [
    "# 1. Model validation functions\n",
    "Validation functions are a way to perform crossvalidation on recommender system models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6d82188",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ratings(surprise_format=False):\n",
    "    df_ratings = pd.read_csv(C.EVIDENCE_PATH / C.RATINGS_FILENAME)\n",
    "    if surprise_format:\n",
    "        reader = Reader(rating_scale=C.RATINGS_SCALE)\n",
    "        data = Dataset.load_from_df(df_ratings[['userId', 'movieId', 'rating']], reader)\n",
    "        return data\n",
    "    else:\n",
    "        return df_ratings\n",
    "\n",
    "def generate_split_predictions(algo, ratings_dataset, eval_config):\n",
    "    \"\"\"\n",
    "    Generate predictions on a random test set specified in eval_config.\n",
    "    \n",
    "    Parameters:\n",
    "        algo: A Surprise algorithm instance (e.g., SVD, KNNBasic).\n",
    "        ratings_dataset: A Surprise Dataset object.\n",
    "        eval_config: An EvalConfig object containing evaluation parameters (e.g., test_size).\n",
    "    \n",
    "    Returns:\n",
    "        List of predictions made by the algorithm on the test set.\n",
    "    \"\"\"\n",
    "    # Récupérer la proportion test depuis eval_config\n",
    "    test_size = eval_config.test_size\n",
    "    # Diviser le dataset en train/test\n",
    "    trainset, testset = train_test_split(ratings_dataset, test_size=test_size)\n",
    "    # Entraîner le modèle sur le trainset\n",
    "    algo.fit(trainset)\n",
    "    # Faire des prédictions sur le testset\n",
    "    predictions = algo.test(testset)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def generate_loo_top_n(algo, ratings_dataset, eval_config):\n",
    "    \"\"\"Generate top-n recommendations for each user on a random Leave-one-out split (LOO)\"\"\"\n",
    "    #leaveOneOut object with one split\n",
    "    loo = LeaveOneOut(n_splits=1)\n",
    "    # Split the dataset into training and testing sets\n",
    "    trainset, testset = next(loo.split(ratings_dataset))\n",
    "    # Train the algorithm on the training set\n",
    "    algo.fit(trainset)\n",
    "    # Generate the anti-testset\n",
    "    anti_testset = trainset.build_anti_testset()\n",
    "    # Generate predictions on the anti-testset\n",
    "    predictions = algo.test(anti_testset)\n",
    "    # Get top-N recommendations\n",
    "    anti_testset_top_n = get_top_n(predictions, n=eval_config.top_n_value)\n",
    "    return anti_testset_top_n, testset\n",
    "\n",
    "\n",
    "def generate_full_top_n(algo, ratings_dataset, eval_config):\n",
    "    \"\"\"Generate top-n recommendations for each user with full training set (LOO)\"\"\"\n",
    "    # Construire l’ensemble d’entraînement complet à partir de toutes les données\n",
    "    full_trainset = ratings_dataset.build_full_trainset()\n",
    "\n",
    "    # Entraîner l’algorithme sur toutes les données disponibles\n",
    "    algo.fit(full_trainset)\n",
    "\n",
    "    # Générer le anti-testset : tous les items que chaque utilisateur n’a pas encore notés\n",
    "    anti_testset = full_trainset.build_anti_testset()\n",
    "\n",
    "    # Générer les prédictions sur le anti-testset\n",
    "    predictions = algo.test(anti_testset)\n",
    "\n",
    "    # Extraire les top-N recommandations\n",
    "    anti_testset_top_n = get_top_n(predictions, n=eval_config.top_n_value)\n",
    "\n",
    "    return anti_testset_top_n\n",
    "\n",
    "\n",
    "def precompute_information():\n",
    "    \"\"\" Returns a dictionary that precomputes relevant information for evaluating in full mode\n",
    "    \n",
    "    Dictionary keys:\n",
    "    - precomputed_dict[\"item_to_rank\"] : contains a dictionary mapping movie ids to rankings\n",
    "    - (-- for your project, add other relevant information here -- )\n",
    "    \"\"\"\n",
    "  \n",
    "    ratings = load_ratings()\n",
    "    # Compter les évaluations par film et trier par popularité décroissante\n",
    "    item_counts = ratings['movieId'].value_counts().sort_values(ascending=False)\n",
    "    # Mapper chaque film à son rang de popularité (1 = plus populaire)\n",
    "    item_to_rank = {movie: idx + 1 for idx, movie in enumerate(item_counts.index)}\n",
    "    \n",
    "    return {'item_to_rank': item_to_rank}            \n",
    "\n",
    "\n",
    "def create_evaluation_report(eval_config, sp_ratings, precomputed_dict, available_metrics):\n",
    "    \"\"\" Create a DataFrame evaluating various models on metrics specified in an evaluation config.  \n",
    "    \"\"\"\n",
    "    evaluation_dict = {}\n",
    "    for model_name, model, arguments in eval_config.models:\n",
    "        print(f'Handling model {model_name}')\n",
    "        algo = model(**arguments)\n",
    "        evaluation_dict[model_name] = {}\n",
    "        \n",
    "        # Type 1 : split evaluations\n",
    "        if len(eval_config.split_metrics) > 0:\n",
    "            print('Training split predictions')\n",
    "            predictions = generate_split_predictions(algo, sp_ratings, eval_config)\n",
    "            for metric in eval_config.split_metrics:\n",
    "                print(f'- computing metric {metric}')\n",
    "                assert metric in available_metrics['split']\n",
    "                evaluation_function, parameters =  available_metrics[\"split\"][metric]\n",
    "                evaluation_dict[model_name][metric] = evaluation_function(predictions, **parameters) \n",
    "\n",
    "        # Type 2 : loo evaluations\n",
    "        if len(eval_config.loo_metrics) > 0:\n",
    "            print('Training loo predictions')\n",
    "            anti_testset_top_n, testset = generate_loo_top_n(algo, sp_ratings, eval_config)\n",
    "            for metric in eval_config.loo_metrics:\n",
    "                assert metric in available_metrics['loo']\n",
    "                evaluation_function, parameters =  available_metrics[\"loo\"][metric]\n",
    "                evaluation_dict[model_name][metric] = evaluation_function(anti_testset_top_n, testset, **parameters)\n",
    "        \n",
    "        # Type 3 : full evaluations\n",
    "        if len(eval_config.full_metrics) > 0:\n",
    "            print('Training full predictions')\n",
    "            anti_testset_top_n = generate_full_top_n(algo, sp_ratings, eval_config)\n",
    "            for metric in eval_config.full_metrics:\n",
    "                assert metric in available_metrics['full']\n",
    "                evaluation_function, parameters =  available_metrics[\"full\"][metric]\n",
    "                evaluation_dict[model_name][metric] = evaluation_function(\n",
    "                    anti_testset_top_n,\n",
    "                    **precomputed_dict,\n",
    "                    **parameters\n",
    "                )\n",
    "        \n",
    "    return pd.DataFrame.from_dict(evaluation_dict).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e83d1d",
   "metadata": {},
   "source": [
    "# 2. Evaluation metrics\n",
    "Implement evaluation metrics for either rating predictions (split metrics) or for top-n recommendations (loo metric, full metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1849e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hit_rate(anti_testset_top_n, testset):\n",
    "    \"\"\"Compute the average hit over the users (loo metric)\n",
    "    \n",
    "    A hit (1) happens when the movie in the testset has been picked by the top-n recommender\n",
    "    A fail (0) happens when the movie in the testset has not been picked by the top-n recommender\n",
    "    \"\"\"\n",
    "   #implement the function get_hit_rate \n",
    "    hits = 0\n",
    "    total = len(testset)  \n",
    "\n",
    "    # Iterate through each entry in the testset\n",
    "    for user_id, movie_id, _ in testset:\n",
    "        top_n_recommendations = anti_testset_top_n.get(user_id, [])\n",
    "        print(f\"Recommandations pour l'utilisateur {user_id}: {top_n_recommendations}\")\n",
    "        if movie_id in [recommended_movie[0] for recommended_movie in top_n_recommendations]:\n",
    "            hits += 1 \n",
    "\n",
    "    print(f\"Nombre de hits : {hits} sur {total}\")\n",
    "    hit_rate = hits / total if total > 0 else 0\n",
    "    return hit_rate\n",
    "\n",
    "\n",
    "def get_novelty(anti_testset_top_n, item_to_rank):\n",
    "    \"\"\"Compute the average novelty of the top-n recommendation over the users (full metric)\n",
    "    \n",
    "    The novelty is defined as the average ranking of the movies recommended\n",
    "    \"\"\"\n",
    "    total_rank = 0\n",
    "    num_entries = 0\n",
    "    total_items = len(item_to_rank)\n",
    "    for user_recommendations in anti_testset_top_n.values():\n",
    "        for movie_id, _ in user_recommendations:\n",
    "            total_rank += item_to_rank.get(movie_id, total_items + 1)\n",
    "            num_entries += 1\n",
    "    average_rank_sum = total_rank / num_entries if num_entries > 0 else 0\n",
    "    normalized_novelty = average_rank_sum / total_items  # Normalization step\n",
    "    return normalized_novelty\n",
    "    #return average_rank_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9855b3",
   "metadata": {},
   "source": [
    "# 3. Evaluation workflow\n",
    "Load data, evaluate models and save the experimental outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "704f4d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling model baseline_1\n",
      "Training split predictions\n",
      "- computing metric mae\n",
      "- computing metric rmse\n",
      "Training loo predictions\n",
      "Recommandations pour l'utilisateur 11: [(527, 2), (4308, 2), (1214, 2), (4993, 2), (364, 2)]\n",
      "Recommandations pour l'utilisateur 13: [(1997, 2), (1721, 2), (2700, 2), (4993, 2), (527, 2), (364, 2)]\n",
      "Recommandations pour l'utilisateur 17: [(4308, 2), (2028, 2), (4993, 2), (2700, 2), (1214, 2)]\n",
      "Recommandations pour l'utilisateur 19: [(2028, 2), (1214, 2), (2700, 2), (364, 2), (4993, 2), (1997, 2), (4308, 2), (5952, 2), (1721, 2)]\n",
      "Recommandations pour l'utilisateur 23: [(2700, 2), (1997, 2), (2028, 2)]\n",
      "Recommandations pour l'utilisateur 27: [(4993, 2), (1997, 2), (1721, 2), (2700, 2), (5952, 2), (4308, 2), (364, 2), (2028, 2)]\n",
      "Nombre de hits : 6 sur 6\n",
      "Training full predictions\n",
      "Handling model baseline_2\n",
      "Training split predictions\n",
      "- computing metric mae\n",
      "- computing metric rmse\n",
      "Training loo predictions\n",
      "Recommandations pour l'utilisateur 11: [(1214, 4.3776874061001925), (2028, 4.03181761176121), (364, 3.045098885474434), (4308, 2.68228632332338), (527, 2.0356670011718534)]\n",
      "Recommandations pour l'utilisateur 13: [(364, 4.632451540781341), (4993, 4.13519435613909), (527, 3.333528157820125), (1721, 2.9063878166094232), (1997, 2.619736549801657), (2700, 2.21325090431571)]\n",
      "Recommandations pour l'utilisateur 17: [(1214, 4.023216816628896), (2028, 3.4734759867013265), (1997, 3.018747423269561), (4993, 2.1273513775988153), (4308, 2.002025365449762)]\n",
      "Recommandations pour l'utilisateur 19: [(4993, 4.931141904150612), (1997, 4.63898502387296), (2700, 4.608663801758331), (2028, 4.595353151871974), (5952, 4.240868943986358), (1214, 3.9193269930405146), (527, 3.735935727661765), (364, 2.8885708618108534), (1721, 2.2405902772773305)]\n",
      "Recommandations pour l'utilisateur 23: [(2700, 3.4435478937752064), (5952, 2.7366873418151347), (1997, 1.4028048322734632)]\n",
      "Recommandations pour l'utilisateur 27: [(5952, 4.866425471083035), (1997, 4.652044212951592), (1721, 4.46123971108656), (2028, 4.22011130805209), (4308, 3.194797215342357), (2700, 2.908039106210868), (1214, 2.0419692415678377), (364, 1.0561668006560758)]\n",
      "Nombre de hits : 6 sur 6\n",
      "Training full predictions\n",
      "Handling model baseline_3\n",
      "Training split predictions\n",
      "- computing metric mae\n",
      "- computing metric rmse\n",
      "Training loo predictions\n",
      "Recommandations pour l'utilisateur 11: [(4308, 2.8958333333333335), (364, 2.8958333333333335), (1214, 2.8958333333333335), (4993, 2.8958333333333335), (527, 2.8958333333333335)]\n",
      "Recommandations pour l'utilisateur 13: [(1997, 2.8958333333333335), (2700, 2.8958333333333335), (5952, 2.8958333333333335), (4993, 2.8958333333333335), (1721, 2.8958333333333335), (527, 2.8958333333333335)]\n",
      "Recommandations pour l'utilisateur 17: [(4308, 2.8958333333333335), (2028, 2.8958333333333335), (4993, 2.8958333333333335), (5952, 2.8958333333333335), (1214, 2.8958333333333335)]\n",
      "Recommandations pour l'utilisateur 19: [(2028, 2.8958333333333335), (1214, 2.8958333333333335), (2700, 2.8958333333333335), (527, 2.8958333333333335), (4993, 2.8958333333333335), (1997, 2.8958333333333335), (364, 2.8958333333333335), (5952, 2.8958333333333335), (1721, 2.8958333333333335)]\n",
      "Recommandations pour l'utilisateur 23: [(4993, 2.8958333333333335), (1997, 2.8958333333333335), (2700, 2.8958333333333335)]\n",
      "Recommandations pour l'utilisateur 27: [(527, 2.8958333333333335), (1997, 2.8958333333333335), (1721, 2.8958333333333335), (2700, 2.8958333333333335), (5952, 2.8958333333333335), (364, 2.8958333333333335), (4308, 2.8958333333333335), (2028, 2.8958333333333335)]\n",
      "Nombre de hits : 6 sur 6\n",
      "Training full predictions\n",
      "Handling model baseline_4\n",
      "Training split predictions\n",
      "- computing metric mae\n",
      "- computing metric rmse\n",
      "Training loo predictions\n",
      "Recommandations pour l'utilisateur 11: [(527, 3.510894538835507), (1997, 3.383970377750845), (1214, 3.234027144910137), (364, 3.140674328363011), (4308, 3.1243838393764465)]\n",
      "Recommandations pour l'utilisateur 13: [(527, 3.441875793254375), (1997, 3.3242229227909603), (4993, 3.245112911244763), (2028, 3.2006053858151797), (2700, 2.979918161237493), (1721, 2.854443845794396)]\n",
      "Recommandations pour l'utilisateur 17: [(4993, 3.3329147795343217), (2028, 3.0509050920524086), (4308, 2.991840342362339), (5952, 2.9578080235451267), (1214, 2.920051208075029)]\n",
      "Recommandations pour l'utilisateur 19: [(5952, 3.5113446339517798), (4993, 3.460108938107659), (1997, 3.450902169151116), (2028, 3.392562898724265), (364, 3.3618870536696988), (2700, 3.274217513640799), (1214, 3.1432893852188926), (4308, 3.115291143296437), (1721, 2.559845612165319)]\n",
      "Recommandations pour l'utilisateur 23: [(527, 3.1923565435960435), (1997, 3.046460277658938), (2700, 2.7981440057762295)]\n",
      "Recommandations pour l'utilisateur 27: [(527, 3.6593159898817724), (1997, 3.621558950763857), (5952, 3.485171082546505), (2028, 3.4199832353932798), (364, 3.371882492480045), (2700, 3.3216844430502546), (4308, 3.3155702145174812), (1721, 2.7087549979295726)]\n",
      "Nombre de hits : 6 sur 6\n",
      "Training full predictions\n",
      "Résultats de l'évaluation des 4 modèles :\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mae</th>\n",
       "      <th>rmse</th>\n",
       "      <th>hit_rate</th>\n",
       "      <th>novelty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>baseline_1</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.106537</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.603333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>baseline_2</th>\n",
       "      <td>1.610628</td>\n",
       "      <td>1.852330</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.603333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>baseline_3</th>\n",
       "      <td>1.375000</td>\n",
       "      <td>1.436141</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.603333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>baseline_4</th>\n",
       "      <td>1.255630</td>\n",
       "      <td>1.438627</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.603333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 mae      rmse  hit_rate   novelty\n",
       "baseline_1  2.000000  2.106537       1.0  0.603333\n",
       "baseline_2  1.610628  1.852330       1.0  0.603333\n",
       "baseline_3  1.375000  1.436141       1.0  0.603333\n",
       "baseline_4  1.255630  1.438627       1.0  0.603333"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 mae      rmse  hit_rate   novelty\n",
      "baseline_1  2.000000  2.106537       1.0  0.603333\n",
      "baseline_2  1.610628  1.852330       1.0  0.603333\n",
      "baseline_3  1.375000  1.436141       1.0  0.603333\n",
      "baseline_4  1.255630  1.438627       1.0  0.603333\n",
      "Evaluation report successfully exported to: /Users/delhoutecharles/Documents/GitHub/Recommender-Systeem-/data/test/evaluations/evaluation_report_2025_04_30.csv\n"
     ]
    }
   ],
   "source": [
    "AVAILABLE_METRICS = {\n",
    "    \"split\": {\n",
    "        \"mae\": (accuracy.mae, {'verbose': False}),\n",
    "        \"rmse\": (accuracy.rmse, {'verbose': False}),\n",
    "    },\n",
    "    \"loo\": {\n",
    "        \"hit_rate\": (get_hit_rate, {}),\n",
    "    },\n",
    "    \"full\": {\n",
    "        \"novelty\": (get_novelty, {}),\n",
    "    }\n",
    "}\n",
    "\n",
    "sp_ratings = load_ratings(surprise_format=True)\n",
    "precomputed_dict = precompute_information()\n",
    "evaluation_report = create_evaluation_report(EvalConfig, sp_ratings, precomputed_dict, AVAILABLE_METRICS)\n",
    "print(\"Résultats de l'évaluation des 4 modèles :\")\n",
    "display(evaluation_report)  # utile si tu es dans un notebook Jupyter\n",
    "print(evaluation_report)  \n",
    "export_evaluation_report(evaluation_report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
